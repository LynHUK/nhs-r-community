[
  {
    "objectID": "ts-and-cs.html",
    "href": "ts-and-cs.html",
    "title": "Terms and conditions",
    "section": "",
    "text": "Statements of fact and opinion published on this website are those of the respective authors and contributors and not necessarily those of NHS-R Community, its editors, The Strategy Unit, NHS Midlands and Lancashire Commissioning Support Unit, The Health Foundation or other partners and funders.\nNHS-R Community has prepared the content of this website responsibly and carefully. However, NHS-R Community, its editors, The Strategy Unit, NHS Midlands and Lancashire Commissioning Support Unit, The Health Foundation or other partners and funders disclaim all warranties, express or implied, as to the accuracy of the information contained in any of the materials on this website or on other linked websites or on any subsequent links. This includes, but is not by way of limitation:\n\nany implied warranties of merchantability and fitness for a particular purpose.\nany liability for damage to your computer hardware, data, information, materials and business resulting from the information or the lack of information available.\nany errors, omissions, or inaccuracies in the information.\nany decision made or action taken or not taken in reliance upon the information.\n\nNHS-R Community, its editors, The Strategy Unit, NHS Midlands and Lancashire Commissioning Support Unit, The Health Foundation or other partners and funders make no warranty as to the content, accuracy, timeliness or completeness of the information or that the information may be relied upon for any reason and bear no responsibility for the accuracy, content or legality of any linked site or for that of any subsequent links. NHS-R Community, its editors, The Strategy Unit, NHS Midlands and Lancashire Commissioning Support Unit, The Health Foundation or other partners and funders make no warranty that the website service will be uninterrupted or error-free or that any defects can be corrected.\nNHS-R Community, its editors, The Strategy Unit, NHS Midlands and Lancashire Commissioning Support Unit, The Health Foundation or other partners and funders shall not be liable for any losses or damages (including without limitation consequential loss or damage) whatsoever from the use of, or reliance on, the information in this website, or from the use of the internet generally. Links to other websites or the publication of advertisements do not constitute an endorsement or an approval by NHS-R Community, its editors, The Strategy Unit, NHS Midlands and Lancashire Commissioning Support Unit, The Health Foundation or other partners and funders.\nThese disclaimers and exclusions shall be governed by and construed in accordance with the laws of England and Wales under the exclusive jurisdiction of the courts of England and Wales. Those who choose to access this site from outside the United Kingdom are responsible for compliance with local laws if and to the extent local laws are applicable.\nBy using this site, you agree to these terms and conditions of use."
  },
  {
    "objectID": "ts-and-cs.html#legal-disclaimer",
    "href": "ts-and-cs.html#legal-disclaimer",
    "title": "Terms and conditions",
    "section": "",
    "text": "Statements of fact and opinion published on this website are those of the respective authors and contributors and not necessarily those of NHS-R Community, its editors, The Strategy Unit, NHS Midlands and Lancashire Commissioning Support Unit, The Health Foundation or other partners and funders.\nNHS-R Community has prepared the content of this website responsibly and carefully. However, NHS-R Community, its editors, The Strategy Unit, NHS Midlands and Lancashire Commissioning Support Unit, The Health Foundation or other partners and funders disclaim all warranties, express or implied, as to the accuracy of the information contained in any of the materials on this website or on other linked websites or on any subsequent links. This includes, but is not by way of limitation:\n\nany implied warranties of merchantability and fitness for a particular purpose.\nany liability for damage to your computer hardware, data, information, materials and business resulting from the information or the lack of information available.\nany errors, omissions, or inaccuracies in the information.\nany decision made or action taken or not taken in reliance upon the information.\n\nNHS-R Community, its editors, The Strategy Unit, NHS Midlands and Lancashire Commissioning Support Unit, The Health Foundation or other partners and funders make no warranty as to the content, accuracy, timeliness or completeness of the information or that the information may be relied upon for any reason and bear no responsibility for the accuracy, content or legality of any linked site or for that of any subsequent links. NHS-R Community, its editors, The Strategy Unit, NHS Midlands and Lancashire Commissioning Support Unit, The Health Foundation or other partners and funders make no warranty that the website service will be uninterrupted or error-free or that any defects can be corrected.\nNHS-R Community, its editors, The Strategy Unit, NHS Midlands and Lancashire Commissioning Support Unit, The Health Foundation or other partners and funders shall not be liable for any losses or damages (including without limitation consequential loss or damage) whatsoever from the use of, or reliance on, the information in this website, or from the use of the internet generally. Links to other websites or the publication of advertisements do not constitute an endorsement or an approval by NHS-R Community, its editors, The Strategy Unit, NHS Midlands and Lancashire Commissioning Support Unit, The Health Foundation or other partners and funders.\nThese disclaimers and exclusions shall be governed by and construed in accordance with the laws of England and Wales under the exclusive jurisdiction of the courts of England and Wales. Those who choose to access this site from outside the United Kingdom are responsible for compliance with local laws if and to the extent local laws are applicable.\nBy using this site, you agree to these terms and conditions of use."
  },
  {
    "objectID": "ts-and-cs.html#site-content",
    "href": "ts-and-cs.html#site-content",
    "title": "Terms and conditions",
    "section": "Site content",
    "text": "Site content\nAll content published through this site is open under the CC0 licence unless otherwise stated. We make every reasonable effort to locate, contact and acknowledge copyright owners and wish to be informed by any copyright owners who are not properly identified and acknowledged on this website so that we may make any necessary corrections.\nWhere licence terms for individual articles, videos, images and other published content permit republication, you may do so in accordance with the stated terms of the respective licence(s)."
  },
  {
    "objectID": "ts-and-cs.html#what-websites-do-we-link-to",
    "href": "ts-and-cs.html#what-websites-do-we-link-to",
    "title": "Terms and conditions",
    "section": "What websites do we link to?",
    "text": "What websites do we link to?\nNHS-R Community editors and contributors recommend external web links on the basis of their suitability and usefulness for our users.\nIt is not our policy to enter into agreements for reciprocal links.\nThe inclusion of a link to an organisation’s or individual’s website does not constitute an endorsement or an approval by NHS-R Community, its editors, The Strategy Unit, NHS Midlands and Lancashire Commissioning Support Unit, The Health Foundation or other partners and funders of any product, service, policy or opinion of the organisation or individual. NHS-R Community, its editors, The Strategy Unit, NHS Midlands and Lancashire Commissioning Support Unit, The Health Foundation or other partners and funders are not responsible for the content of external websites."
  },
  {
    "objectID": "ts-and-cs.html#what-websites-will-we-not-link-to",
    "href": "ts-and-cs.html#what-websites-will-we-not-link-to",
    "title": "Terms and conditions",
    "section": "What websites will we not link to?",
    "text": "What websites will we not link to?\nWe will not link to websites that contain racist, sexual or misleading content; that promote violence; that are in breach of any UK law; which are otherwise offensive to individuals or to groups of people.\nThe decision of NHS-R Community is final on publishing of content and no correspondence will be entered into.\nIf you wish to report a concern, please email nhs.rcommunity@nhs.net."
  },
  {
    "objectID": "ts-and-cs.html#software-and-services",
    "href": "ts-and-cs.html#software-and-services",
    "title": "Terms and conditions",
    "section": "Software and services",
    "text": "Software and services\nSource code and files for this site are available from GitHub. Use of our GitHub repository is governed by the Contributor Covenant Code of Conduct and further details for contribution can be found in NHS-R Statement on Tools.\nThis site is built using Quarto, an open-source scientific and technical publishing system developed by Posit. Quarto source code and software licences are available from GitHub.\nThe NHS-R Community website is hosted by GitHub Pages.\nThis site uses Google Analytics 4 for web analytics reporting."
  },
  {
    "objectID": "ts-and-cs.html#code-of-conduct",
    "href": "ts-and-cs.html#code-of-conduct",
    "title": "Terms and conditions",
    "section": "Code of conduct",
    "text": "Code of conduct\nHow to technically contribute to this website can be found in the Statement on Tools and the Code of Conduct can be found in the repository Code of Conduct."
  },
  {
    "objectID": "ts-and-cs.html#notice-and-takedown-policy",
    "href": "ts-and-cs.html#notice-and-takedown-policy",
    "title": "Terms and conditions",
    "section": "Notice and Takedown Policy",
    "text": "Notice and Takedown Policy\n\nincludes sensitive data redaction\nDetails of the Notice and Takedown Policy can be found in the NHS-R Way book along with details on how submissions with sensitive information will be rectified."
  },
  {
    "objectID": "licence.html",
    "href": "licence.html",
    "title": "Licence",
    "section": "",
    "text": "This website, its content and code are released under a CC0 1.0 Universal.\nBlogs that are published through this site will also be under this license and can be referenced to personal sites if the author wishes.\n\n  \n\nFor more information on the licences used by NHS-R Community go to the chapter Style Guide for code in the NHS-R Way book.\n\n\n\n Back to top"
  },
  {
    "objectID": "index.html#books",
    "href": "index.html#books",
    "title": "NHS-R Community Quarto website",
    "section": "Books",
    "text": "Books\n\n\n\n\n\n\n\n\n\n\n\nHealth Inequalities\n\n\nThis project is a collection of information and knowledge related to analytical work on “Health Inequalities”, as performed in the NHS and the wider UK Health and Social…\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNHS-R Way book\n\n\nEverything you need or want to know about NHS-R Community including: how to contribute and get involved, code styles, training preparation materials.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOpen Analytics Resources\n\n\nUseful links for health and care analysts and data scientists.\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#blog",
    "href": "index.html#blog",
    "title": "NHS-R Community Quarto website",
    "section": "Blog",
    "text": "Blog\n\n\n\n\n\n\n\n\n\nRecoding an NA and back again\n\n\n\n\n\n\nZoë Turner\n\n\nMar 12, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nShowcasing a function - separate()\n\n\n\n\n\n\nZoë Turner\n\n\nMar 12, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding a Quarto website for NHS-R Community\n\n\n\n\n\n\nZoë Turner\n\n\nFeb 24, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "CODE_OF_CONDUCT.html",
    "href": "CODE_OF_CONDUCT.html",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, caste, color, religion, or sexual identity and orientation.\nWe pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.\n\n\n\nExamples of behavior that contributes to a positive environment for our community include:\n\nDemonstrating empathy and kindness toward other people\nBeing respectful of differing opinions, viewpoints, and experiences\nGiving and gracefully accepting constructive feedback\nAccepting responsibility and apologizing to those affected by our mistakes, and learning from the experience\nFocusing on what is best not just for us as individuals, but for the overall community\n\nExamples of unacceptable behavior include:\n\nThe use of sexualized language or imagery, and sexual attention or advances of any kind\nTrolling, insulting or derogatory comments, and personal or political attacks\nPublic or private harassment\nPublishing others’ private information, such as a physical or email address, without their explicit permission\nOther conduct which could reasonably be considered inappropriate in a professional setting\n\n\n\n\nCommunity leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful.\nCommunity leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.\n\n\n\nThis Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event.\n\n\n\nInstances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement at nhs.rcommunity@nhs.net. All complaints will be reviewed and investigated promptly and fairly.\nAll community leaders are obligated to respect the privacy and security of the reporter of any incident.\n\n\n\nCommunity leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:\n\n\nCommunity Impact: Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community.\nConsequence: A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.\n\n\n\nCommunity Impact: A violation through a single incident or series of actions.\nConsequence: A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.\n\n\n\nCommunity Impact: A serious violation of community standards, including sustained inappropriate behavior.\nConsequence: A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.\n\n\n\nCommunity Impact: Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals.\nConsequence: A permanent ban from any sort of public interaction within the community.\n\n\n\n\nThis Code of Conduct is adapted from the Contributor Covenant, version 2.1, available at https://www.contributor-covenant.org/version/2/1/code_of_conduct.html.\nCommunity Impact Guidelines were inspired by [Mozilla’s code of conduct enforcement ladder][https://github.com/mozilla/inclusion].\nFor answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq. Translations are available at https://www.contributor-covenant.org/translations."
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#our-pledge",
    "href": "CODE_OF_CONDUCT.html#our-pledge",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, caste, color, religion, or sexual identity and orientation.\nWe pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community."
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#our-standards",
    "href": "CODE_OF_CONDUCT.html#our-standards",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "Examples of behavior that contributes to a positive environment for our community include:\n\nDemonstrating empathy and kindness toward other people\nBeing respectful of differing opinions, viewpoints, and experiences\nGiving and gracefully accepting constructive feedback\nAccepting responsibility and apologizing to those affected by our mistakes, and learning from the experience\nFocusing on what is best not just for us as individuals, but for the overall community\n\nExamples of unacceptable behavior include:\n\nThe use of sexualized language or imagery, and sexual attention or advances of any kind\nTrolling, insulting or derogatory comments, and personal or political attacks\nPublic or private harassment\nPublishing others’ private information, such as a physical or email address, without their explicit permission\nOther conduct which could reasonably be considered inappropriate in a professional setting"
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#enforcement-responsibilities",
    "href": "CODE_OF_CONDUCT.html#enforcement-responsibilities",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful.\nCommunity leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate."
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#scope",
    "href": "CODE_OF_CONDUCT.html#scope",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event."
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#enforcement",
    "href": "CODE_OF_CONDUCT.html#enforcement",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement at nhs.rcommunity@nhs.net. All complaints will be reviewed and investigated promptly and fairly.\nAll community leaders are obligated to respect the privacy and security of the reporter of any incident."
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#enforcement-guidelines",
    "href": "CODE_OF_CONDUCT.html#enforcement-guidelines",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:\n\n\nCommunity Impact: Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community.\nConsequence: A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.\n\n\n\nCommunity Impact: A violation through a single incident or series of actions.\nConsequence: A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.\n\n\n\nCommunity Impact: A serious violation of community standards, including sustained inappropriate behavior.\nConsequence: A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.\n\n\n\nCommunity Impact: Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals.\nConsequence: A permanent ban from any sort of public interaction within the community."
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#attribution",
    "href": "CODE_OF_CONDUCT.html#attribution",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "This Code of Conduct is adapted from the Contributor Covenant, version 2.1, available at https://www.contributor-covenant.org/version/2/1/code_of_conduct.html.\nCommunity Impact Guidelines were inspired by [Mozilla’s code of conduct enforcement ladder][https://github.com/mozilla/inclusion].\nFor answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq. Translations are available at https://www.contributor-covenant.org/translations."
  },
  {
    "objectID": "books/posts/open-analytics-resources/index.html",
    "href": "books/posts/open-analytics-resources/index.html",
    "title": "Open Analytics Resources",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "books/posts/health-inequalities/index.html",
    "href": "books/posts/health-inequalities/index.html",
    "title": "Health Inequalities",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "blog/why-government-needs-sustainable-software-too.html",
    "href": "blog/why-government-needs-sustainable-software-too.html",
    "title": "Why Government needs sustainable software too",
    "section": "",
    "text": "Unlike most of the 2017/2018 cohort, when I applied to become a fellow of the Software Sustainability Institute, I was a civil servant rather than an academic. In this blog post I want to talk about why Government needs sustainable software, the work being done to deliver it, and the lessons we learnt after the first year. But Government already has sustainable software…\nThere’s quite a bit of disambiguation that needs to be done to the statement ‘Government needs sustainable software’. In fact, Government already has sustainable software, and lots of it. One need only look at alphagov, the GitHub organisation for the Government Digital Service. Sustainable, often open source, software is alive and well here, written by professional software developers, and in many other places in central and local Government alike. But this isn’t the whole story.\nThere are other parts of Government that write software, but like many in academia, you may have a hard time convincing them of this fact. In central Government (this is where my experience lies, so I will focus largely upon it) there are literally thousands of statisticians, operational researchers, social researchers, economists, scientists, and engineers. Any one of these may be writing code in a variety of languages in the course of their daily work, but don’t identify as software developers. It’s among these professions that there are tasks that will look most familiar to the academic researcher. Government statisticians in particular are tasked with producing periodic publications which incorporate data, visualisations, and analyses, much like academic outputs.\nSo in this blog post, I’m really talking about bespoke software that is used to create Government statistical publications."
  },
  {
    "objectID": "blog/why-government-needs-sustainable-software-too.html#why-sustainability-is-so-important-for-government",
    "href": "blog/why-government-needs-sustainable-software-too.html#why-sustainability-is-so-important-for-government",
    "title": "Why Government needs sustainable software too",
    "section": "Why sustainability is so important for Government",
    "text": "Why sustainability is so important for Government\nThe reasons for sustainability in academic publications have been well documented by the Software Sustainability Institute, but I would argue that it is even more important that Government writes reproducible and sustainable software for its statistical publications. Here’s why:\n\nThe outputs really matter\nI don’t want to downplay the importance of research outputs, publishing accurate science is critical to advancing human knowledge. What is different about research is that there is rarely a single source of truth. If a research group publishes a groundbreaking finding, we all take notice; but we don’t trust the findings until they have been replicated preferably by several other groups.\nIt’s not like that in Government. If a Government department publishes a statistic, in many cases that is the single source of truth, so it is critical that the statistics are both timely and accurate.\n\n\nPublications are often produced by multiple people\nThe second way that Government statistical publications differ from academic scientific publications is that they are often produced by a team of people that is regularly changing. This means that even at the point that it is being produced it needs to be easy for another member of the team to pick up the work and run with it. If someone goes on holiday, or is sick at the critical moment, their colleagues need to be able to pick up from where they left off immediately, and understand all the idiosyncrasies perfectly. The knowledge simply cannot rest in one person’s head.\nMore than that, since publications are often periodic (for example monthly, or annual) and analysts typically change role once a year, the work will very likely need to be handed off to someone new on a regular basis. It is essential therefore that these processes are well documented, and that the code that is being handed over works as expected.\n\n\nThe taxpayer pays for it\nObviously, the longer it takes a team of statisticians to produce a statistical report in Government, the more it costs to the taxpayer, and all Government departments have an interest in being efficient, and reducing unnecessary waste.\nAdditionally, since Government statistical publications are paid for by the public, where possible Government should be open and publish its workings. Coding in the open is already an important part of the culture among digital professions, adopting sustainable software practices allows statistical publications to be produced with the same openness.\n\n\nWorking towards sustainability\nI started working in Government as a Data Scientist after doing a PhD and post-doc in environmental science. I’d attended two Software Carpentry workshops during this time, and wrote my PhD in LaTeX and R. On joining Government it was clear that we could apply some of these lessons to improve the reporting workflow in Government.\nWorking with the Department for Digital, Culture, Media, and Sport (DCMS) we had a first attempt at implementing a reproducible workflow for a statistical publication that was being produced with manual processes using a number of spreadsheets, a statistical computing package, and a word processor. We used RMarkdown to rewrite the publication, and abstracted the logic into an R package freely available on GitHub, complete with continuous integration from travis and appveyor.\nIn March of 2017 we published this work in a blog post, and worked hard to publicise this work with a large number of presentations and demonstrations to other Government departments. The prototype generated lots of interest; in particular an initial estimate that it could save 75% of the time taken to produce the same publication using the old methods.\nBy November we blogged again about successful trials of this approach in two further departments: the Ministry of Justice (MoJ), the Department for Education (DfE). We also produced a GitBook describing the various steps in more detail. Most of this is sensible software development practice; but it’s something that many Government analysts have not done before.\nBy the end of the year, the ideas had gained enough traction in the Government statistical community, that the Director General for the Office of Statistics Regulation (the body responsible for ensuring quality among official statistics) reported that this work was his favourite innovation of the year, although he wasn’t so keen on the name!\nWork continues to bring these techniques to a wider audience. There’s now a free online course built by one of my former colleagues to help civil servants get started, and a number of departments, particularly the MoJ are making great strides to incorporate these techniques into their workflows."
  },
  {
    "objectID": "blog/why-government-needs-sustainable-software-too.html#lessons-learnt",
    "href": "blog/why-government-needs-sustainable-software-too.html#lessons-learnt",
    "title": "Why Government needs sustainable software too",
    "section": "Lessons learnt",
    "text": "Lessons learnt\nA year or so after we set out with the intention of bringing sustainable software to Government statisticians, here are some of the lessons that I would like to share."
  },
  {
    "objectID": "blog/why-government-needs-sustainable-software-too.html#reproducibility-is-technical-sustainability-is-social",
    "href": "blog/why-government-needs-sustainable-software-too.html#reproducibility-is-technical-sustainability-is-social",
    "title": "Why Government needs sustainable software too",
    "section": "Reproducibility is technical, sustainability is social",
    "text": "Reproducibility is technical, sustainability is social\nWe called the first prototype a ‘Reproducible Analytical Pipeline’ and acronym ‘RAP’ has stuck. This is not a very good name on reflection because it belies the main difficulty in transitioning from manual workflows into something more automated: making it sustainable. It’s very well creating beautiful, abstracted, replicable data workflows, but they are completely useless if no one knows how to use them, or to update them. That situation is more dangerous than the manual workflows that exist in many places at present, because at least the barrier to entry for tortuous manual processes is lower: you don’t need to know how to program to interpret a complicated spreadsheet, you just need a lot of patience.\nWhat this move from manual to automated implies is a recognition of the need for specialists; organisations will need to recruit specialists, make use of the ones they already have, and upskill other staff. This is a challenge that all organisations will need to rise to if they are to make these new methods stick.\nThis is likely to be less of a problem for academia, where within certain fields there is already an expectation that researchers will be able to use particular tools, and there may be more time to develop expertise away from operational pressures. However, there also exists a powerful disincentive: because journal articles are closer to ‘one off’ than a periodic report, it is less critical that researchers leave the code behind a paper in a good state, as they may never need to come back to it again."
  },
  {
    "objectID": "blog/why-government-needs-sustainable-software-too.html#senior-buy-in-is-critical",
    "href": "blog/why-government-needs-sustainable-software-too.html#senior-buy-in-is-critical",
    "title": "Why Government needs sustainable software too",
    "section": "Senior buy-in is critical",
    "text": "Senior buy-in is critical\nIn just over a year, we went from seeing an opportunity to scaling the idea across a number of Government departments, traditionally very conservative organisations. Getting the buy-in of senior officials was absolutely critical in our ability to get the idea accepted.\nIt’s important to realise early that senior managers are often interested in very different things to the users of the software, so messages need to be targeted to gain traction with the right audience. For instance, an incentive for managers in academia might be: mitigating the risk of errors that could lead to retraction, rather than by the expectation of cost savings."
  },
  {
    "objectID": "blog/why-government-needs-sustainable-software-too.html#time-is-money",
    "href": "blog/why-government-needs-sustainable-software-too.html#time-is-money",
    "title": "Why Government needs sustainable software too",
    "section": "Time is money",
    "text": "Time is money\nOne of the reasons that we managed to make a big impact quickly is because Government departments are always keen to reduce costs. If a publication takes a team of four people a few weeks to produce, the cost quickly adds up. This is a feature of Government (and indeed industry) which is not shared by academia. Yes, it matters that work is delivered on time, but in my experience researcher time is a much more elastic resource. I was much more likely to work all evening or over the weekend as a PhD student or post doctoral researcher than I was as a civil servant; it was almost an expectation. For this reason, the financial imperative seems to be a much less powerful incentive in academia."
  },
  {
    "objectID": "blog/why-government-needs-sustainable-software-too.html#its-not-all-about-the-code",
    "href": "blog/why-government-needs-sustainable-software-too.html#its-not-all-about-the-code",
    "title": "Why Government needs sustainable software too",
    "section": "It’s not all about the code",
    "text": "It’s not all about the code\nNotwithstanding my comments about sustainability, it is important to note that reproducibility does not stop with reproducible code. We also need to worry about the data, and the environment. The former is particularly difficult in a Government setting, as one department often relies on another to provide the data, meaning that there is a less clear route to source than many academics enjoy. There are important initiatives underway in Government, such as GOV.UK Registers, which oversees the development of canonical lists of important information critical to the running of the country. Not all data can be treated in this way, and whilst taking snapshots of data may be a blunt instrument, it works when you don’t have control of where it comes from."
  },
  {
    "objectID": "blog/why-government-needs-sustainable-software-too.html#call-to-arms",
    "href": "blog/why-government-needs-sustainable-software-too.html#call-to-arms",
    "title": "Why Government needs sustainable software too",
    "section": "Call to arms",
    "text": "Call to arms\nAlmost all the projects I have referred to in this blog post are open source, and available on GitHub, so follow the links above if you are interested. There’s also two presentations on the topic available as slides (Earl conference 2017 and Government Statistical Service conference 2017) which give more technical details on the projects.\nThis blog is written by Matthew Upson, Data Scientist at Juro and was originally posted on the Software Sustainability website.\nThis blog has been formatted to remove Latin Abbreviations."
  },
  {
    "objectID": "blog/the-operator.html",
    "href": "blog/the-operator.html",
    "title": "The :: operator",
    "section": "",
    "text": "Most of the functionality in R comes from additional packages that you load. Sometimes two packages will have a function with the same name but they will do different things. In a situation where you have multiple packages with functions with the same name loaded, R will use the the function from the package you loaded the latest. As you can imagine, this can sometimes create problems. If you are lucky, you get an error message but if you are unlucky your code runs but with an unexpected result.\nLet me give you an example. I always load the dplyr package. Look what happens when I use summarize to calculate the mean sepal length by species.\n\nlibrary(dplyr)\n#&gt; Warning: package 'dplyr' was built under R version 4.3.2\n#&gt; \n#&gt; Attaching package: 'dplyr'\n#&gt; The following objects are masked from 'package:stats':\n#&gt; \n#&gt;     filter, lag\n#&gt; The following objects are masked from 'package:base':\n#&gt; \n#&gt;     intersect, setdiff, setequal, union\n\niris %&gt;% \n  group_by(Species) %&gt;% \n  summarize(sepal_length_mean=mean(Sepal.Length))\n#&gt; # A tibble: 3 × 2\n#&gt;   Species    sepal_length_mean\n#&gt;   &lt;fct&gt;                  &lt;dbl&gt;\n#&gt; 1 setosa                  5.01\n#&gt; 2 versicolor              5.94\n#&gt; 3 virginica               6.59\n\nCreated on 2024-02-21 with reprex v2.1.0\nSay that I then realise that I need the Hmisc package and load it. Look what happens when I rerun the same code as above.\n\n``` r\nlibrary(Hmisc)\n#&gt; Warning: package 'Hmisc' was built under R version 4.3.2\n#&gt; \n#&gt; Attaching package: 'Hmisc'\n#&gt; The following objects are masked from 'package:base':\n#&gt; \n#&gt;     format.pval, units\n\niris %&gt;% \n group_by(Species) %&gt;% \n summarize(sepal_length_mean=mean(Sepal.Length))\n#&gt; Error in iris %&gt;% group_by(Species) %&gt;% summarize(sepal_length_mean = mean(Sepal.Length)): could not find function \"%&gt;%\"\n\nCreated on 2024-02-21 with reprex v2.1.0\nR is now using the summarize function from the Hmisc package and I get an error because the syntax is wrong. The best way to solve this problem is to use the :: operator.Writing packagename::functionname tells R which package to get the function from.\n\niris3 &lt;- iris %&gt;% \n group_by(Species) %&gt;% \n dplyr::summarize(sepal_length_mean=mean(Sepal.Length))\n#&gt; Error in iris %&gt;% group_by(Species) %&gt;% dplyr::summarize(sepal_length_mean = mean(Sepal.Length)): could not find function \"%&gt;%\"\n\nCreated on 2024-02-21 with reprex v2.1.0"
  },
  {
    "objectID": "blog/the-operator.html#namespace-issues",
    "href": "blog/the-operator.html#namespace-issues",
    "title": "The :: operator",
    "section": "",
    "text": "Most of the functionality in R comes from additional packages that you load. Sometimes two packages will have a function with the same name but they will do different things. In a situation where you have multiple packages with functions with the same name loaded, R will use the the function from the package you loaded the latest. As you can imagine, this can sometimes create problems. If you are lucky, you get an error message but if you are unlucky your code runs but with an unexpected result.\nLet me give you an example. I always load the dplyr package. Look what happens when I use summarize to calculate the mean sepal length by species.\n\nlibrary(dplyr)\n#&gt; Warning: package 'dplyr' was built under R version 4.3.2\n#&gt; \n#&gt; Attaching package: 'dplyr'\n#&gt; The following objects are masked from 'package:stats':\n#&gt; \n#&gt;     filter, lag\n#&gt; The following objects are masked from 'package:base':\n#&gt; \n#&gt;     intersect, setdiff, setequal, union\n\niris %&gt;% \n  group_by(Species) %&gt;% \n  summarize(sepal_length_mean=mean(Sepal.Length))\n#&gt; # A tibble: 3 × 2\n#&gt;   Species    sepal_length_mean\n#&gt;   &lt;fct&gt;                  &lt;dbl&gt;\n#&gt; 1 setosa                  5.01\n#&gt; 2 versicolor              5.94\n#&gt; 3 virginica               6.59\n\nCreated on 2024-02-21 with reprex v2.1.0\nSay that I then realise that I need the Hmisc package and load it. Look what happens when I rerun the same code as above.\n\n``` r\nlibrary(Hmisc)\n#&gt; Warning: package 'Hmisc' was built under R version 4.3.2\n#&gt; \n#&gt; Attaching package: 'Hmisc'\n#&gt; The following objects are masked from 'package:base':\n#&gt; \n#&gt;     format.pval, units\n\niris %&gt;% \n group_by(Species) %&gt;% \n summarize(sepal_length_mean=mean(Sepal.Length))\n#&gt; Error in iris %&gt;% group_by(Species) %&gt;% summarize(sepal_length_mean = mean(Sepal.Length)): could not find function \"%&gt;%\"\n\nCreated on 2024-02-21 with reprex v2.1.0\nR is now using the summarize function from the Hmisc package and I get an error because the syntax is wrong. The best way to solve this problem is to use the :: operator.Writing packagename::functionname tells R which package to get the function from.\n\niris3 &lt;- iris %&gt;% \n group_by(Species) %&gt;% \n dplyr::summarize(sepal_length_mean=mean(Sepal.Length))\n#&gt; Error in iris %&gt;% group_by(Species) %&gt;% dplyr::summarize(sepal_length_mean = mean(Sepal.Length)): could not find function \"%&gt;%\"\n\nCreated on 2024-02-21 with reprex v2.1.0"
  },
  {
    "objectID": "blog/text-mining-term-frequency-analysis-and-word-cloud-creation.html",
    "href": "blog/text-mining-term-frequency-analysis-and-word-cloud-creation.html",
    "title": "Text Mining – Term Frequency analysis and Word Cloud creation in R",
    "section": "",
    "text": "Analysing the pre-conference workshop sentiments\nLoading in the required packages:\ninstall_or_load_pack &lt;- function(pack) {\n  create.pkg &lt;- pack[!(pack %in% installed.packages()[, \"Package\"])]\n  if (length(create.pkg)) {\n    install.packages(create.pkg, dependencies = TRUE)\n  }\n  sapply(pack, require, character.only = TRUE)\n}\npackages &lt;- c(\n  \"ggplot2\", \"tidyverse\", \"data.table\", \"wordcloud\", \"tm\", \"wordcloud2\",\n  \"scales\", \"tidytext\", \"devtools\", \"twitteR\", \"caret\", \"magrittr\", \"RColorBrewer\", \"tidytext\", \"ggdendro\",\n  \"tidyr\", \"topicmodels\", \"SnowballC\", \"gtools\"\n)\ninstall_or_load_pack(packages)\nThis function was previously covered in blog post: https://nhsrcommunity.com/blog/a-simple-function-to-install-and-load-packages-in-r/.\nHere I specify that I want to load the main packages for dealing with sentiment and discourse analysis in R. Libraries such as {tm}, {wordcloud} and {wordcloud2} are loaded for working with this type of data."
  },
  {
    "objectID": "blog/text-mining-term-frequency-analysis-and-word-cloud-creation.html#choosing-the-file-to-import",
    "href": "blog/text-mining-term-frequency-analysis-and-word-cloud-creation.html#choosing-the-file-to-import",
    "title": "Text Mining – Term Frequency analysis and Word Cloud creation in R",
    "section": "Choosing the file to import",
    "text": "Choosing the file to import\nThe file we have to import is a prepared csv file and instead of hard coding the path to load the file from I simply use:\n\npath &lt;- choose.files()\n\n\n\nScreenshot of Select files window that pops up to allow files to be chosen\n\nThis is a special function which allows you to open a dialog UI from R:\nFrom this dialog I select the csv file I want to be imported. Once I have selected the csv and hit open, the path variable will be filled with the location of the file to work with."
  },
  {
    "objectID": "blog/text-mining-term-frequency-analysis-and-word-cloud-creation.html#creating-the-r-data-frame",
    "href": "blog/text-mining-term-frequency-analysis-and-word-cloud-creation.html#creating-the-r-data-frame",
    "title": "Text Mining – Term Frequency analysis and Word Cloud creation in R",
    "section": "Creating the R Data Frame",
    "text": "Creating the R Data Frame\nTo create the data frame I can now pass the variable path to the read_csv() command:\n\nworkshop_sentiment &lt;- read_csv(path, col_names = T)\n\nThis will read the textual data from the workshops in to a data frame with 2 columns. The first relates to what the attendees enjoyed about the workshop and the second relates to improvements that can be made:\n\n\nScreenshot of the highlights and improvements text from a pre-conference survey"
  },
  {
    "objectID": "blog/text-mining-term-frequency-analysis-and-word-cloud-creation.html#function-to-create-textual-corpus",
    "href": "blog/text-mining-term-frequency-analysis-and-word-cloud-creation.html#function-to-create-textual-corpus",
    "title": "Text Mining – Term Frequency analysis and Word Cloud creation in R",
    "section": "Function to create textual corpus",
    "text": "Function to create textual corpus\nAs I want to replicate this for highlights and improvements – I have created a function that could be replicated with any text analysis to create what is known as a text corpus (see: https://cran.r-project.org/web/packages/tm/vignettes/tm.pdf) this creates a series of documents, in our case sentences.\n\ncorpus_tm &lt;- function(x) {\n  library(tm)\n\n  corpus_tm &lt;- Corpus(VectorSource(x))\n}\n\nThis function allows you to pass any data frame to the function and creates a corpus for each data frame you pass to the function. The data frame would be passed to the x parameter. The VectorSource() function creates an element for each part of the corpus."
  },
  {
    "objectID": "blog/text-mining-term-frequency-analysis-and-word-cloud-creation.html#create-corpus-for-highlights-and-improvements-data-frame",
    "href": "blog/text-mining-term-frequency-analysis-and-word-cloud-creation.html#create-corpus-for-highlights-and-improvements-data-frame",
    "title": "Text Mining – Term Frequency analysis and Word Cloud creation in R",
    "section": "Create Corpus for Highlights and Improvements data frame",
    "text": "Create Corpus for Highlights and Improvements data frame\nNow the function has been created, I can simply pass the two separate data frames I created before to create two corpuses:\n\ncorpus_positive &lt;- corpus_tm(ws_highlights$Highlights)\n\ncorpus_improvements &lt;- corpus_tm(ws_improvements$Improvements)\n\nThe code block above shows that I create a corpus for the positive (highlights) data frame and an improvements corpus. This will display as hereunder in your environment:\n\n\nScreenshot of the corpus output from the Environment in RStudio\n\n##Function to clean data in the corpus\nThe most common cleaning task of working with text data is to remove things like punctuation, common English words, and so on This is something I have to repeat multiple times when dealing with discourse analysis:\n\nclean_corpus &lt;- function(corpus_to_use) {\n  library(magrittr)\n\n  library(tm)\n\n  corpus_to_use %&gt;%\n    tm_map(removePunctuation) %&gt;%\n    tm_map(stripWhitespace) %&gt;%\n    tm_map(content_transformer(function(x) iconv(x, to = \"UTF-8\", sub = \"byte\"))) %&gt;%\n    tm_map(removeNumbers) %&gt;%\n    tm_map(removeWords, stopwords(\"en\")) %&gt;%\n    tm_map(content_transformer(tolower)) %&gt;%\n    tm_map(removeWords, c(\"etc\", \"ie\", \"eg\", stopwords(\"english\")))\n}\n\nThe parameter here takes the corpus object previously created and uses the corpus passed to:\n\nRemove punctuation\n\nStrip out whitespace between each text item, as the VectorSource has stripped out each word from each sentence in the data frame\n\nChange the underlying formatting of the text to UTF-8\n\nRemove numbers\n\nRemove common English word (stop words)\n\nChange the case to lower case\n\nRemove a custom vector of words to adjust for things like “e.g.”, “i.e.”, “etc”.\n\nTo clean the corpus objects I simply pass the original corpus objects back through this function to perform cleaning:\n\ncorpus_positive &lt;- clean_corpus(corpus_positive)\n\ncorpus_improvements &lt;- clean_corpus(corpus_improvements)\n\nInspection of one of the data frames confirms that this has successfully been cleaned:"
  },
  {
    "objectID": "blog/text-mining-term-frequency-analysis-and-word-cloud-creation.html#create-termdocumentmatrix-to-attain-frequent-terms",
    "href": "blog/text-mining-term-frequency-analysis-and-word-cloud-creation.html#create-termdocumentmatrix-to-attain-frequent-terms",
    "title": "Text Mining – Term Frequency analysis and Word Cloud creation in R",
    "section": "Create TermDocumentMatrix to attain frequent terms",
    "text": "Create TermDocumentMatrix to attain frequent terms\nThe term document matrix (explained well here: https://www.youtube.com/watch?v=dE10fBCDWQc) can be used with the corpus to identify frequent terms by classification on a matrix. However, more code is needed to do this:\n\n find_freq_terms_fun &lt;- function(corpus_in){\n\n  doc_term_mat &lt;- TermDocumentMatrix(corpus_in)\n\n  freq_terms &lt;- findFreqTerms(doc_term_mat)[1:max(doc_term_mat$nrow)]\n\n  terms_grouped &lt;-\n\n    doc_term_mat[freq_terms,] %&gt;%\n\n    as.matrix() %&gt;%\n\n    rowSums() %&gt;%\n\n    data.frame(Term=freq_terms, Frequency = .) %&gt;%\n\n    arrange(desc(Frequency)) %&gt;%\n\n    mutate(prop_term_to_total_terms=Frequency/nrow(.))\n\n  return(data.frame(terms_grouped))\n\n }\n\nThis function needs explanation. The function uses as a single parameter the corpus that you need to pass in, then a variable is created to create the doc_term_mat which uses the tm TermDocumentMatrix.\nNext, I use the findFreqTerms function to iterate from the first entry to the maximum number of rows in the matrix. These are the powerhouses of the function, as they highlight how many times a word has been used in a sentence across all the rows of text.\nThe terms_grouped variable then slices the term matrix with the frequent terms, this is converted to a matrix, sum of each row are calculated i.e. the number of times the word appears. Then, a data frame is created of the terms in the function with the headings term and Frequency.\nNext, we use the power of {dplyr} to use arrange by the frequency descending and to add a mutated column to the data frame to calculate the proportion of that specific term over all terms. The return(data.frame(terms_group)) then forces R to return the results of the function.\nI then pass my data frames (highlights and improvements) to the function I have just created to see if this method works:\n\npositive_freq_terms &lt;- data.frame(find_freq_terms_fun(corpus_positive))\n\nimprovement_freq_terms &lt;- data.frame(find_freq_terms_fun(corpus_improvements))\n\nThese will be built as data frames and can be viewed in R Studio’s Data environment window:\n\n\nScreenshot of the RStudio’s data environment window with terms and frequency\n\nThis has worked just as expected. You could now use ggplot2 to produce a bar chart / pareto chart of the terms."
  },
  {
    "objectID": "blog/text-mining-term-frequency-analysis-and-word-cloud-creation.html#create-a-word-cloud-with-the-wordcloud2-package",
    "href": "blog/text-mining-term-frequency-analysis-and-word-cloud-creation.html#create-a-word-cloud-with-the-wordcloud2-package",
    "title": "Text Mining – Term Frequency analysis and Word Cloud creation in R",
    "section": "Create a Word Cloud with the {wordcloud2} package",
    "text": "Create a Word Cloud with the {wordcloud2} package\nR has a {wordcloud} package that produces relatively nice looking word clouds, but {wordcloud2} surpasses this in terms of visualisation. To use this function is easy now I have the frequent terms data frame – using the highlights data frame this can be implemented by using the below syntax:\n\nwordcloud2(positive_freq_terms[,1:2],\n\n           shape=\"pentagon\",\n\n           color=\"random-dark\")\n\nTo use the function I pass the data frame and use the term and frequency fields only to use the visualisation. There are a number of options and these can be accessed by using the help(\"wordcloud2\") function. Here I use the shape and color parameters to set the display of the word cloud:\n\n\nScreenshot of the word cloud with Code being the largest word, followed by plot, useful, pick and facets\n\nThis can be exported in the viewer window by using the Export function.\nThis word cloud relates to the pre workshop prior to the conference. I personally thought the NHS-R conference was amazing and I was honoured to have a spot to speak amongst so many other brilliant R users.\nR is so versatile – every day is like a school day when you are learning it, but what a journey.\nThis blog has been edited for NHS-R styles."
  },
  {
    "objectID": "blog/success-story-lydia-briggs.html",
    "href": "blog/success-story-lydia-briggs.html",
    "title": "Success story - Dr Lydia Briggs, Data Scientist, GOSH",
    "section": "",
    "text": "What was the problem/challenge you were trying to address?\nHospital surgical teams regularly hold meetings to discuss cases and events that have happened in their departments. The patient data to be discussed can be complicated and varied, requiring analysts to manually go through many notes, reports and summaries in the electronic patient record system. By developing an extraction process and incorporating generalizable coding with R, this lengthy and manual process can be automated and can save valuable hours of analyst time.\n\n\nHow has R helped you? Any particular libraries/products/packages you found the most useful?\nR is a useful tool for this project as it allows for easy linkage of different datasets at the patient level and allows for informative visualisation. For example, a patient who is currently in the cardiology ward can easily be linked to their hospital admission history, previous procedures, laboratory results and so on and the breadth of this information can be shown in an interactive time line plot using ggplot2 and plotly. As the data is extracted on a weekly basis, a {targets} pipeline has been beneficial in saving processing time and in compartmentalising the features and functions.\n\n\nWhat is the result?\nWe developed a targets pipeline which outputs a markdown report and a shiny app which displays the features required by the hospital department. A shiny app allows the department to interact with the data through selecting features and plotly timeline visualisations. Due to the ease of joining datasets together using R, more information can be provided and presented in the department meetings in an easy to understand format.\nThis blog has been formatted to remove Latin Abbreviations.\n\n\n\n\n Back to topReuseCC0CitationFor attribution, please cite this work as:\nBriggs, Lydia. 2022. “Success Story - Dr Lydia Briggs, Data\nScientist, GOSH.” March 21, 2022. https://nhs-r-community.github.io/nhs-r-community//blog/success-story-lydia-briggs.html."
  },
  {
    "objectID": "blog/simpler-sql-with-dplyr.html",
    "href": "blog/simpler-sql-with-dplyr.html",
    "title": "Simpler SQL with dplyr",
    "section": "",
    "text": "Comparing dplyr with SQL nested queries\nFollowing on from my last post, where I demonstrated R to some first time R users, I want to do a wee comparison of dplyr V SQL, so that folks, particularly those in the NHS who might be R curious, can see just what the fuss is about.\nTo do so I want to recap on the example I showed at the AphA Scotland event.\nThis,in turn goes back to some work I’ve been doing with Neil Pettinger, where we are looking at ways to visualise patient flow.\nThis relies on a spreadsheet that Neil originally put together. Part of my demo was to explain how to recreate the visualisation in R, but I also showed some of the data transformation steps carried out using dplyr and some fellow tidyverse helpers.\nIn this post I want to focus on that a but further, by showing the SQL code I would write to arrive at the same end result.\nIn order to do this I imported Neil’s spreadsheet (which I’ve uploaded - with Neil’s permission to the repo RowOfDots) to into a SQL Server table (by using the built in import wizard, for a quick but not reproducible way of ingesting the data).\nHere’s how that looks:\n\nNB - ALL patient names are entirely made up.\nAs a reminder, for this task we need to create a column that mimics Excel’s floor function and reduces the MovementDateTime field to the nearest 15 mins. We also want to get a count of how many patient were either moving IN or OUT during each 15 minute segment of the day.\n\nSELECT [MovementDateTime],\n[FirstName],\n[LastName],\n[Ward_Dept],\n[Staging_Post],\n[Movement_Type],\n[IN_OUT],\ncast(round(floor(cast([MovementDateTime] AS float(53))*24*4)/(24*4),5) AS smalldatetime) AS Movement15,\n(CASE WHEN IN_OUT = 'IN' THEN 1 ELSE -1 END) AS [counter]\nFROM [DB].[dbo].[TABLENAME]\nGO\n\nYou’d need to replace the database and table names to suit. I’m not going to explain the code for flooring the datetime field - just know that it works, but you may want to compare the syntax for the case when statement with the equivalent dplyr code ( see later).\nHere is the table output - with the 2 new columns at the end:\n\nNow things get more complicated.\nI have a counter field, but I want to get a cumulative count by each 15 minute segment, staging post and whether this was a movement in or out.\nOne way to do this is to wrap the original query inside another query, so that our newly created counter column can be utilised. This is a similar idea to the the method of mutating a column in dplyr, and having it available within the next pipe.\nWe have to make use of SQL’s windowing functionality to create virtual groupings and orders within the data ( SQL is a set based language, and there is no concept of row order within a set. Therefore to get a cumulative count, we need to make SQL think in terms of rows by partitioning the data by the desired grouping columns and providing columns to order by):\n\nSELECT        x.[MovementDateTime],\nx.[FirstName],\nx.[LastName],\nx.[Ward_Dept],\nx.[Staging_Post],\nx.[Movement_Type],\nx.[IN_OUT],\nx.[Movement15],\nx.[counter],\nROW_NUMBER() OVER (PARTITION BY IN_OUT, Movement_Type,Staging_Post,Movement15 ORDER BY (MovementDateTime))AS R_Number\nFROM\n(SELECT [MovementDateTime],\n[FirstName],\n[LastName],\n[Ward_Dept],\n[Staging_Post],\n[Movement_Type],\n[IN_OUT],\ncast(round(floor(cast([MovementDateTime] AS float(53))*24*4)/(24*4),5) AS smalldatetime) AS Movement15,\n(CASE WHEN IN_OUT = 'IN' THEN 1 ELSE -1 END) AS [counter]\nFROM [DB].[dbo].[TABLENAME])x\nUnderstanding windowing techniques is a great SQL skill to have. Don’t forget where you first saw this ;)!\n\nUnderstanding windowing techniques is a great SQL skill to have. Don’t forget where you first saw this ;)!\nA couple of things to note here are that when we wrap or “nest” the original query, I gave it the alias ‘x’. You do need to provide an alias for this inner query, or the outer query won’t work. Although not strictly necessary, I also prefixed the column names in the outer query so it’s clear that I am selecting the columns from the “virtual” table defined by the inner query.\nHere’s the output with our new Row number (or RNumber) field.\n\nAlmost done, but this is still not in the right format - I need to get an accurate cumulative count. Once more, I take the previous query, and nest that inside a new query - so you can see this is similar to lots of base R style manipulation where the code starts from the middle, or an end, and works back.\n\nSELECT y.MovementDateTime,\ny.FirstName,\ny.LastName,\ny.Ward_Dept,\ny.Staging_Post,\ny.Movement_Type,\ny.IN_OUT,\ny.Movement15,\ny.[counter],\ny.[counter] * y.R_Number AS Movement_15_SEQNO\nFROM (\nSELECT x.MovementDateTime,\nx.FirstName,\nx.LastName,\nx.Ward_Dept,\nx.Staging_Post,\nx.Movement_Type,\nx.IN_OUT,\nx.Movement15,\nx.[counter],\nROW_NUMBER() OVER (PARTITION BY IN_OUT, Movement_Type,Staging_Post,Movement15 ORDER BY (MovementDateTime))AS R_Number\nFROM\n(SELECT [MovementDateTime],\n[FirstName],\n[LastName],\n[Ward_Dept],\n[Staging_Post],\n[Movement_Type],\n[IN_OUT],\ncast(round(floor(cast([MovementDateTime] AS float(53))*24*4)/(24*4),5) AS smalldatetime) AS Movement15,\n(CASE WHEN IN_OUT = 'IN' THEN 1 ELSE -1 END) AS [counter]\nFROM [DB].[dbo].[TABLENAME])x) y\nORDER BY MovementDateTime\nGO\n\nTo recap - our first query floored the movement time to 15 minute intervals and gave us a counter field, we then used that counter field to generate a row number field. Now, even if I’d ordered the result of the second query by MovementDateTime, it still wouldn’t suffice because the rownumbers are all positive, and I want them to be negative when the movement was a movement OUT.\nWe can’t manipulate the row number field within the same query that it is created, so we nest the whole lot once more, this time arranging in the correct time order and multiplying the counter field by our row number field.\nYou’ll notice the second query has been aliased (with a ‘y’) and the columns prefixed so that is is clear exactly where the query is obtaining the data from.\nThis gives us our final output:\n\nA reminder of the dplyr code I used:\n\nplot_data &lt;- data %&gt;%\nmutate(Movement15 = lubridate::floor_date(MovementDateTime,\"15 minutes\")) %&gt;%\ngroup_by(IN_OUT, Movement_Type,Staging_Post,Movement15) %&gt;%\nmutate(counter = case_when(\nIN_OUT == 'IN' ~ 1,\nIN_OUT == 'OUT' ~ -1)) %&gt;%\nmutate(Movement_15_SEQNO = cumsum(counter)) %&gt;%\nungroup()\n\nAnd here is the output - compare with above:\n\nA lot more elegant? Definitely.\nAnother approach to writing the code in SQL would be to use a Common Table Expression, which is a more straightforward of writing and reading it. It’s a similar idea in that you create virtual tables with queries that then run top to bottom until you get your final output. However that is a post for another day :)\nWhat I hope you get from this post is that dplyr and other packages (lubridate for example) really do make life easier for data manipulation.\nLook at the SQL for flooring the date, compared to the lubridate call. Look at the elegance of mutating new columns and having them available within the next chain, compared to horrendous multi-layered nested queries (this one was pretty tame - imagine a few more levels on top of that). You can see how traditional SQL can get unwieldy.\nDplyr is a fantastic asset to the R community, and I hope it might prove to be a great hook to get R further established within the analytical departments of the NHS.\nThis blog was written by John MacKintosh, NHS data analyst based in Inverness, Scotland, and was originally posted on his blog site [johnmackintosh.net]](https://johnmackintosh.net/blog/2018-05-31-dplyr-for-the-win/).\n\n\n Back to topReuseCC0CitationFor attribution, please cite this work as:\nMacKintosh, John. 2018. “Simpler SQL with Dplyr.” June 7,\n2018. https://nhs-r-community.github.io/nhs-r-community//blog/simpler-sql-with-dplyr.html."
  },
  {
    "objectID": "blog/roadmap-to-collaborative-working-using-r-in-the-nhs-part-i-workflows.html",
    "href": "blog/roadmap-to-collaborative-working-using-r-in-the-nhs-part-i-workflows.html",
    "title": "Roadmap to collaborative working using R in the NHS: Part I- Workflows",
    "section": "",
    "text": "We finally have a tool that is data independent. R allows the scripting of a data science methodology that we can share and develop and which does not necessarily depend on carrying patient data with it. This means that the door to working collaboratively on solutions is now wide open. But before this happens in a truly robust way there are several hurdles that need to be overcome. This is the first part of a series of articles in which I hope to outline the roadmap to networking our solutions in R so we can work collaboratively between Trusts to accelerate data science solutions for healthcare data.\n\nStandardising R Workflows in the NHS\nIn the throws of a new(ish) tool such as R, everybody has a way of working which is slightly different to one another. This is how best practice evolves, but in the end the best practice has to be standardised so that developers can work together. This is an article outlining my best practice for R script organisation and is an invitation for comment to see if there is any interest in developing a NHS standard for working and organising our solutions.\n\n\nPrinciples of R script development\nAs the R development environment is so methods based, it makes sense to have a standardised way to develop scripts so that different developers understand the basic workflow of data and can focus on the specific methodology for the specific problem rather than disentangle endless subsets of data and how each is cleaned and merged etc. I use various principles when developing a script and useful approach to R script development.\n\n\na) An R script should focus on a specific problem.\nA solution is only as good as the question. Questions come in a variety of scopes and shapes and there is an art to asking a solvable question which is beyond the limits of this article.\nHaving defined a question, a script should set out to be the solution to that question and not be a generic answer. Generic answers belong as functions or collections of functions called packages. An R script should tackle specific problems such as ‘How many endoscopies did we perform last year’ and you find that this kind of question is asked a lot (’How many x did we perform last y years) then the script might become a function and a collection of functions might become a package.\n\n\nb) The R script should access the minimal dataset needed and avoid intermediate datasets.\nThere is a real danger with data analysis that the data set used is huge but you only need a part of it. With R you can have the ability to specify the data used from within the script so that you should use the minimum dataset that is pertinent to the question. In fact the whole script should be specific to the question being asked. The data access should, as far as possible also be from the data repository rather than an intermediate dataset. For example you can specify a SQL query from within R to an electronic patient record (EPR) rather than get a data dump from the EPR into, for example, an Excel spreadsheet, and then import the Excel spreadsheet. It’s just more secure and avoids versioning issues with the data dump.\n\n\nc) An R script should be organised according to a standardised template\nAll analysis I perform adheres to a specific workflow for each script so that the script is separated into specific subsections that perform types of actions on the data. This also incorporates the important aspect of commenting on each part of the workflow. This is important so that developers can understand the code further down the line. The workflow I use is as follows:\n## Title ##\n\n## Aim ##\n\n## Libraries ##\n\n## Data access ##\n\n## Data cleaning & Data merging ##\n\n## Data mutating (creating columns from pre-existing data) ##\n\n## Data forking (filtering and subsetting) ##\n\n## Documenting the sets (usually creation of consort type diagrams with diagrammeR##\n\n## Documenting the code with CodeDepends ##\n\n## Data analysis ##\n\n\n\nData access\nThe title of the script including the author and date is written at the top. The aim of the script is then stated along with any explanation (why am I doing this and so on). The workflow makes sure that all libraries are loaded at the beginning. Data access is also maintained at the top so anyone can immediately see the starting point for the analysis. Data access should specify the minimal dataset needed to answer the specific question of the script as explained above. For example there is no point using a dataset of all endoscopies between 2001 and 2011 when your script is only looking at colonoscopy specifically. I also try to avoid functions such as file.choose() as I like to keep the path to the source file documented, whether it is a local or remote repository.\n\n\nData cleaning & Data merging\nThe difficult task of data cleaning and merging with other datasets is then performed. One of my data principles is that when working with datasets you should start with the smallest dataset that answers all of your questions and then filter down for each question rather than build up a dataset throughout a script, so I like to merge external datasets early when possible. This could be called early binding technically but given the data access part of the code specifies a data set that is question-driven, I am early binding to a late-bound set (if that makes sense).\n\n\nData mutating\nOnce cleaning and merging is performed, subsets of data for specific subsections of the question can be done and then the statistical analysis performed on each subset as needed.\nI find it very useful to keep track of the subsets of data being used. This allows for sanity checking but also enables a visual overview of any data that may have been ‘lost’ along the way. I routinely use {diagrammeR} for this purpose which gives me consort type diagrams of my dataset flows.\nThe other aspect is to examine the code documentation and for this I use {codeDepends} which allows me to create a flow diagram of my code (rather than the data sets). Using {diagrammeR} and {codeDepends} allows me to get an overview of my script rather than trying to debug line by line.\n\n\n{codeDepends} is no longer on CRAN but this may be the original repository https://github.com/duncantl/CodeDepends\n\nR scripts should exist with a standardised folder structure for each script\n\nR scripts often exist within a project. You may be outputting image files you want access to later, as well as needing other files. R studio maintains R scripts as a project and creates a file system around each script. There are several packages that will also create a file dependency system for a script so that at the very least the organisation around the R script is easy to navigate. There are several ways to do this and some packages exist that will set this up for you.\n\n\ne) R files should have a standard naming convention.\nThis is the most frustrating problem when developing R scripts. I have a few scripts that extract text from medical reports. I also have a few scripts that do time series analysis on patients coming to have endoscopies. And again a few that draw Circos plots of patient flows through the department. In the end that is a lot of scripts. There is a danger of creating a load of folders with names like ‘endoscopyScripts’ and ‘timeSeries’ that don’t categorise my scripts according to any particular system. The inevitable result is lost scripts and repeated development. Categorization and labelling systems are so important so you can prevent re-inventing the same script. As the entire thrust of what I do with R is in the end to develop open source packages, I choose to name scripts and their folders according to the questions I am asking. The naming convention I use is as follows\nTop level folder: Name according to question domains (defined by generic dataset)\nScript name: Defined by question in the script dataset_FinalAnalysis\nThe R developer will soon come to realise the question domains that are most frequently asked and I would suggest that this is used as the naming convention for top-level folders. I would avoid categorising files according to the method of analysis. As an example, I develop a lot of scripts for the extraction of data from endoscopies. In general I either do time series analysis on them or I do quality analysis. The questions I ask of the data are things like: ‘How many colonoscopies did we do last year’ or ‘How are all the endoscopists performing when measured by their diagnostic detection rates for colonoscopy’. I could name the files ‘endoscopyTimeSeries’ or ‘endoscopyQualityAssess’ but this mechanistic labelling doesn’t tell me much. By using question based labelling I can start to see patterns when looking over my files. According to my naming convention I should create a folder called ‘Endoscopy’ and then the script names should be ‘Colonoscopies_DxRate and ‘Colonoscopies_ByYear’. The next time I want to analyse a diagnostic rate, maybe for a different data set like gastroscopies, I can look through my scripts and see I have done a similar thing already and re-use it.\nIn the end, the role of categorizing scripts in this way allows me to see a common pattern of questions. The re-usability of already answered questions is really the whole point of scripting solutions. Furthermore it allows the deeply satisfying creation of generic solutions which can be compiled into functions and then into packages. This has already been expanded upon here.\n\n\nf) Always use a versioning system for your R scripts\nR scripts need to be versioned as scripts may change over time. A versioning system is essential to any serious attempt at providing solutions. There are two aspects to versioning. Firstly the scripts may change and secondly the packages may change. Dependency versioning can be dealt with by using checkpoints within the scripts. This essentially freezes the dependency version so that the package that worked with the current version of the script can be used. I have found this very useful for the avoidance of script errors that are out of my control.\nThe other issue is that of versioning between developers. I routinely use Github as my versioning system. This is not always available from within Trusts but there are other versioning systems that can be used in house only. Whichever is used, versioning to keep track of the latest workable scripts is essentially to prevent chaos and insanity from descending into the development environment. A further plea for open source versioning is the greater aspiration of opening up R scripts in healthcare to the wider public so that everyone can have a go at developing solutions and perhaps we can start to network solutions between trusts.\n\n\nConclusion:\nThere is a greater aim here, which I hope to expand on in a later article, which is the development of a solutions network. In all healthcare trusts, even outside of the UK, we have similar questions, albeit with different datasets. The building blocks I have outlined above are really a way of standardising in-house development using R so that we can start to share solutions between trusts and understand each other’s solutions. A major step forward in sharing solutions would be to develop a way of sharing (and by necessity categorising) the questions we have in each of our departments. Such a network of solution sharing in the NHS (and beyond) would require a CRAN (or rOpenSci) type pathway of open source building, and validation as well as standardised documentation but once this is done it would represent a step change in analytics in the NHS. The steps I have shared above certainly help me in developing solutions and certainly help in the re-use of what I have already built rather than re-creating solutions from scratch.\nThis blog was written by Sebastian Zeki, Consultant Gastroenterologist at Guy’s and St Thomas’ NHS Foundation Trust.\nThis blog has been formatted to remove Latin Abbreviations.\n\n\n\n\n Back to topReuseCC0CitationFor attribution, please cite this work as:\nZeki, Sebastian. 2019. “Roadmap to Collaborative Working Using R\nin the NHS: Part I- Workflows.” January 7, 2019. https://nhs-r-community.github.io/nhs-r-community//blog/roadmap-to-collaborative-working-using-r-in-the-nhs-part-i-workflows.html."
  },
  {
    "objectID": "blog/nhs-r-conference-was-it-worth-it.html",
    "href": "blog/nhs-r-conference-was-it-worth-it.html",
    "title": "NHS-R Conference: was it worth it?",
    "section": "",
    "text": "Soon after I started in this role I was offered several free places to a conference on Big Data. It was aimed at Healthcare and with Big Data in the title it sounded very exciting. Sadly, like many things around ‘Big Data’ it was baffling. There were a few good, high-level, presentations but it turned out the conference was funded by private companies and they had bought the opportunity to showcase their wares. I didn’t learn anything practical and I would be hard pressed to recall anything from the event now. All in all, it was a disappointment.\nTime passed and I tentatively started messing around with R. Not being someone who can learn well from books and videos, I learn better from someone telling me what they’ve written, I went to a couple of free R training events, one of which was run by NHS-R. At that training there was talk of an NHS-R conference and I couldn’t help but get that familiar tingle of excitement. Yet, in the back of my head now was the nagging feeling that it might be like that Big Data conference. After all, these things are not easy to set up, I was still overwhelmed by what R could do and still just learning and, well, how good could it really be?\nOptimism won out, along with the fact that the conference was free and a few colleagues were going so I could get a lift. I had nothing to lose. I didn’t even have to build a case to go for my managers let alone myself. At worst I could expect a day out and a free coffee, maybe a biscuit. It was settled. I would even take my own lunch.\nIt’s been a fair number of weeks since the event, which is a pretty good time to evaluate something like this. I always find that there is a buzz after any conference or meeting. The buzz of course can be as much negative as positive energy so it’s best to wait a few days before rviewing, I think. Plus, I’m not that used to writing down my thoughts. After all of my doubts then, did it live up to my vague and overly excitable expectations? Oh yes, it did. In buckets.\nI’ve worked with some intelligent and creative people in the NHS and it turns out these weren’t isolated pockets of great people doing great work. The NHS, and other organisations close to it, are chock full of these people. We had speakers from Public Health, universities, foundations, private consultancies, Acute and Mental Health Trusts. In fact, I know it’s called NHS-R but it’s not an exclusive club and nor should it be.\nWe were shown finished pieces of work in R as well as the code itself which I really enjoyed. I’m an analyst and I like to see the logic and the code as that’s where I work – in the detail. Best of all though was the networking. I met a number of people I knew either from courses or from their blogs/R packages. Unlike the ‘Big Data’ conference where I had to track someone down on LinkedIn to ask about one of the presentations, and sadly heard nothing, I have contacted some of the speakers through Twitter and email and had conversations. There is a sense that everyone is in this together and that competitive edge you sometimes get in IT is notably missing.\nThe other great thing about this conference was that it was free and that should not be underestimated. Being free, it attracted those non-management, just starting out in R, not even statisticians and only did maths to GSCE level people like me. The NHS, like many other public-sector organisations, are feeling the pinch and more than ever we need support and training for the hidden side of the NHS – its data people. And it’s those people I hope you will see at the next NHS-R conference showing their achievements and sharing their enthusiasm, because, whilst this was a training/conference event, really it was the coming together of a community.\n\n\n\n Back to topReuseCC0CitationFor attribution, please cite this work as:\nTurner, Zoë. 2018. “NHS-R Conference: Was It Worth It?”\nOctober 26, 2018. https://nhs-r-community.github.io/nhs-r-community//blog/nhs-r-conference-was-it-worth-it.html."
  },
  {
    "objectID": "blog/nhs-open-source-public-datasets.html",
    "href": "blog/nhs-open-source-public-datasets.html",
    "title": "NHS open source public datasets – creating realistic synthetic datasets",
    "section": "",
    "text": "This blog is an initial attempt to garner interest in a project to create NHS synthetic datasets in a range of fields and also to understand the underlying principles around creating synthetic healthcare data and its intricacies.\nHealthcare data is increasingly electronic. With the huge datasets that the NHS is in the process of collecting for patient care comes an impetus to standardise data collection between hospitals and trusts. This is good for patient care not least because it allows for standardised analysis of large datasets. This standardisation is in progress across many of the medical specialities but several have already reached a mature stage. A good example is the National Endoscopy Database which aims to automatically collect data from all endoscopic procedures in the UK according to a standardised template. This will allow for the analysis of variation in endoscopic performance, quality and outcomes amongst many other outputs.\nThe analysis of these datasets will need to be validated, reproducible and of course progressive as the desired metrics change. The analyses will therefore require two things\nOngoing input from analysts to maintain the methodology and code base to perform the analysis.\nCreative ideas for the representation of the datasets.\nHS datasets and the methods for their manipulation are likely to attract a lot of interest from diverse sources such as pharmaceutical companies to healthcare software developers and academic researchers. By restricting access to the datasets because of privacy issues we also, by necessity restrict the speed at which solutions can be found using these datasets as analysis will only be carried out by a small sample of authorised analysts\nThe obvious solution here is to create NHS datasets which are constructed according to accepted and used data templates, but to populate them with synthetic data and then to allow open source access to these datasets.\nSynthetic data is not always easy to create. The vast majority of NHS electronic data is still semi-structured free text. Reports also have to make sense internally so that, for example, an endoscopy report describing a stomach ulcer has to contain text that is relevant to the ulcer finding. It gets even more complex when further reports are written that reference a report in another dataset. An example is the histopathology report from a biopsy taken from the stomach ulcer. This biopsy report will obviously have to be reporting on the stomach ulcer and the text will be about pathology findings relevant to the description of an ulcer.\nAn example of an attempt at creating a synthetic medical dataset can be found here using the above example (https://github.com/sebastiz/FakeEndoReports). This contains some description of how the reports are created and I have tried to derive some principles regarding how to make fake healthcare datasets in general based on this example.\nThere are of course many other datasets that would be incredibly powerful if they were created an open sourced. One such datasets is the NHS patient administration system on which most statistics about waiting times and patient pathways are based. Another is the Hospital Episode Statistics (HES) which collect information regarding all NHS appointments (in-patient and outpatient) and which is being used to create data linkage between a wide range of data repositories (My attempt at creating synthetic HES data can be found here but is still incomplete at the moment: https://github.com/sebastiz/HesMineR)\nR is the perfect language to create such synthetic datasets and it would be a valuable addition to the NHS-R armamentarium to have a package that contained synthetic NHS datasets so that open source solutions can be more quickly and creatively derived.\n\n\n\n Back to topReuseCC0CitationFor attribution, please cite this work as:\nZeki, Sebastian. 2018. “NHS Open Source Public Datasets – Creating\nRealistic Synthetic Datasets.” August 16, 2018. https://nhs-r-community.github.io/nhs-r-community//blog/nhs-open-source-public-datasets.html."
  },
  {
    "objectID": "blog/navigating-the-data-driven-frontier-insights-from-an-nhs-r-committee-member.html",
    "href": "blog/navigating-the-data-driven-frontier-insights-from-an-nhs-r-committee-member.html",
    "title": "Navigating the Data-Driven Frontier: Insights from an NHS-R Committee Member",
    "section": "",
    "text": "Greetings, esteemed members of the NHS-R Community, data enthusiasts, and healthcare aficionados! It is with great excitement that I share my recent journey as a committee member with the NHS-R Community, a vibrant hub dedicated to championing the use of R and data science tools in the UK health and care system."
  },
  {
    "objectID": "blog/navigating-the-data-driven-frontier-insights-from-an-nhs-r-committee-member.html#the-community",
    "href": "blog/navigating-the-data-driven-frontier-insights-from-an-nhs-r-committee-member.html#the-community",
    "title": "Navigating the Data-Driven Frontier: Insights from an NHS-R Committee Member",
    "section": "The Community",
    "text": "The Community\nEstablished in 2018, the NHS-R Community has rapidly evolved into a dynamic collective of professionals from diverse backgrounds. Our community encompasses members from public sector organizations, including Local Authorities and Civil Service, academia, and the voluntary sector. We are united by a shared passion for advancing healthcare through the transformative power of data. While our core language is R, we are equally enthusiastic about embracing a broad spectrum of data science tools, recognizing their integral role in shaping the future of healthcare analytics."
  },
  {
    "objectID": "blog/navigating-the-data-driven-frontier-insights-from-an-nhs-r-committee-member.html#steering-the-course-the-committees-impact",
    "href": "blog/navigating-the-data-driven-frontier-insights-from-an-nhs-r-committee-member.html#steering-the-course-the-committees-impact",
    "title": "Navigating the Data-Driven Frontier: Insights from an NHS-R Committee Member",
    "section": "Steering the Course: The Committee’s Impact",
    "text": "Steering the Course: The Committee’s Impact\nAs a committee member, my role involves active participation in shaping the community’s approach, focus, and overall strategy. We meet regularly to deliberate on initiatives, conferences, and the overarching direction for the community. The committee serves as a crucial forum for collective decision-making, ensuring that our initiatives align with the evolving needs of the community and contribute to positive change."
  },
  {
    "objectID": "blog/navigating-the-data-driven-frontier-insights-from-an-nhs-r-committee-member.html#committee-meetings-a-glimpse-into-our-world",
    "href": "blog/navigating-the-data-driven-frontier-insights-from-an-nhs-r-committee-member.html#committee-meetings-a-glimpse-into-our-world",
    "title": "Navigating the Data-Driven Frontier: Insights from an NHS-R Committee Member",
    "section": "Committee Meetings: A Glimpse into Our World",
    "text": "Committee Meetings: A Glimpse into Our World\nOur meetings are a lively exchange of ideas, experiences, and visions for the future. Whether we’re brainstorming innovative projects, refining strategies, or planning impactful conferences, each committee member brings a unique perspective to the table. The collaborative spirit is palpable as we navigate the dynamic landscape of data science in healthcare."
  },
  {
    "objectID": "blog/navigating-the-data-driven-frontier-insights-from-an-nhs-r-committee-member.html#driving-positive-change",
    "href": "blog/navigating-the-data-driven-frontier-insights-from-an-nhs-r-committee-member.html#driving-positive-change",
    "title": "Navigating the Data-Driven Frontier: Insights from an NHS-R Committee Member",
    "section": "Driving Positive Change",
    "text": "Driving Positive Change\nBeing part of the committee means playing a pivotal role in fostering collaboration and steering the community toward meaningful outcomes. We celebrate diversity in thought and expertise, recognizing that it is the key to unlocking the full potential of data science in healthcare."
  },
  {
    "objectID": "blog/navigating-the-data-driven-frontier-insights-from-an-nhs-r-committee-member.html#contributions-welcome",
    "href": "blog/navigating-the-data-driven-frontier-insights-from-an-nhs-r-committee-member.html#contributions-welcome",
    "title": "Navigating the Data-Driven Frontier: Insights from an NHS-R Committee Member",
    "section": "Contributions Welcome",
    "text": "Contributions Welcome\nOne of the hallmarks of our community is the open invitation for contributions. Our GitHub repository: NHS-R Community hosts a wealth of resources, including packages and training materials, and we wholeheartedly welcome contributions from community members. It’s an empowering experience to witness the collective impact of our shared knowledge and skills."
  },
  {
    "objectID": "blog/navigating-the-data-driven-frontier-insights-from-an-nhs-r-committee-member.html#join-the-conversation",
    "href": "blog/navigating-the-data-driven-frontier-insights-from-an-nhs-r-committee-member.html#join-the-conversation",
    "title": "Navigating the Data-Driven Frontier: Insights from an NHS-R Committee Member",
    "section": "Join the Conversation",
    "text": "Join the Conversation\nIf you’re passionate about data science, healthcare, and making a positive impact, the NHS-R Community is the place to be. Whether you’re an R enthusiast, a data science wizard, or a healthcare professional with an interest in analytics, there’s a place for you in our diverse and welcoming community.\nAs I continue my journey as a committee member, I’m excited about the endless possibilities that lie ahead. Together, we’re shaping the future of healthcare analytics, one R script at a time.\nStay tuned for more updates from the NHS-R Community, where data meets healthcare, and innovation knows no bounds. See you in the data-driven frontier!"
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Blogs",
    "section": "",
    "text": "Recoding an NA and back again\n\n\n“Replace NAs with replace_na() and code to NA with na_if()” \n\n\n\nR tips\n\n\n\n\n\n\nMar 12, 2024\n\n\nZoë Turner\n\n\n2 min\n\n\n304 words\n\n\n\n\n\n\n\n\n\n\n\n\nShowcasing a function - separate()\n\n\n“Separating codes from one cell couldn’t be any easier in R” \n\n\n\nR tip\n\n\n\n\n\n\nMar 12, 2024\n\n\nZoë Turner\n\n\n1 min\n\n\n145 words\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding a Quarto website for NHS-R Community\n\n\n“Delving into the blog history of NHS-R Community as the site moves into the future with Quarto” \n\n\n\nReflections\n\n\n\n\n\n\nFeb 24, 2024\n\n\nZoë Turner\n\n\n7 min\n\n\n1,218 words\n\n\n\n\n\n\n\n\n\n\n\n\nNavigating the Data-Driven Frontier: Insights from an NHS-R Committee Member\n\n\n‘It is with great excitement that I share my recent journey as a committee member with the NHS-R Community, a vibrant hub dedicated to championing the use of R and data science tools in the UK health and care system.’ \n\n\n\nCommittee\n\n\n\n\n\n\nFeb 6, 2024\n\n\nPrajwal Khairnar\n\n\n3 min\n\n\n466 words\n\n\n\n\n\n\n\n\n\n\n\n\nSuccess story - Kate Cheema, Director of Health Intelligence, British Heart Foundation\n\n\n“Success story” \n\n\n\nReflections\n\n\n\n\n\n\nMar 21, 2022\n\n\nKate Cheema\n\n\n3 min\n\n\n550 words\n\n\n\n\n\n\n\n\n\n\n\n\nSuccess story - Dr Lydia Briggs, Data Scientist, GOSH\n\n\n“Success story” \n\n\n\nReflections\n\n\n\n\n\n\nMar 21, 2022\n\n\nLydia Briggs\n\n\n2 min\n\n\n261 words\n\n\n\n\n\n\n\n\n\n\n\n\nSuccess story - Dr William Bryant, Senior Data Scientist, GOSH\n\n\n“Success story” \n\n\n\nReflections\n\n\n\n\n\n\nMar 21, 2022\n\n\nWilliam Bryant\n\n\n1 min\n\n\n157 words\n\n\n\n\n\n\n\n\n\n\n\n\nCount of working days function\n\n\nCreating a function to work out the number of working days a seasonal bus ticket can be used. \n\n\n\nFunctions\n\n\n\n\n\n\nJul 16, 2019\n\n\nZoë Turner\n\n\n7 min\n\n\n1,212 words\n\n\n\n\n\n\n\n\n\n\n\n\nA run chart is not a run chart is not a run chart\n\n\n“Understanding a run chart and the {qicharts2} package.” \n\n\n\nRun charts\n\n\n\n\n\n\nFeb 26, 2019\n\n\nJacob Anhøj\n\n\n18 min\n\n\n3,519 words\n\n\n\n\n\n\n\n\n\n\n\n\nA simple function to create nice correlation plots\n\n\n“Hacking a visualisation function and creating a new one.” \n\n\n\nFunctions\n\n\nStatistics\n\n\n\n\n\n\nJan 31, 2019\n\n\nGary Hutson\n\n\n11 min\n\n\n2,110 words\n\n\n\n\n\n\n\n\n\n\n\n\nFrom script-based development to function-based development and onwards to Package Based development: part 2\n\n\n“Building a package.” \n\n\n\nPackages\n\n\nFunctions\n\n\n\n\n\n\nJan 7, 2019\n\n\nAndrew Hill\n\n\n9 min\n\n\n1,613 words\n\n\n\n\n\n\n\n\n\n\n\n\nRoadmap to collaborative working using R in the NHS: Part I- Workflows\n\n\n“We finally have a tool that is data independent. This means that the door to working collaboratively on solutions is now wide open.” \n\n\n\nWorkflow\n\n\n\n\n\n\nJan 7, 2019\n\n\nSebastian Zeki\n\n\n11 min\n\n\n2,005 words\n\n\n\n\n\n\n\n\n\n\n\n\nBut this worked the last time I ran it!\n\n\n“Debugging when your code worked and now doesn’t.” \n\n\n\nPackages\n\n\nDebugging\n\n\n\n\n\n\nDec 20, 2018\n\n\nZoë Turner\n\n\n5 min\n\n\n989 words\n\n\n\n\n\n\n\n\n\n\n\n\nLocal Public Health joins the paRty\n\n\n“Setting up an R group for Publich Health Intelligence” \n\n\n\nReflections\n\n\n\n\n\n\nNov 22, 2018\n\n\nAndy Evans\n\n\n3 min\n\n\n525 words\n\n\n\n\n\n\n\n\n\n\n\n\nThoughts on the NHS-R conference\n\n\n“Reflections on the 2018 Conference” \n\n\n\nConference\n\n\n\n\n\n\nNov 15, 2018\n\n\nJohn MacKintosh\n\n\n3 min\n\n\n457 words\n\n\n\n\n\n\n\n\n\n\n\n\nNHS-R Conference: was it worth it?\n\n\n“Reflections on the 2018 Conference” \n\n\n\nConference\n\n\n\n\n\n\nOct 26, 2018\n\n\nZoë Turner\n\n\n4 min\n\n\n681 words\n\n\n\n\n\n\n\n\n\n\n\n\nText Mining – Term Frequency analysis and Word Cloud creation in R\n\n\n“Analysing the pre-conference workshop sentiments” \n\n\n\nText Mining\n\n\nConference\n\n\n\n\n\n\nOct 22, 2018\n\n\nGary Hutson\n\n\n7 min\n\n\n1,291 words\n\n\n\n\n\n\n\n\n\n\n\n\nNHS-R Community Conference: My experience of the day\n\n\n“Reflections on the 2018 Conference” \n\n\n\nConference\n\n\n\n\n\n\nOct 16, 2018\n\n\nNeil Pettinger\n\n\n4 min\n\n\n742 words\n\n\n\n\n\n\n\n\n\n\n\n\nFrom script-based development to function-based development and onwards to Package Based development\n\n\n“How to build a function.” \n\n\n\nFunctions\n\n\n\n\n\n\nOct 15, 2018\n\n\nAndrew Hill\n\n\n6 min\n\n\n1,175 words\n\n\n\n\n\n\n\n\n\n\n\n\nA simple function to install and load packages in R\n\n\n“A function to install packages if needed and load at the same time.” \n\n\n\nPackages\n\n\n\n\n\n\nAug 17, 2018\n\n\nGary Hutson\n\n\n2 min\n\n\n388 words\n\n\n\n\n\n\n\n\n\n\n\n\nNHS open source public datasets – creating realistic synthetic datasets\n\n\n“Syntethic or dummy data sets in the NHS” \n\n\n\nSynthetic dataset\n\n\n\n\n\n\nAug 16, 2018\n\n\nSebastian Zeki\n\n\n3 min\n\n\n588 words\n\n\n\n\n\n\n\n\n\n\n\n\nHow to extrapolate data from data\n\n\n“Extracting data using {grepl} and {stringr}” \n\n\n\nStrings\n\n\n\n\n\n\nJul 23, 2018\n\n\nSebastian Zeki\n\n\n3 min\n\n\n552 words\n\n\n\n\n\n\n\n\n\n\n\n\nEvolution of the R user\n\n\n“The stages of learning R (with excitement factor ratings)” \n\n\n\nReflections\n\n\n\n\n\n\nJul 13, 2018\n\n\nSeb Fox, Julian Flowers and Georgina Anderson, Public Health England.\n\n\n6 min\n\n\n1,118 words\n\n\n\n\n\n\n\n\n\n\n\n\nSimpler SQL with dplyr\n\n\n“Comparing dplyr with SQL nested queries” \n\n\n\nR tips\n\n\nSQL\n\n\ndplyr\n\n\nPatient Flow\n\n\n\n\n\n\nJun 7, 2018\n\n\nJohn MacKintosh\n\n\n6 min\n\n\n1,129 words\n\n\n\n\n\n\n\n\n\n\n\n\nDiverging Bar Charts – Plotting Variance with ggplot2\n\n\n“The aim here is to create a diverging bar chart that shows variance above and below an average line.” \n\n\n\nR tips\n\n\nggplot2\n\n\nBase R\n\n\n\n\n\n\nMay 24, 2018\n\n\nGary Hutson\n\n\n5 min\n\n\n936 words\n\n\n\n\n\n\n\n\n\n\n\n\nDiverging Dot Plot and Lollipop Charts – Plotting Variance with ggplot2\n\n\n“The aim here is to create a diverging bar chart that shows variance above and below an average line.” \n\n\n\nR tips\n\n\nggplot2\n\n\nBase R\n\n\n\n\n\n\nMay 24, 2018\n\n\nGary Hutson\n\n\n4 min\n\n\n669 words\n\n\n\n\n\n\n\n\n\n\n\n\nHistogram with auto binning in ggplot2\n\n\n“Building the Histogram with auto binning” \n\n\n\nR tips\n\n\nggplot2\n\n\nBase R\n\n\n\n\n\n\nMay 24, 2018\n\n\nGary Hutson\n\n\n3 min\n\n\n497 words\n\n\n\n\n\n\n\n\n\n\n\n\nWhy Government needs sustainable software too\n\n\n“In this blog post I want to talk about why Government needs sustainable software, the work being done to deliver it, and the lessons we learnt after the first year.” \n\n\n\nSoftware\n\n\n\n\n\n\nMay 24, 2018\n\n\nMatthew Upson\n\n\n10 min\n\n\n1,914 words\n\n\n\n\n\n\n\n\n\n\n\n\nR studio shortcuts\n\n\n“There is a full list of short cuts here and I have pulled together my three most used shortcuts.” \n\n\n\nR tips\n\n\n\n\n\n\nMay 21, 2018\n\n\nEmma Vestesson\n\n\n1 min\n\n\n129 words\n\n\n\n\n\n\n\n\n\n\n\n\nThe :: operator\n\n\n“Sometimes two packages will have a function with the same name but they will do different things.” \n\n\n\nR tips\n\n\n\n\n\n\nMay 21, 2018\n\n\nEmma Vestesson\n\n\n3 min\n\n\n405 words\n\n\n\n\n\n\n\n\n\n\n\n\nImporting and exporting Data\n\n\n‘There are a large number of file types that are able to store data. R is usually able to import most of them but there are some caveats.’ \n\n\n\nR tips\n\n\n\n\n\n\nMay 15, 2018\n\n\nS Zeki\n\n\n1 min\n\n\n196 words\n\n\n\n\n\n\n\n\n\n\n\n\nThe joy of R\n\n\n‘Hello. My name is Julian and I am an R addict. I got hooked about 3 years ago when I took on a new role in Public Health England developing a public health data science team.’ \n\n\n\nReflections\n\n\n\n\n\n\nApr 9, 2018\n\n\nJulian Flowers\n\n\n5 min\n\n\n830 words\n\n\n\n\n\n\n\n\n\n\n\n\nAiming for a wrangle-free (or reduced) world\n\n\n‘I work as a Data Scientist at Public Health England. I am part of a small team that have a role in trying to modernise how we “do” data.’ \n\n\n\nReflections\n\n\n\n\n\n\nMar 23, 2018\n\n\nSebastian Fox\n\n\n6 min\n\n\n1,193 words\n\n\n\n\n\n\n\n\n\n\n\n\nNHS meets R\n\n\n‘Hello and welcome to our nascent NHS-R Community; a community dedicated to promoting the learning, application and utilisation of R in the National Health Service (NHS) in the United Kingdom.’ \n\n\n\nNHS-R\n\n\n\n\n\n\nMar 19, 2018\n\n\n3 min\n\n\n533 words\n\n\n\n\n\n\nNo matching items\n\n\n  \n\n Back to topReuseCC0"
  },
  {
    "objectID": "blog/how-to-extrapolate-data-from-data.html",
    "href": "blog/how-to-extrapolate-data-from-data.html",
    "title": "How to extrapolate data from data",
    "section": "",
    "text": "There are many occasions when a column of data needs to be created from an already existing column for ease of data manipulation. For example, perhaps you have a body of text as a pathology report and you want to extract all the reports where the diagnosis is ‘dysplasia’.\nYou could just subset the data using grepl so that you only get the reports that mention this word…but what if the data needs to be cleaned prior to subsetting like excluding reports where the diagnosis is normal but the phrase ‘No evidence of dysplasia’ is present. Or perhaps there are other manipulations needed prior to subsetting.\nThis is where data accordionisation is useful. This simply means the creation of data from (usually) a column into another column in the same dataframe.\nThe neatest way to do this is with the mutate function from the {dplyr} package which is devoted to data cleaning. There are also other ways which I will demonstrate at the end.\nThe input data here will be an endoscopy data set:\n\nAge &lt;- sample(1:100, 130, replace = TRUE)\nDx &lt;- sample(c(\"NDBE\", \"LGD\", \"HGD\", \"IMC\"), 130, replace = TRUE)\nTimeOfEndoscopy &lt;- sample(1:60, 130, replace = TRUE)\n\nlibrary(dplyr)\n\nEMRdf &lt;- data.frame(Age, Dx, TimeOfEndoscopy, stringsAsFactors = F)\n\nPerhaps you need to calculate the number of hours spent doing each endoscopy rather than the number of minutes\n\nEMRdftbb &lt;- EMRdf %&gt;% mutate(TimeOfEndoscopy / 60)\n\n# install.packages(\"knitr\")\nlibrary(knitr)\nlibrary(kableExtra)\n\n# Just show the top 20 results\n\nkable(head(EMRdftbb, 20))\n\n\n\nAge\nDx\nTimeOfEndoscopy\nTimeOfEndoscopy/60\n\n\n\n87\nNDBE\n24\n0.4000000\n\n\n41\nNDBE\n48\n0.8000000\n\n\n15\nLGD\n40\n0.6666667\n\n\n16\nHGD\n19\n0.3166667\n\n\n38\nIMC\n22\n0.3666667\n\n\n87\nIMC\n52\n0.8666667\n\n\n29\nHGD\n55\n0.9166667\n\n\n20\nIMC\n45\n0.7500000\n\n\n83\nNDBE\n45\n0.7500000\n\n\n97\nHGD\n22\n0.3666667\n\n\n17\nNDBE\n50\n0.8333333\n\n\n81\nNDBE\n16\n0.2666667\n\n\n39\nNDBE\n40\n0.6666667\n\n\n43\nHGD\n34\n0.5666667\n\n\n12\nIMC\n56\n0.9333333\n\n\n62\nNDBE\n11\n0.1833333\n\n\n68\nLGD\n51\n0.8500000\n\n\n37\nLGD\n35\n0.5833333\n\n\n92\nLGD\n7\n0.1166667\n\n\n38\nIMC\n57\n0.9500000\n\n\n\n\n\nThat is useful but what if you want to classify the amount of time spent doing each endoscopy as follows: &lt;0.4 hours is too little time and &gt;0.4 hours is too long.\nUsing ifelse() with mutate for conditional accordionisation.\nFor this we would use ifelse(). However this can be combined with mutate() so that the result gets put in another column as follows\n\nEMRdf2 &lt;- EMRdf %&gt;%\n  mutate(TimeInHours = TimeOfEndoscopy / 60) %&gt;%\n  mutate(TimeClassification = ifelse(TimeInHours &gt; 0.4, \"Too Long\", \"Too Short\"))\n\n# Just show the top 20 results\n\nkable(head(EMRdf2, 20))\n\n\n\nAge\nDx\nTimeOfEndoscopy\nTimeInHours\nTimeClassification\n\n\n\n87\nNDBE\n24\n0.4000000\nToo Short\n\n\n41\nNDBE\n48\n0.8000000\nToo Long\n\n\n15\nLGD\n40\n0.6666667\nToo Long\n\n\n16\nHGD\n19\n0.3166667\nToo Short\n\n\n38\nIMC\n22\n0.3666667\nToo Short\n\n\n87\nIMC\n52\n0.8666667\nToo Long\n\n\n29\nHGD\n55\n0.9166667\nToo Long\n\n\n20\nIMC\n45\n0.7500000\nToo Long\n\n\n83\nNDBE\n45\n0.7500000\nToo Long\n\n\n97\nHGD\n22\n0.3666667\nToo Short\n\n\n17\nNDBE\n50\n0.8333333\nToo Long\n\n\n81\nNDBE\n16\n0.2666667\nToo Short\n\n\n39\nNDBE\n40\n0.6666667\nToo Long\n\n\n43\nHGD\n34\n0.5666667\nToo Long\n\n\n12\nIMC\n56\n0.9333333\nToo Long\n\n\n62\nNDBE\n11\n0.1833333\nToo Short\n\n\n68\nLGD\n51\n0.8500000\nToo Long\n\n\n37\nLGD\n35\n0.5833333\nToo Long\n\n\n92\nLGD\n7\n0.1166667\nToo Short\n\n\n38\nIMC\n57\n0.9500000\nToo Long\n\n\n\n\n\nNote how we can chain the mutate() function together.\nUsing multiple ifelse()\nWhat if we want to get more complex and put several classifiers in? We just use more ifelse’s:\n\nEMRdf2 &lt;- EMRdf %&gt;%\n  mutate(TimeInHours = TimeOfEndoscopy / 60) %&gt;%\n  mutate(TimeClassification = ifelse(TimeInHours &gt; 0.8, \"Too Long\", ifelse(TimeInHours &lt; 0.5, \"Too Short\", ifelse(TimeInHours &gt;= 0.5 & TimeInHours &lt;= 0.8, \"Just Right\", \"N\"))))\n\n# Just show the top 20 results\n\nkable(head(EMRdf2, 20))\n\n\n\nAge\nDx\nTimeOfEndoscopy\nTimeInHours\nTimeClassification\n\n\n\n87\nNDBE\n24\n0.4000000\nToo Short\n\n\n41\nNDBE\n48\n0.8000000\nJust Right\n\n\n15\nLGD\n40\n0.6666667\nJust Right\n\n\n16\nHGD\n19\n0.3166667\nToo Short\n\n\n38\nIMC\n22\n0.3666667\nToo Short\n\n\n87\nIMC\n52\n0.8666667\nToo Long\n\n\n29\nHGD\n55\n0.9166667\nToo Long\n\n\n20\nIMC\n45\n0.7500000\nJust Right\n\n\n83\nNDBE\n45\n0.7500000\nJust Right\n\n\n97\nHGD\n22\n0.3666667\nToo Short\n\n\n17\nNDBE\n50\n0.8333333\nToo Long\n\n\n81\nNDBE\n16\n0.2666667\nToo Short\n\n\n39\nNDBE\n40\n0.6666667\nJust Right\n\n\n43\nHGD\n34\n0.5666667\nJust Right\n\n\n12\nIMC\n56\n0.9333333\nToo Long\n\n\n62\nNDBE\n11\n0.1833333\nToo Short\n\n\n68\nLGD\n51\n0.8500000\nToo Long\n\n\n37\nLGD\n35\n0.5833333\nJust Right\n\n\n92\nLGD\n7\n0.1166667\nToo Short\n\n\n38\nIMC\n57\n0.9500000\nToo Long\n\n\n\n\n\nUsing multiple ifelse() with grepl() or string_extract\nOf course we need to extract information from text as well as numeric data. We can do this using grepl() or string_extract() from the library(stringr).\nLet’s say we want to extract all the samples that had IMC. We don’t want to subset the data, just extract IMC into a column that says IMC and the rest say ’Non-IMC’\nUsing the dataset above:\n\nlibrary(stringr)\n\nEMRdf$MyIMC_Column &lt;- str_extract(EMRdf$Dx, \"IMC\")\n\n# to fill the NA's we would do:EMRdf$MyIMC_Column&lt;-ifelse(grepl(\"IMC\",EMRdf$Dx),\"IMC\",\"NoIMC\")\n\n# Another way to do this (really should be for more complex examples when you want to extract the entire contents of the cell that has the match)\n\nEMRdf$MyIMC_Column &lt;- ifelse(grepl(\"IMC\", EMRdf$Dx), str_extract(EMRdf$Dx, \"IMC\"), \"NoIMC\")\n\nSo data can be usefully created from data for further analysis.\nHopefully this way of extrapolating data and especially using conditional expressions to categorise data according to some rules is a helpful way to get more out of your data.\nPlease follow @gastroDS on twitter\nThis article originally appeared on https://sebastiz.github.io/gastrodatascience/ and has been edited to render in Quarto and had NHS-R styles applied.\n\n\n\n Back to topReuseCC0CitationFor attribution, please cite this work as:\nZeki, Sebastian. 2018. “How to Extrapolate Data from Data.”\nJuly 23, 2018. https://nhs-r-community.github.io/nhs-r-community//blog/how-to-extrapolate-data-from-data.html."
  },
  {
    "objectID": "blog/from-script-based-development-to-function-based-development-and-onwards-to-package-based-development.html",
    "href": "blog/from-script-based-development-to-function-based-development-and-onwards-to-package-based-development.html",
    "title": "From script-based development to function-based development and onwards to Package Based development",
    "section": "",
    "text": "At the NHS R Conference, I suggested to people that they should embrace the idea of package-based development rather than script-based work.\nI’m going to talk you through that process, using the simplest of scripts – ‘Hello World’. I’m going to assume that you’re using the freely available RStudio Desktop Edition as the editor for this: other versions of RStudio are likely to be essentially identical. Non R-Studio users may need to revert to more basic principles.\nFirst let’s write ‘Hello World’ – the simplest R script in the world. Open a new R script file and get your script underway:\n\nmessage(\"Hello World from the NHS-R Community!\")\n\nSave it (for posterity).\nIn the conference, we discussed that generally writing functions is more helpful than writing scripts as it gives you greater re-usability. This example is a very trivial one (in fact so trivial as to be nearly redundant).\nSo we consider our script carefully and determine – what does it DO? Clearly it’s a way of greeting a person. What if we wanted to greet the person in a different way? What if we wanted to greet a different person?\nSo we have defined it’s purpose, and the parameters that are likely to be useful to others.\nLet’s re-write our script to be more useable.\nWe define a function using the function function. You can see a much more detailed tutorial of how to do this here: http://stat545.com/block011_write-your-own-function-01.html.\nA function is defined by assigning the result of the function() function to a variable which is the function name. The parameters of function() are our new parameter names in our function.\nIt is really important to name your function clearly so people know what it does. Generally use active verbs to describe intent, and a consistent naming scheme. Also choose appropriate and clear names for the parameters. So let’s call our new function greet_person, and we will call our parameters greeting and recipient.\nOur new code will look like this. Stick this into a new R script for now and run it:\n\ngreet_person &lt;- function(greeting, sender) {\n  message(greeting, \" from \", sender)\n}\n\nOnce you’ve run your script you can now call your function from the console:\ngreet_person(“Hello World”, “the NHS-R Community!”) And of course if you want to use a different greeting we can now change our parameter value:\ngreet_person(“Welcome”, “the NHS-R Community!”) So far so good.\nBut – we’ve had to repeat our sender parameter. What if we know we’re usually going to use that first Hello World greeting; but we just want the option of doing something different if the situation arises?\nWe can get around that by supplying default values. In the function() function we can set a value to both greeting and sender using =. Let’s set default values for greet_person:\n\ngreet_person &lt;- function(greeting = \"Hello World\", sender = \"the NHS-R Community!\") {\n  message(greeting, \" from \", sender)\n}\n\nNow if you want our ‘default’ message you can just call:\n\ngreet_person()\n\nBut you can customise either parameter without having to specify anything you don’t want to change:\n\ngreet_person(sender = \"Drew Hill\")\n\nInstead of “Drew Hill” from our previous example, you’ll see the sender is “1”.\nWhat if you accidentally sent a vector of names? R will turn those into a concatenated string of names without spaces:\n\ngreet_person(sender = c(\"Bob\", \"Jim\"))\n\nSome things however certainly could break this process – so it is really important to check that you can handle the inputs you receive within a function before trying to use them.\nThe first thing we need to do is to make sure we are dealing with something that can be turned into a character. We can check that by using the is.character function – which returns TRUE if a given value is TRUE, and FALSE if it is not something that can be turned into a character.\nIf is.character is false, we want to stop with an error:\n\ngreet_person &lt;- function(greeting = \"Hello World\", sender = \"the NHS-R Community!\") {\n  if (!is.character(greeting)) {\n    stop(\"greeting must be a string\")\n  }\n\n  if (!is.character(sender)) {\n    stop(\"sender must be a string\")\n  }\n\n\n  message(greeting, \" from \", sender)\n}\n\nWe can test how this works by using NULL as a parameter: in real life this happens quite a lot as you try to pass a variable to your new function but forget to set the variable earlier on!\n\ngreet_person(sender = NULL)\n\nError in greet_person(sender = NULL) : sender must be a string\nWe also know that our function actually isn’t very good at handling vectors of strings (that is where there is more than one name): it will simply shove them all together without spaces. However it works and is perfectly functional. So we have a design decision: do we want to allow that, or not? A third way might be to allow it but to use a warning – perhaps a little over the top in our example, but for complex examples that may make more sense. Whereas stop will halt the code and force you to fix your bugs, the warning() function lets the code continue but tells you to go back and do it better later. Let’s add a warning if there was more than one sender:\n\ngreet_person &lt;- function(greeting = \"Hello World\", sender = \"the NHS-R Community!\") {\n  if (!is.character(greeting)) {\n    stop(\"greeting must be a string\")\n  }\n\n  if (!is.character(sender)) {\n    stop(\"sender must be a string\")\n  }\n\n\n  if (length(sender) &gt; 1) {\n    warning(\"greet_person isn't very good at handling more than one sender. It is better to use just one sender at a time.\")\n  }\n\n  message(greeting, \" from \", sender)\n}\n\nIf we now called the function with two senders we’d be able to do so but would get politely told that it’s not a good idea:\n\ngreet_person(sender = c(\"Jim\", \"Bob\"))\n\nSo – hopefully from this you’ve moved from having a script which would only do precisely what you wanted in a single set of circumstances, to now having a natty little function which will say greet whoever you want, with the type of greeting that you want.\nAs an exercise to complete: imagine you work in the NHS-R community welcoming team. You are tasked with sending greetings from the team on a regular basis.\nYou used to use a script to do this and had to remember to get the style right every time – but now you sit at your console , run your script containing your function, and greet_person() on demand.\nYour boss has come to you and urgently wants you to change your way of working. Rather than sending a greeting from the team using just a single team name, he wants you to send the individual names in the greeting from both Jim and Bob.\nHave a think about how you could change the function so that we can cope with multiple senders.\nThe greetings will continue as we then think about scaling up the NHS R Community Greetings division in our next instalment.\nThis blog was written by:\nDr. Andrew Hill\nClinical Lead for Stroke, St Helens and Knowsley Teaching Hospitals\nThis blog has been edited for NHS-R Style and to ensure running of code in Quarto.\n\n\n\n Back to topReuseCC0CitationFor attribution, please cite this work as:\nHill, Andrew. 2018. “From Script-Based Development to\nFunction-Based Development and Onwards to Package Based\nDevelopment.” October 15, 2018. https://nhs-r-community.github.io/nhs-r-community//blog/from-script-based-development-to-function-based-development-and-onwards-to-package-based-development.html."
  },
  {
    "objectID": "blog/evolution-of-the-r-user.html",
    "href": "blog/evolution-of-the-r-user.html",
    "title": "Evolution of the R user",
    "section": "",
    "text": "Image adapted from source: https://www.r-project.org/logo/"
  },
  {
    "objectID": "blog/evolution-of-the-r-user.html#projects",
    "href": "blog/evolution-of-the-r-user.html#projects",
    "title": "Evolution of the R user",
    "section": "Projects",
    "text": "Projects\nExcitement factor: 2 (they aren’t exciting but they are sooooo useful)\n\n\nScreenshot of RStudio Project wizard\n\nUseful link: https://support.rstudio.com/hc/en-us/articles/200526207-Using-Projects\nIt is difficult to share RMarkdown documents so other people can “knit” them. There are issues around working directories and imported files that are saved in a location other people can’t access. Here are some normal conversation starters for people in this situation:\n“Save these files in the same folder as the markdown document…”\nOr\n“Set your working directory to here and save this file here…”\nUsers may have heard of projects at this point but never reaped their benefits. Projects solve many of these problems.\nThere are the added benefits of being able to work on multiple projects at the same time and being able to pick up a project where a user has left off."
  },
  {
    "objectID": "blog/diverging-bar-charts-plotting-variance-with-ggplot2.html",
    "href": "blog/diverging-bar-charts-plotting-variance-with-ggplot2.html",
    "title": "Diverging Bar Charts – Plotting Variance with ggplot2",
    "section": "",
    "text": "Diverging Bar Charts\nThe aim here is to create a diverging bar chart that shows variance above and below an average line. In this example I will use Z Scores to calculate the variance, in terms of standard deviations, as a diverging bar. This example will use the mtcars stock dataset, as most of the data I deal with day-to-day is patient sensitive.\nData preparation\nThe code below sets up the plotting libraries, attaches the data and sets a theme:\n\nlibrary(ggplot2)\ntheme_set(theme_classic())\ndata(\"mtcars\") # load data\n\nNext, we will change some of the columns in the data frame and perform some calculations on the data frame:\n\nmtcars$CarBrand &lt;- rownames(mtcars) # Create new column for car brands and names\n\nAs commented, this line uses the existing mtcars data frame and uses the dollar sign notation that is add a new column, or refer to a column, to create a column name called CarBrand. Then we assign the car brand (&lt;-) with the rownames from the data frame. This is obviously predicated on there being some row names in the data frame, otherwise you would have to name the rows using rownames().\nAdding a Z Score calculation\nA Z score is a calculation which uses the x observation subtracts said observation from the mean and divides by the standard deviation. The link shows the mathematics behind this, for anyone who is interested.\nThe following code shows how we would implement this score:\n\nmtcars$mpg_z_score &lt;- round((mtcars$mpg - mean(mtcars$mpg)) / sd(mtcars$mpg), digits = 2)\n\nThe statistics behind the calculation have already been explained, but I have also used the round() function to round the results down to 2 digits.\nCreating a cut off (above/below mean)\nThe next step is to use conditional algebra (first advocated by one of my heroes George Boole) to check whether the Z score I have just created is greater or less than 0:\n\nmtcars$mpg_type &lt;- ifelse(mtcars$mpg_z_score &lt;= 0, \"below\", \"above\")\n\nThe ifelse() block looks at whether the Z Score is below 0, if so tag as below average, otherwise show this as above.\nThe next two steps are to convert the Car Brand into a unique factor and to sort by the Z Score calculations:\nNow, I have everything I need to start to compute the plot. Great stuff, so let’s get plotting.\n\nmtcars &lt;- mtcars[order(mtcars$mpg_z_score), ] # Ascending sort on Z Score\nmtcars$CarBrand &lt;- factor(mtcars$CarBrand, levels = mtcars$CarBrand)\n\nCreating the plot\nFirst, I will start with creating the base plot:\n\nggplot(mtcars, aes(x = CarBrand, y = mpg_z_score, label = mpg_z_score))\n\nHere, I pass in the mtcars data frame and set the aesthetics layer (aes) of the x axis to the brand of car (CarBrand). The y axis is the Z score I created for miles per gallon (mpg) and the label is also set to the z score.\nNext, I will add on the geom_bar geometry:\n\nggplot(mtcars, aes(x = CarBrand, y = mpg_z_score, label = mpg_z_score)) +\n  geom_bar(stat = \"identity\", aes(fill = mpg_type), width = .5)\n\nThis indicates that I need to use the mpg_z_score field by forcing the stat=\"identity\" option. If this was not added, then it would simply count the number of times the Car Brand appears as a frequency count (not what I want!). Then, I stipulate the fill type of the bar to be equal to whether the value deviates above and below 0 – remember we created a field in the data preparation stage to store whether this deviates below and above 0 and called it mpg_type. The last parameter is the width parameter to indicate the width of the bars.\nNext:\n\nggplot(mtcars, aes(x = CarBrand, y = mpg_z_score, label = mpg_z_score)) +\n  geom_bar(stat = \"identity\", aes(fill = mpg_type), width = .5) +\n  scale_fill_manual(\n    name = \"Mileage (deviation)\",\n    labels = c(\"Above Average\", \"Below Average\"),\n    values = c(\"above\" = \"#00ba38\", \"below\" = \"#0b8fd3\")\n  )\n\nI use the scale_fill_manual() ggplot option to add the name to the legend, specify the label names using the combine function and stipulate that the values that are above average need to be hex coded by the value and the below values to a different code. I have weirdly chosen blue and green as an alternative to red, as I know we have accessibility there. We are nearly there, the final step is:\n\nggplot(mtcars, aes(x = CarBrand, y = mpg_z_score, label = mpg_z_score)) +\n  geom_bar(stat = \"identity\", aes(fill = mpg_type), width = .5) +\n  scale_fill_manual(\n    name = \"Mileage (deviation)\",\n    labels = c(\"Above Average\", \"Below Average\"),\n    values = c(\"above\" = \"#00ba38\", \"below\" = \"#0b8fd3\")\n  ) +\n  labs(\n    subtitle = \"Z score (normalised) mileage for mtcars'\",\n    title = \"Diverging Bar Plot (ggplot2)\", caption = \"Produced by Gary Hutson\"\n  ) +\n  coord_flip()\n\nHere, I have added the labs layer on to the plot. This is a way to label your plots to show more meaningful values than would be included by default. So, within labs I use subtitle, title and caption to add labels to the chart. Finally, the important command is to add the coord_flip() command to the chart – without this you would have vertical bars instead of horizontal. I think this type of chart looks better horizontal, thus the reason for the inclusion of the command.\nThe final chart, looks as illustrated hereunder:\nThis blog was written by Gary Hutson, Principal Analyst, Activity & Access Team, Information & Insight at Nottingham University Hospitals NHS Trust, and was originally posted at Hutson-Hacks.\nThis blog has been formatted to remove Latin Abbreviations and edited for NHS-R Style and to ensure running of code in Quarto..\n\n\n Back to topReuseCC0CitationFor attribution, please cite this work as:\nHutson, Gary. 2018. “Diverging Bar Charts – Plotting Variance with\nGgplot2.” May 24, 2018. https://nhs-r-community.github.io/nhs-r-community//blog/diverging-bar-charts-plotting-variance-with-ggplot2.html."
  },
  {
    "objectID": "blog/coding-to-and-from-NA.html",
    "href": "blog/coding-to-and-from-NA.html",
    "title": "Recoding an NA and back again",
    "section": "",
    "text": "This was written originally in an Excel spreadsheet and used {datapasta} to copy into R as code to build the same data frame. {datapasta} can be access through RStudio as an Addin as well as code. Find out more about {datapasta} from the Introduction to R and R Studio course."
  },
  {
    "objectID": "blog/coding-to-and-from-NA.html#create-data",
    "href": "blog/coding-to-and-from-NA.html#create-data",
    "title": "Recoding an NA and back again",
    "section": "",
    "text": "This was written originally in an Excel spreadsheet and used {datapasta} to copy into R as code to build the same data frame. {datapasta} can be access through RStudio as an Addin as well as code. Find out more about {datapasta} from the Introduction to R and R Studio course."
  },
  {
    "objectID": "blog/coding-to-and-from-NA.html#recoding-to-na",
    "href": "blog/coding-to-and-from-NA.html#recoding-to-na",
    "title": "Recoding an NA and back again",
    "section": "Recoding to NA",
    "text": "Recoding to NA\n\nsurvey &lt;- tibble::tribble(\n  ~Survey.Response, ~Code,\n       \"Response1\",   -9L,\n       \"Response2\",    2L,\n       \"Response3\",   10L,\n       \"Response4\",    0L,\n       \"Response5\",    5L,\n       \"Response6\",   -9L,\n        \"Missing\", NA\n  )"
  },
  {
    "objectID": "blog/coding-to-and-from-NA.html#recode-to-na",
    "href": "blog/coding-to-and-from-NA.html#recode-to-na",
    "title": "Recoding an NA and back again",
    "section": "Recode to NA",
    "text": "Recode to NA\n\nlibrary(tidyverse)\n\nsurvey |&gt; \n  mutate(new_column = na_if(Code, -9))\n\n# A tibble: 7 × 3\n  Survey.Response  Code new_column\n  &lt;chr&gt;           &lt;int&gt;      &lt;int&gt;\n1 Response1          -9         NA\n2 Response2           2          2\n3 Response3          10         10\n4 Response4           0          0\n5 Response5           5          5\n6 Response6          -9         NA\n7 Missing            NA         NA\n\n\nIt’s also possible to use the numbers and case_when():\n\nsurvey |&gt; \n1  mutate(new_column = case_when(Code &lt; 0 ~ NA,\n2                                Code == 0 ~ 1000,\n3                                .default = Code))\n\n\n1\n\nWhere Code is less than 0 then code to NA.\n\n2\n\nWhere Code is equal to 0 then recode to 1000 which is a number that will stand out.\n\n3\n\nFor everything else return the original data from column Code.\n\n\n\n\n# A tibble: 7 × 3\n  Survey.Response  Code new_column\n  &lt;chr&gt;           &lt;int&gt;      &lt;dbl&gt;\n1 Response1          -9         NA\n2 Response2           2          2\n3 Response3          10         10\n4 Response4           0       1000\n5 Response5           5          5\n6 Response6          -9         NA\n7 Missing            NA         NA\n\n\nOr ifelse() where there are only two options:\n\nsurvey |&gt; \n  mutate(new_column = ifelse(Code &lt; 0, NA, Code))\n\n# A tibble: 7 × 3\n  Survey.Response  Code new_column\n  &lt;chr&gt;           &lt;int&gt;      &lt;int&gt;\n1 Response1          -9         NA\n2 Response2           2          2\n3 Response3          10         10\n4 Response4           0          0\n5 Response5           5          5\n6 Response6          -9         NA\n7 Missing            NA         NA"
  },
  {
    "objectID": "blog/coding-to-and-from-NA.html#recode-from-na",
    "href": "blog/coding-to-and-from-NA.html#recode-from-na",
    "title": "Recoding an NA and back again",
    "section": "Recode from NA",
    "text": "Recode from NA\n\nsurvey |&gt; \n  mutate(new_column2 = replace_na(Code, 1000))\n\n# A tibble: 7 × 3\n  Survey.Response  Code new_column2\n  &lt;chr&gt;           &lt;int&gt;       &lt;int&gt;\n1 Response1          -9          -9\n2 Response2           2           2\n3 Response3          10          10\n4 Response4           0           0\n5 Response5           5           5\n6 Response6          -9          -9\n7 Missing            NA        1000"
  },
  {
    "objectID": "blog/building-a-quarto-website.html",
    "href": "blog/building-a-quarto-website.html",
    "title": "Building a Quarto website for NHS-R Community",
    "section": "",
    "text": "A few years ago Mohammed A Mohammed, who was integral to getting NHS-R Community set up, suggested I might like to get involved by creating a website for NHS-R Community. At that time I had no idea where to start so my involvement stalled and a website was built, with help from a 3rd party, in WordPress. WordPress is a great website tool that can support very complicated sites like NHS-R Community’s, as we have both a static site and an Events Management system which means that we don’t need to rely (or pay!) for Eventbrite. We’ve used the system for sign up to hundreds of webinars and workshops, not to mention several hundred people attending conferences over the years.\n\n\nIn the time it’s taken from that initial suggestion about building the website to now, where I help to manage the site, I’ve had the opportunity to build websites for myself using {distill} and Hugo Apéro as well as the release of Quarto which I was incredibly keen to try out for website creation.\nI’d used Quarto for slides and reports and been hugely impressed with it. Getting started with Quarto is relatively easy if you’ve coded for a while, but like with anything, you have to start off at the basics and quickly want more complex functionality as you get going. This is even more of an issue when you are moving from one system to another and it’s more of a translation than a build-from-scratch project.\n\n\nAlthough Quarto has come from the R side of coding it can be used by any other language coder as it supports Python, Julia and Observable. Quarto output can be built in RStudio but equally as easily in other programs like VS Code.\nLuckily for me, many people had already jumped into Quarto websites and their code was available online to delve into. I particularly was keen to follow the work of the Brian Tarran for the Royal Statistical Society’s Real World Data Science website because, at first glance, this doesn’t give itself away as being built on Quarto! And I also really like Silvia Canelón’s personal blog site for its beauty and functionality. Silvia has particularly worked hard on accessibility of her website and has written a lot of CSS code which I’m learning through what she has shared.\nAccessibility is something that the WordPress site needed considerable work on, particularly when checked with a website called the Web Accessbility Evaluation Tool which Silvia recommends on her blog site. The new Quarto site will need this too of course but now that the code is out in the open, anyone can contribute. That can be either making changes to the code but also highlighting problems as issues that can then be open to everyone to view.\n\n\n\n\n\nI’ll mention accessibility again as that’s hugely important and I like the fact that the people working on Quarto also think a lot about accessibility. Adding alt text has never been easier and any code (in the YAML) that refers to an image nearly always has functionality to add image alt text too.\n\n\n\nThe search function for Quarto books is really impressive and the same functionality can be found in Quarto websites. There might be a plug in within WordPress to make searching through blogs for particular words but with Quarto it’s not something that needs adding - it’s there from the start. I now no longer have to think about tags or categories for blogs to make them easier to find (and then remember them to find the blogs which is the trickier part for me!).\n\n\n\nThis was a bit of a niggle with WordPress in that it’s not really a website for coders to show their code examples (although there may be a plug in for that!). Quarto handles code snippets and examples by beautifully formatting it to look different to text but also runs the code for you if you want. It’s meant that the charts that are talked about in some blogs are produced as the code is run, not as static pictures that also can’t be copied easily as a picture doesn’t give the functionality for highlighting to copy.\nUsually a blogger will choose a nice image to complement their writing and Quarto gives that functionality but I had a lovely surprise when charts that are created by the blog code became the thumbnail image automatically.\n\n\n\nHaving a website that can publish R or Python code is an absolute joy. I used to write blogs in RMarkdown, render them to html and then copy the output to WordPress and in the copying and pasting often lost links or formatting.\nBeing open to contribution of course also means that the repository will be available to people to do pull requests. I’ve set the repository so that it’s not necessary to Render the website to contribute so any additions or changes just need to be made to the qmd files.\n\n\n\nNow this point could have come from any change in website but I’ve taken the opportunity to go through each blog and transfer it to Quarto. There are no doubt quicker ways to do this as I can export from WordPress and then work on code to to get it to a format that can be published, but I started with moving a couple of blogs to get started and I’ve really enjoyed this even though it takes a bit more time to do.\nStarting in 2018 when the blogs first started I didn’t know R at all. I started learning with NHS-R Community so these early blogs meant very little to me at the time. As I’ve gone through each blog, formatting them and checking the R code runs (I found a couple of tiny mistakes this way) and the links still work I’ve been more like an archivist than a coder. My favourite discovery, and shock, was that I found that RAP (Reproducible Analytical Pipelines) was shared with the NHS-R Community in 2018 although I only really heard about it from 2020. It was very much like the moment I experienced when I went through my old Geography school books and found I’d been taught in those lessons about Index of Multiple Deprivation (IMD) but I had absolutely no recollection of it at all! It hadn’t been the right time for me to really hear about it and yet in time, it’s become something that I now really love working with. You can read more about IMD in a Quarto collaborative book for NHS-R Community called Health Inequalities.\n\n\n\n\nKnowing where to start with contributions can be a barrier so I’ve written out a few things to try to help people do this. Firstly, our Quarto book (yes another one - they are additive be warned!) Statement on Tools includes some technical appendices including Contributing to GitHub repositories.\nIf you want to get involved with the NHS-R Community more generally we are building on the NHS-R Way which essentially documents the community. We’ve got a number of activities listed out and these are all open to get involved with, or set up something else.\nWe can be found, as a community, in Slack and our central email is nhs.rcommunity@nhs.net."
  },
  {
    "objectID": "blog/building-a-quarto-website.html#moving-to-purely-coded-websites",
    "href": "blog/building-a-quarto-website.html#moving-to-purely-coded-websites",
    "title": "Building a Quarto website for NHS-R Community",
    "section": "",
    "text": "In the time it’s taken from that initial suggestion about building the website to now, where I help to manage the site, I’ve had the opportunity to build websites for myself using {distill} and Hugo Apéro as well as the release of Quarto which I was incredibly keen to try out for website creation.\nI’d used Quarto for slides and reports and been hugely impressed with it. Getting started with Quarto is relatively easy if you’ve coded for a while, but like with anything, you have to start off at the basics and quickly want more complex functionality as you get going. This is even more of an issue when you are moving from one system to another and it’s more of a translation than a build-from-scratch project.\n\n\nAlthough Quarto has come from the R side of coding it can be used by any other language coder as it supports Python, Julia and Observable. Quarto output can be built in RStudio but equally as easily in other programs like VS Code.\nLuckily for me, many people had already jumped into Quarto websites and their code was available online to delve into. I particularly was keen to follow the work of the Brian Tarran for the Royal Statistical Society’s Real World Data Science website because, at first glance, this doesn’t give itself away as being built on Quarto! And I also really like Silvia Canelón’s personal blog site for its beauty and functionality. Silvia has particularly worked hard on accessibility of her website and has written a lot of CSS code which I’m learning through what she has shared.\nAccessibility is something that the WordPress site needed considerable work on, particularly when checked with a website called the Web Accessbility Evaluation Tool which Silvia recommends on her blog site. The new Quarto site will need this too of course but now that the code is out in the open, anyone can contribute. That can be either making changes to the code but also highlighting problems as issues that can then be open to everyone to view."
  },
  {
    "objectID": "blog/building-a-quarto-website.html#benefits-to-quarto",
    "href": "blog/building-a-quarto-website.html#benefits-to-quarto",
    "title": "Building a Quarto website for NHS-R Community",
    "section": "",
    "text": "I’ll mention accessibility again as that’s hugely important and I like the fact that the people working on Quarto also think a lot about accessibility. Adding alt text has never been easier and any code (in the YAML) that refers to an image nearly always has functionality to add image alt text too.\n\n\n\nThe search function for Quarto books is really impressive and the same functionality can be found in Quarto websites. There might be a plug in within WordPress to make searching through blogs for particular words but with Quarto it’s not something that needs adding - it’s there from the start. I now no longer have to think about tags or categories for blogs to make them easier to find (and then remember them to find the blogs which is the trickier part for me!).\n\n\n\nThis was a bit of a niggle with WordPress in that it’s not really a website for coders to show their code examples (although there may be a plug in for that!). Quarto handles code snippets and examples by beautifully formatting it to look different to text but also runs the code for you if you want. It’s meant that the charts that are talked about in some blogs are produced as the code is run, not as static pictures that also can’t be copied easily as a picture doesn’t give the functionality for highlighting to copy.\nUsually a blogger will choose a nice image to complement their writing and Quarto gives that functionality but I had a lovely surprise when charts that are created by the blog code became the thumbnail image automatically.\n\n\n\nHaving a website that can publish R or Python code is an absolute joy. I used to write blogs in RMarkdown, render them to html and then copy the output to WordPress and in the copying and pasting often lost links or formatting.\nBeing open to contribution of course also means that the repository will be available to people to do pull requests. I’ve set the repository so that it’s not necessary to Render the website to contribute so any additions or changes just need to be made to the qmd files.\n\n\n\nNow this point could have come from any change in website but I’ve taken the opportunity to go through each blog and transfer it to Quarto. There are no doubt quicker ways to do this as I can export from WordPress and then work on code to to get it to a format that can be published, but I started with moving a couple of blogs to get started and I’ve really enjoyed this even though it takes a bit more time to do.\nStarting in 2018 when the blogs first started I didn’t know R at all. I started learning with NHS-R Community so these early blogs meant very little to me at the time. As I’ve gone through each blog, formatting them and checking the R code runs (I found a couple of tiny mistakes this way) and the links still work I’ve been more like an archivist than a coder. My favourite discovery, and shock, was that I found that RAP (Reproducible Analytical Pipelines) was shared with the NHS-R Community in 2018 although I only really heard about it from 2020. It was very much like the moment I experienced when I went through my old Geography school books and found I’d been taught in those lessons about Index of Multiple Deprivation (IMD) but I had absolutely no recollection of it at all! It hadn’t been the right time for me to really hear about it and yet in time, it’s become something that I now really love working with. You can read more about IMD in a Quarto collaborative book for NHS-R Community called Health Inequalities."
  },
  {
    "objectID": "blog/building-a-quarto-website.html#contributions-are-welcome",
    "href": "blog/building-a-quarto-website.html#contributions-are-welcome",
    "title": "Building a Quarto website for NHS-R Community",
    "section": "",
    "text": "Knowing where to start with contributions can be a barrier so I’ve written out a few things to try to help people do this. Firstly, our Quarto book (yes another one - they are additive be warned!) Statement on Tools includes some technical appendices including Contributing to GitHub repositories.\nIf you want to get involved with the NHS-R Community more generally we are building on the NHS-R Way which essentially documents the community. We’ve got a number of activities listed out and these are all open to get involved with, or set up something else.\nWe can be found, as a community, in Slack and our central email is nhs.rcommunity@nhs.net."
  },
  {
    "objectID": "blog/a-simple-function-to-install-and-load-packages-in-r.html",
    "href": "blog/a-simple-function-to-install-and-load-packages-in-r.html",
    "title": "A simple function to install and load packages in R",
    "section": "",
    "text": "I was starting to despair at the amount of packages I end up using during the task of transforming, cleaning, modelling and validating some of my models. I thought there must be a simple approach to dealing with this?\nSo, I started to ponder if I could create a function that would just install and then load the packages straight into R in one go. I found the solution and it can be applied to all your projects – all you have to do is supply the list of packages to the function and “hey presto!”\nRun the below function and I will explain what this does:\n\ninstall_or_load_pack &lt;- function(pack) {\n  create.pkg &lt;- pack[!(pack %in% installed.packages()[, \"Package\"])]\n\n  if (length(create.pkg)) {\n    install.packages(create.pkg, dependencies = TRUE)\n  }\n\n  sapply(pack, require, character.only = TRUE)\n\n  # I know I should be using purr here, but this is before the Tidyverse is loaded. I know you Tidyverse trend setters will have me here.\n}\n\nThis creates a function, in which, you can pass a vector of packages you want to load. The sweet point with this function is that if the packages are not installed, this function will do that and then load them, so the next time you come to using the function it will just load them into your project – instead of installing them. I said it was cool, or I thought you might find it cooler than I expected, either way I still think it’s cool.\nThe last step is to make a call to the function and specify the list of packages you need:\n\npackages &lt;- c(\"ggplot2\", \"plotly\", \"data.table\", \"tidyverse\", \"caret\")\n\ninstall_or_load_pack(packages)\n\nIf the packages are not installed then this will show an installation series in the console window, otherwise it will just flag a Boolean value to show that they are now active in the project:\n\n   ggplot2     plotly data.table  tidyverse      caret \n      TRUE       TRUE       TRUE       TRUE       TRUE \n\nThis blog was written by Gary Hutson, Principal Analyst, Activity & Access Team, Information & Insight at Nottingham University Hospitals NHS Trust, and was originally posted on Hutson Hacks.\n\n\n\n Back to topReuseCC0CitationFor attribution, please cite this work as:\nHutson, Gary. 2018. “A Simple Function to Install and Load\nPackages in R.” August 17, 2018. https://nhs-r-community.github.io/nhs-r-community//blog/a-simple-function-to-install-and-load-packages-in-r.html."
  },
  {
    "objectID": "blog/a-run-chart-is-not-a-run-chart-is-not-a-run-chart.html",
    "href": "blog/a-run-chart-is-not-a-run-chart-is-not-a-run-chart.html",
    "title": "A run chart is not a run chart is not a run chart",
    "section": "",
    "text": "Run charts are simple and powerful tools that help discriminate between random and non-random variation in data over time – for example, measures of healthcare quality.\nRandom variation is present in all natural processes. In a random process we cannot know the exact value of the next outcome, but from studying previous data we may predict the probability of future outcomes. So, a random process is predictable. Non-random variation, on the other hand, appears when something new, sometimes unexpected, starts to influence the process. This may be the result of intended changes made to improve the process or unintended process deterioration. The ability to tell random from non-random is crucial in quality improvement. One way of achieving this is runs analysis.\nIn general, a run is defined as a sequence of like items that is preceded and followed by one or more items of another type or by no item. Items can be heads and tails, odd and even numbers, numbers above and below a certain value, and so on.\nRuns analysis is based on knowledge of the natural distributions and limits of run lengths and the number of runs in random processes. For example, if we toss a coin 10 times and get all heads (1 run of length 10), we would think that something non-random is affecting the game. Likewise, if a run chart includes a run of 10 data points on the same side of the centre line, we would start looking for an explanation.\n\n\n\nFigure 1: Run charts from random numbers. A: random variation. B: non-random variation in the form of an upwards shift introduced after data point number 16 and identified by unusually long and few runs and signalled by a red, dashed centre line. See text for details on how to identify non-random variation.\n\n\nSpecifically, a run chart may be regarded as a coin tossing game where the data points represent heads and tails depending on their position above and below the centre line (ignoring data points that fall directly on the centre line). If the process is random, the data points will be randomly distributed around the centre (Figure 1A). A shift in process location will affect the distribution of data points and will eventually present itself by non-random patterns in data, which can be identified by statistical tests (Figure 1B).\nSwed and Eisenhart studied the expected number of runs in random sequences. If the number of runs is too small or too large, it is an indication that the sequence is not random (1). To perform Swed and Eisenhart’s runs test one must either do rather complicated calculations or look up the limits for the expected number of runs in tables based on the total number of runs in the sequence and the number of items of each kind. Simplified tables for use with run charts have been developed for up to 60 data points (2). For more than 60 data points, the limits can be calculated using the normal approximation of the runs distribution function (3). For example, in a run chart with 24 data points, the expected number of runs (95% prediction limits) is 8-18.\nChen proposed an alternative to Swed and Eisenhart’s method. Instead of counting the number of runs, Chen counts the number of shifts in the sequence, i.e. when a value of one kind is followed by a value of another kind, which is one less than the number of runs (4). To avoid confusing Chen’s shifts in sequence with shifts in process location, I use the term crossings.\nIn run charts, crossings are easily counted by counting the number of times the graph crosses the median line. If the process is random, the chance of crossing or not crossing the median line between two adjacent data points is fifty-fifty. Thus, the total number of crossings has a binomial distribution, b(n−1,0.5), where n is the number of data points and 0.5 is the success probability.\nWe should consider whether we are interested in both sides of the distribution, too few and/or too many crossings. By nature, a shift in process location following process improvement or deterioration will result in fewer crossings than expected. But unusually many crossings (oscillation) is also a sign of non-random variation, which will appear if data are negatively autocorrelated, that is, if any high number tends to be followed by a low number and vice versa. However, oscillation is not an effect of the process shifting location, but most likely a result of a poorly designed measure or sampling issues (5 p175). Chen recommends using one-sided tests suited for the purpose of the analysis, i.e. whether one is interested in detecting shifts or oscillations (4).\nFor example, for a run chart with 24 data points we could choose the lower fifth percentile of the cumulative binomial distribution of 23 trials with a success probability of 0.5 as our critical value for the lower limits of crossings. This is easily calculated in R using the qbinom() function, qbinom(p = 0.05, size = 24 - 1, prob = 0.5) = 8, i.e. fewer than 8 crossings would be unusual and suggest that the process is shifting. In Figure 1B non-random variation in the form of a shift is identified by the fact that the chart has only 6 crossings when at least 8 would be expected from 24 random numbers.\nThe number of crossings (and runs) is inversely related to the lengths of runs. All things being equal, fewer crossings give longer runs and vice versa. Therefore, a test for unusually long runs is also commonly used as a means to identify shifts. A simple example is the “classic” rule of thumb of a run of 8 or more data points on the same side of the centre line. But just like the expected number of crossings, the expected length of the longest run depends on the total number of data points. In a run chart with, say, 100 data points, we should not be surprised to find a run of 8.\nThe distribution of longest runs has been described in detail by Schilling (6–8). The expected length of the longest run either above or below the median is log2(n), where n is the total number of data points, excluding data points that fall directly on the centre line. Approximately 95% of the longest runs are predicted to be within ±3 of the expected value. For the purpose of detecting a shift, we are interested in the upper prediction limit for longest run, which is log2(n)+3 (rounded to the nearest integer). For example, in a run chart of 24 data points, the upper 95% prediction limit for the longest run is round(log2(24) + 3) = 8, i.e. a run of more than 8 indicates a shift. Figure 1B has an unusually long run of 9 consecutive data points on the same side of the centre line.\nA trend is a special form of a run, where like items are defined as data points that are bigger or smaller than the preceding one. The trend test was developed by Olmstead who provided tables and formulas for the probabilities of trends of different lengths depending on the total number of data points (9). For example, with less than 27 data points in total, the chance of having a trend of 6 or more data points going up or down is less than 5%. Note that Olmstead defines a trend as the number of jumps rather than the number of data points that surround the jumps.\nIn summary, there are (at least) four unusual run patterns that may be used to identify non-random variation in run charts:\n\nToo many runs\n\nToo few runs\n\nToo long runs\n\nToo long trends\n\nThe selection of rules and the choice of critical values to define too many, too few and too long have significant influence on the statistical properties of run charts. This is the subject of the following sections."
  },
  {
    "objectID": "blog/a-run-chart-is-not-a-run-chart-is-not-a-run-chart.html#appendix-critical-values-for-longest-run-and-number-of-crossings",
    "href": "blog/a-run-chart-is-not-a-run-chart-is-not-a-run-chart.html#appendix-critical-values-for-longest-run-and-number-of-crossings",
    "title": "A run chart is not a run chart is not a run chart",
    "section": "Appendix: Critical values for longest run and number of crossings",
    "text": "Appendix: Critical values for longest run and number of crossings\n![Screenshot of critical values for longest run and number of crossings](img/appendix-1-critic"
  },
  {
    "objectID": "blog/a-simple-function-to-create-nice-correlation-plots.html",
    "href": "blog/a-simple-function-to-create-nice-correlation-plots.html",
    "title": "A simple function to create nice correlation plots",
    "section": "",
    "text": "The problem\nI was working with a dataset where I wanted to assess the correlation of different variables in R. As much as I like R – the outputs from the console window leave something to be desired (in terms of data visualisation). Therefore, I wanted a way to visualise these correlations in a nicer / cleaner / crisper way. The solution to this is to use a correlation plot.\nThe package I used for creating my correlation plots was the {corrplot} package, this can be installed and loaded into the R workspace by using the syntax below:\n\n# install.packages(\"corrplot\")\nlibrary(corrplot)\n\nAt this point I would encourage you to check out help for the corrplot function, as it allows you to pass a multitude of parameters to the function.\nDeconstructing the function\nAs mentioned previously, this plotting function has a multitude of uses, but all the parameters can be off putting to a newbie! This was me 6 years ago vigorously typing ‘how to do this with R relating to x’ and bombarding stackoverflow and other useful websites with questions. Shout out to RBloggers as well!\nThe function I have created uses the functionality of the {corrplot} packages, but it simplifies the inputs. I will include the function in stages to explain each step, however, if you just want to use the function and are not bothered with the underpinnings then skip the following section:\nStep 1 – Function parameters\nParameters of the function are as below:\n\ncreate_gh_style_corrplot &lt;- function(df_numeric_vals,\n                                     method_corrplot,\n                                     colour_min,\n                                     colour_middle,\n                                     colour_max = \"green\") {}\n\nThe parameters to pass to the function are:\ndf_numeric_vals this means a data frame of numeric values only, so any categorical (factor) data needs to be stripped out before passing the data frame to the function; method_corrplot this is a numeric range from 1 – 5. So, for a shaded correlation plot you would use 1. Further examples of the various options will be discussed when I describe how the if statement works. colour_min this uses a gradient colour setting for the negative positive correlations. An example of an input here would be “green”. colour_middle this is the middle range colour, normally I set this equal to (=) “white”. colour_max this is the colour of the strong positive correlations For information on the strength of correlations, refer to this simple guide - what is R value correlation?.\nStep 2 – Creating the conditional (IF statement) to select correlation plot type\nThe below conditional statement uses the input of the function e.g. 1-5 to select the type of chart to display. This is included in the code block below:\n\nlibrary(corrplot)\n\ncreate_gh_style_corrplot &lt;- function(df_numeric_vals,\n                                     method_corrplot,\n                                     colour_min,\n                                     colour_middle,\n                                     colour_max = \"green\") {\n  if (method_corrplot == 1) {\n    type_var &lt;- \"shade\"\n    method_corrplot &lt;- type_var\n  } else if (method_corrplot == 2) {\n    type_var &lt;- \"number\"\n    method_corrplot &lt;- type_var\n  } else if (method_corrplot == 3) {\n    type_var &lt;- \"pie\"\n    method_corrplot &lt;- type_var\n  } else if (method_corrplot == 4) {\n    type_var &lt;- \"ellipse\"\n    method_corrplot &lt;- type_var\n  } else if (method_corrplot == 5) {\n    type_var &lt;- \"circle\"\n    method_corrplot &lt;- type_var\n  } else {\n    type_var &lt;- \"shade\"\n    method_corrplot &lt;- type_var\n  }\n}\n\nWhat does this do then? Well firstly nested in the function I make sure that the {corrplot} library is referenced to allow for the correlation plot functionality to be used. The next series of steps repeat this method:\n\nBasically, this says that if the method_corrplot parameter of the function equals input 1, 2, 3, and so on – then select the relevant type of correlation plot.\nThe type_var is a variable that sets the value of the variable equal to the string stated. These strings link directly back to the parameters of the corrplot function, as I know a type of correlation plot is equal to shade or number, and so on.\nFinally, the last step is to convert method_corrplot equal to the textual type specified in the preceding bullet.\n\nIn essence, what has been inputted as numeric value into the parameter that is. 1; set the type_var equal to a text string that matches something that corrplot is expecting and then set the method_corrplot variable equal to that of the type variable. Essentially, turning the integer value passed into the parameter into a string / character output.\nStep 3 – Hacking the corrplot function\n\ncreate_gh_style_corrplot &lt;- function(df_numeric_vals,\n                                     method_corrplot,\n                                     colour_min,\n                                     colour_middle,\n                                     colour_max = \"green\") {\n  if (method_corrplot == 1) {\n    type_var &lt;- \"shade\"\n    method_corrplot &lt;- type_var\n  } else if (method_corrplot == 2) {\n    type_var &lt;- \"number\"\n    method_corrplot &lt;- type_var\n  } else if (method_corrplot == 3) {\n    type_var &lt;- \"pie\"\n    method_corrplot &lt;- type_var\n  } else if (method_corrplot == 4) {\n    type_var &lt;- \"ellipse\"\n    method_corrplot &lt;- type_var\n  } else if (method_corrplot == 5) {\n    type_var &lt;- \"circle\"\n    method_corrplot &lt;- type_var\n  } else {\n    type_var &lt;- \"shade\"\n    method_corrplot &lt;- type_var\n  }\n\n  corrplot(cor(df_numeric_vals, use = \"all.obs\"),\n    method = method_corrplot,\n    order = \"AOE\",\n    addCoef.col = \"black\",\n    number.cex = 0.5,\n    tl.cex = 0.6,\n    tl.col = \"black\",\n    col = colorRampPalette(c(colour_min, colour_middle, colour_max))(200),\n    cl.cex = 0.3\n  )\n}\n\nLet’s explain this function.\nSo, the corrplot function is the main driver for this and the second nested cor is just as important, as this is the command to create a correlation matrix. The settings are to use the df_numeric_vals data frame as the data to use with the function, the use = 'all.obs' just tells the function to use all observations in the data frame and the method = method_corrplot uses the if statement I created in step 2 to select the relevant chart from the input. The order uses the angular ordering method and the addCoef.col = 'black' sets the coefficient values to always show black, as well as the colour of the labels. The background colour of the correlation plot uses the colorRampPalette function to create a gradient scale for the function and the parameters of each of the colour settings like to those inputs I explained in step 1.\nThe full function is detailed here:\n\ncreate_gh_style_corrplot &lt;- function(df_numeric_vals,\n                                     method_corrplot,\n                                     colour_min,\n                                     colour_middle,\n                                     colour_max = \"green\") {\n  library(corrplot)\n\n  if (method_corrplot == 1) {\n    type_var &lt;- \"shade\"\n    method_corrplot &lt;- type_var\n  } else if (method_corrplot == 2) {\n    type_var &lt;- \"number\"\n    method_corrplot &lt;- type_var\n  } else if (method_corrplot == 3) {\n    type_var &lt;- \"pie\"\n    method_corrplot &lt;- type_var\n  } else if (method_corrplot == 4) {\n    type_var &lt;- \"ellipse\"\n    method_corrplot &lt;- type_var\n  } else if (method_corrplot == 5) {\n    type_var &lt;- \"circle\"\n    method_corrplot &lt;- type_var\n  } else {\n    type_var &lt;- \"shade\"\n    method_corrplot &lt;- type_var\n  }\n\n\n  corrplot(cor(df_numeric_vals, use = \"all.obs\"),\n    method = method_corrplot,\n    order = \"AOE\",\n    addCoef.col = \"black\",\n    number.cex = 0.5,\n    tl.cex = 0.6,\n    tl.col = \"black\",\n    col = colorRampPalette(c(colour_min, colour_middle, colour_max))(200),\n    cl.cex = 0.3\n  )\n}\n\nIf you want to use the function, just copy and paste this code into a R script file and this will create the function for you. Please remember to install the {corrplot} package by using install.packages(corrplot).\nUtilising the function\nThe example dataset I will use here is the mpg sample file provided by {ggplot2}. Load the R script provided towards the end of the last section first, as this will create the function in R’s environment. Next, add this code to the end to look at the various different iterations and charts that can be created from the data:\n\nlibrary(ggplot2)\n##------------------CREATE DATASET---------------------------------------\n\nnumeric_df &lt;- data.frame(mpg[c(3,5,8,9)])\n\n#This relates to the numeric variables in the data frame to use with my function\n##------------------USE FUNCTION-----------------------------------------\n\ncreate_gh_style_corrplot(numeric_df,1, \"steelblue2\",\"white\", \"whitesmoke\")\ncreate_gh_style_corrplot(numeric_df,2, \"steelblue2\",\"black\", \"black\")\ncreate_gh_style_corrplot(numeric_df,3, \"steelblue2\",\"white\", \"whitesmoke\")\ncreate_gh_style_corrplot(numeric_df,4, \"steelblue2\",\"white\", \"whitesmoke\")\ncreate_gh_style_corrplot(numeric_df,5, \"steelblue2\",\"white\", \"whitesmoke\")\n\nThe outputs of the charts are reliant on the correlation plot type select 1-5, and the colour ranges selected. You can choose any colour and I would recommend using the command colours() in R console or script to pull up the list of colours native to R.\nHow about these visualisations:\n\nlibrary(ggplot2)\n##------------------CREATE DATASET---------------------------------------\n\nnumeric_df &lt;- data.frame(mpg[c(3,5,8,9)])\n\n#This relates to the numeric variables in the data frame to use with my function\n##------------------USE FUNCTION-----------------------------------------\n\ncreate_gh_style_corrplot(numeric_df,1, \"steelblue2\",\"white\", \"whitesmoke\")\n\n\n\n\n\n\ncreate_gh_style_corrplot(numeric_df,2, \"steelblue2\",\"black\", \"black\")\n\n\n\n\n\n\ncreate_gh_style_corrplot(numeric_df,3, \"steelblue2\",\"white\", \"whitesmoke\")\n\n\n\n\n\n\ncreate_gh_style_corrplot(numeric_df,4, \"steelblue2\",\"white\", \"whitesmoke\")\n\n\n\n\n\n\ncreate_gh_style_corrplot(numeric_df,5, \"steelblue2\",\"white\", \"whitesmoke\")\n\n\n\n\n\n\n\nEach plot can be tailored to suite your needs. I tend to like blue shades, but go all out and choose whatever colours you like. The R source code is accessible on GitHub.\nI do hope you will use this function to maximise your correlation plots – its all about relationships!\nThis blog was written by Gary Hutson, Principal Analyst, Activity & Access Team, Information & Insight at Nottingham University Hospitals NHS Trust, and was originally posted on Hutson’s Hacks.\nThis blog has been edited for NHS-R Style and to ensure running of code in Quarto.\n\n\n Back to topReuseCC0CitationFor attribution, please cite this work as:\nHutson, Gary. 2019. “A Simple Function to Create Nice Correlation\nPlots.” January 31, 2019. https://nhs-r-community.github.io/nhs-r-community//blog/a-simple-function-to-create-nice-correlation-plots.html."
  },
  {
    "objectID": "blog/aiming-for-a-wrangle-free-or-reduced-world.html",
    "href": "blog/aiming-for-a-wrangle-free-or-reduced-world.html",
    "title": "Aiming for a wrangle-free (or reduced) world",
    "section": "",
    "text": "I work as a Data Scientist at Public Health England. I am part of a small team that have a role in trying to modernise how we “do” data. I have been an analyst in one way or another for most of my working life. In my role as an analyst, as with most analysts, my biggest focus was on the accuracy of my outputs, but I’ve always got frustrated very quickly with repetitive tasks, which are all too common for analytical roles. In fact, I remember when these frustrations first began. It was during my Masters when, as part of my dissertation, my supervisor asked me to draw a map using ArcMap software. I hadn’t had any previous experience of the software before this moment. Instead of asking for help, which I should have done, I went away and tried to import the files I was given. What I didn’t know was that I didn’t have access to the correct licence to import the files I was using. I thought I was doing something wrong. I did manage to open the files in Excel though and I could see they contained coordinates. I could see that if I interpolated the coordinates of the points, I could create a file that I would be able to upload into ArcMap and it would look like what I was aiming for - the problem was that there were millions of coordinates and they were split over multiple files! This is the moment in my life where I discovered “Record Macro”. I managed to record a few instances of what I wanted to do, and then manipulated the recorded code to repeat the task for everything. I felt very smug going to my supervisor the following week and handing him a map with pink, yellow and red blobs illustrating height contours of a water basin. To say he wasn’t impressed would be an understatement. He pulled up a map on his screen to show me what it should look like. His screen essentially showed what Google Satellite now provides us. My smugness quickly turned to self-doubt.\nIn many ways this example is typical of my experience as an analyst. I have received data in many ways, from people or through systems and databases, but to manipulate (or wrangle, as it is commonly called now) those data to what my manager wanted to see would take a number of days. The data may contain a table for each month, where each month was a different tab in a spreadsheet. If I was lucky each tab would be formatted identically, but more often than not there would be different numbers of columns or rows (sigh). Sometimes, one month might (helpfully) have an extra blank row at the top or maybe some merged cells. I would sit there bringing all those data into one place thinking of myself as part of a sandwich assembly line, picking up all the raw ingredients, assembling them in the right way for somebody else to enjoy. I really wanted to enjoy it! Surely this can be done better and faster. How did analysts on detective shows instantly get the information the senior detective required at the tap of a few keys?\nI first used R 3 years ago. R has completely changed the way that I see data. It has formalised all my previous frustrations. It has words for things that I have thought but could never explain. It encourages data to be “done” properly. Before R I had never heard of an analytical pipeline (I realise this isn’t exclusively an R thing). Everything I had done was about getting data, spending time wrangling it, analysing it and finally presenting it to someone else (for their enjoyment). R gave me R Markdown. Here I could do all of these steps in one script. There was no need for me to write a Word document to sit alongside my Excel workbook to explain where I got the data from, what tabs 2 to 7 do, and why I’ve hard-coded 34.84552 in cell D4. There was no need for me to write step by step instructions for how to draw a bar chart on one axis and a line chart on another within the same graph. The ability to become transparent in my workings was ideal for my lazy nature as the description is written in the code. Not only was my working transparent, it was also completely reproducible. If someone else had access to the same data as me, they could run my script and it would produce the same outputs.\nMy biggest revelation though was being introduced to tidy data. This was my game changer. I had often heard the quote that analysts spent 80% of their time manipulating data and 20% analysing it. That chimed with me. As is written in the paper referenced above:\n“tidy datasets are all alike but every messy dataset is messy in its own way”\nAs the paper describes, tidy data has three features:\nEach variable forms a column. Each observation forms a row. Each type of observational unit forms a table.\nIt is hard to describe or appreciate this really until you think about the dataset you’re working with. Are you really struggling to get it into the format you need to make it easy to work with? The example the paper provides can be seen below:\n\nlibrary(gt)\nlibrary(tidyr)\nlibrary(dplyr)\n\nuntidy_data &lt;- tibble::tribble(\n  ~person, ~treatmentA, ~treatmentB,\n  \"John Smith\",          NA,          2L,\n  \"Jane Doe\",         16L,         11L,\n  \"Mary Johnson\",          3L,          1L\n)\n\n\ngt(untidy_data)\n\n\n\n\n\nperson\ntreatmentA\ntreatmentB\n\n\n\nJohn Smith\nNA\n2\n\n\nJane Doe\n16\n11\n\n\nMary Johnson\n3\n1\n\n\n\n\n\ntidy_data &lt;- untidy_data |&gt; \n  pivot_longer(cols = c(\"treatmentA\", \"treatmentB\"),\n               names_to = \"treatment\",\n               values_to = \"result\") |&gt; \n  arrange(treatment)\n\ngt(tidy_data)\n\n\n\n\n\nperson\ntreatment\nresult\n\n\n\nJohn Smith\ntreatmentA\nNA\n\n\nJane Doe\ntreatmentA\n16\n\n\nMary Johnson\ntreatmentA\n3\n\n\nJohn Smith\ntreatmentB\n2\n\n\nJane Doe\ntreatmentB\n11\n\n\nMary Johnson\ntreatmentB\n1\n\n\n\n\n\n\nFigure 1, the table on above illustrates an untidily formatted table. The table below presents the same data but in tidy format\nAs an analyst, working with tidy data is a rare pleasure. Analytical tasks become seamless as it allows you to use the tidyverse package. Summarising data for groups within your dataset or creating models based on subgroups are an additional one or two lines of understandable code rather than 20 to 30.\nI look forward to a world where tidy data becomes the norm. In this world analysts will be spending 80% of their time analysing the data. We will be using data in a timely fashion, and it will be informing decision making even more than it currently does. We will be combining different datasets to create fuller pictures for the decisions we are informing. We will be learning new techniques for analysing the data rather than new techniques for manipulating them. Wrangling will become a thing of the past and most importantly, we will get to enjoy the sandwich that we’ve made.\n\n\n\n Back to topReuseCC0CitationFor attribution, please cite this work as:\nFox, Sebastian. 2018. “Aiming for a Wrangle-Free (or Reduced)\nWorld.” March 23, 2018. https://nhs-r-community.github.io/nhs-r-community//blog/aiming-for-a-wrangle-free-or-reduced-world.html."
  },
  {
    "objectID": "blog/but-this-worked-the-last-time-i-ran-it.html",
    "href": "blog/but-this-worked-the-last-time-i-ran-it.html",
    "title": "But this worked the last time I ran it!",
    "section": "",
    "text": "A common scenario: you are new to a programming language, chuffed that you’ve done a few bits and progressing well. A colleague hands you their code to run a regular report. That’s fine, you’ve got a smattering of understanding, and this is existing code, what could go wrong? But the code doesn’t run. You check through it but nothing looks obviously out of place and then your colleague adds to your utter confusion by saying it works on their machine.\nAnother scenario: you are new to a programming language and have written some code. It’s been working surprisingly well and you feel very pleased with your progress but then, suddenly, it fails. You look on at the computer, stunned. It worked the last time you ran it! You’ve made no changes and yet lots of red errors now appear and these errors are, quite frankly, utterly baffling and even googling them turns up all manner of strange discussions which might as well be in Old English (sort of familiar but you have no idea what it’s saying).\nThe solution: well there won’t necessarily be just one solution but here are a few things I’ve picked up in my early days using RStudio. I’m still in those early days and I probably haven’t encountered all that could possibly go wrong so please add to these and definitely comment if I’ve made any glaring errors!\n\nMy colleague can run this script on this very same computer but I can’t! What’s that all about?\n\nRStudio allows you to install and run your own packages (if your network allows) and that’s really useful when you just want to try something out, follow up on a recommendation or install something a training course has required. Given our strict network and IT installations this is quite a liberating experience!\nBut what isn’t apparent when you are merrily installing packages is that these are installed to your own folder so on a shared network this may not be accessible by a colleague. Step one in the solving the problem is to check the package is installed on your machine and your profile.\nYou may now be familiar with:\ninstall.packages(\"dplyr\") to install and then library(dplyr)\nbut consider using\nif (!require(dplyr)) install.packages(\"dplyr\")\nso RStudio will install this package if it is missing. Very useful when sharing R scripts as they can just be run with no typing by the recipient.\n\nI ran this code the other day and it was fine and now I get errors – but I haven’t done anything!\n\nThis happened recently to a colleague and prompted me to write this blog because I thought, this is probably the kind of thing that happens all of the time and if no one tells you this how could you know? Well there is Google but it’s too much to type I ran this code the other day and it now gives me an error….\nMy colleague’s code had been working, she hadn’t made any changes but one day it just failed. It wasn’t as if she ran it, ran it again seconds later and it failed, this was a run-it-one-day, it works and run-it-the-next-day, it fails. She asked if I could help and all I could think of was to run it myself. Not exactly expert support I thought. A bit like calling IT with a computer problem and being asked if you’ve rebooted your machine but a bit more advanced than “have you switched it on?”. Something strange happened when I ran it; it worked.\nJust as a plug for another blogger she was recreating the plot from this blog:\nhttps://www.johnmackintosh.com/blog/2017-12-21-flow/\nhttps://github.com/johnmackintosh/RowOfDots\nThis was puzzling but I had a faint recollection from other R people’s stories that you should keep your packages are up to date. One course had even said about updating these regularly and had, thankfully, shown us what to do.\nPackages are regularly being updated, a few tweaks here and there I guess. Plus many are built on other packages (like {dplyr} and {ggplot2}) so if they are updated then it’s like a domino effect. RStudio is nicely set up so you don’t have to go to the internet to find out individually what you need to update, you just need to go to ‘Packages’ in the bottom right hand panel of RStudio, select ‘Update’ which has a green circle with an arrow and it brings up a list of what needs updating.\n\n\n\nScreenshot of the packages tab from RStudio along with Files, Plots, Help and Viewer\n\n\nIf you’ve not done this for a while you may have quite a few updates!\n\n\n\nScreenshot of the packages needed to be updated from selecting the RStudio update\n\n\nEagle eyed readers may recognise this Public Health package {fingertipscharts}. If not, check it out!\n\n\n{fingertipscharts} is not on CRAN and can be found at the GitHub repository https://github.com/ukhsa-collaboration/fingertipscharts\nIf you are like me you may have installed some packages that you now rarely use and have no idea what they are. They may ask the following in the console (bottom left of the screen):\nDo you want to install from sources the packages which need compilation?\n\n\n\nScreenshot of the RStudio Console message when packages require compilation\n\n\nThis prompt is so that the package can be updated by building it on your computer. I’ve got a couple of packages that I have tried to do this but each time I go to check for updates they are still requesting an update so I just say no so I can fly through the other updates.\nFinally, a bit of a vague warning as I don’t understand this part but I once updated packages after I’d run a couple of scripts. This meant that a couple of the packages that needed updating were already loaded and so things got a bit muddled. I’m not entirely sure if this is a problem but I now shut all projects and code and run a new R Studio screen to do updates.\nThis blog was written by Zoe Turner, Senior Information Analyst at Nottinghamshire Healthcare NHS Foundation Trust.\n\n\n\n Back to topReuseCC0CitationFor attribution, please cite this work as:\nTurner, Zoë. 2018. “But This Worked the Last Time I Ran\nIt!” December 20, 2018. https://nhs-r-community.github.io/nhs-r-community//blog/but-this-worked-the-last-time-i-ran-it.html."
  },
  {
    "objectID": "blog/count-of-working-days-function.html",
    "href": "blog/count-of-working-days-function.html",
    "title": "Count of working days function",
    "section": "",
    "text": "It’s at this time of year I need to renew my season ticket and I usually get one for the year. Out of interest, I wanted to find out how much the ticket cost per day, taking into account I don’t use it on weekends or my paid holidays. I started my workings out initially in Excel but got as far as typing the formula =WORKDAYS() before I realised it was going to take some working out and perhaps I should give it a go in R as a function…\nChris Beeley had recently shown me functions in R and I was surprised how familiar they were as I’ve seen them on Stack Overflow (usually skimmed over those) and they are similar to functions in SQL which I’ve used (not written) where you feed in parameters.\nWhen I write code I try to work out how each part works and build it up but writing a function requires running the whole thing and then checking the result, the objects that are created in the function do not materialise so are never available to check. Not having objects building up in the environment console is one of the benefits of using a function, that and not repeating scripts which then ALL need updating if something changes."
  },
  {
    "objectID": "blog/count-of-working-days-function.html#bus-ticket-function",
    "href": "blog/count-of-working-days-function.html#bus-ticket-function",
    "title": "Count of working days function",
    "section": "Bus ticket function",
    "text": "Bus ticket function\nThis is the final function which if you run you’ll see just creates a function.\n\n# Week starts on Sunday (1)\nDailyBusFare_function &lt;- function(StartDate, EmployHoliday, Cost, wfh){\n\n  startDate &lt;- dmy(StartDate)\n  endDate &lt;- as.Date(startDate) %m+% months(12)\n\n# Now build a sequence between the dates:\n  myDates &lt;-seq(from = startDate, to = endDate, by = \"days\")\n\n  working_days &lt;- sum(wday(myDates)&gt;1&wday(myDates)&lt;7)-length(holidayLONDON(year = lubridate::year(startDate))) - EmployHoliday - wfh\n\nper_day &lt;- Cost/working_days\n\nprint(per_day)\n}\n\nRunning the function you feed in parameters which don’t create their own objects:\nDailyBusFare_function(\"11/07/2019\",27,612,1)\n[1] 2.707965"
  },
  {
    "objectID": "blog/count-of-working-days-function.html#going-through-each-line",
    "href": "blog/count-of-working-days-function.html#going-through-each-line",
    "title": "Count of working days function",
    "section": "Going through each line:",
    "text": "Going through each line:\nTo make sure each part within the function works it’s best to write it in another script or move the bit betweeen the curly brackets {}.\nFirstly, the startDate is self explanatory but within the function I’ve set the endDate to be dependent upon the startDate and be automatically 1 year later.\nOriginally when I was trying to find the “year after” a date I found some documentation about {lubridate}’s function dyear():\n\n# Next couple of lines needed to run the endDate line!\nlibrary(lubridate)\nstartDate &lt;- dmy(\"11/07/2019\")\n\nendDate &lt;- startDate + dyears(1)\n\nbut this gives an exact year after a given date and doesn’t take into account leap years. I only remember this because 2020 will be a leap year so the end date I got was a day out!\nInstead, Chris Beeley suggested the following:\n\nendDate &lt;- as.Date(startDate) %m+% months(12)\n\nNext, the code builds a sequence of days. I got this idea of building up the days from the blogs on getting days between two dates but it has also come in use when plotting over time in things like SPCs when some of the time periods are not in the dataset but would make sense appearing as 0 count.\n\nlibrary(lubridate)\n\n# To run so that the sequencing works\n# using as.Date() returns incorrect date formats 0011-07-20 so use dmy from\n# lubridate to transform the date\n\n  startDate &lt;- dmy(\"11/07/2019\")\n  endDate &lt;- as.Date(startDate) %m+% months(12)\n\n# Now build a sequence between the dates:\n  myDates &lt;- seq(from = startDate, to = endDate, by = \"days\")\n\nAll of these return values so trying to open them from the Global Environment won’t do anything. It is possible view the first parts of the values but also typing:\n\n# compactly displays the structure of object, including the format (date in this case)\nstr(myDates)\n\n Date[1:367], format: \"2019-07-11\" \"2019-07-12\" \"2019-07-13\" \"2019-07-14\" \"2019-07-15\" ...\n\n# gives a summary of the structure\nsummary(myDates)\n\n        Min.      1st Qu.       Median         Mean      3rd Qu.         Max. \n\"2019-07-11\" \"2019-10-10\" \"2020-01-10\" \"2020-01-10\" \"2020-04-10\" \"2020-07-11\" \n\n\nTo find out what a function does type ?str or?summary in the console. The help file will then appear in the bottom right Help screen.\nNext I worked out working_days. I got the idea from a blog which said to use length and which:\n\n  working_days &lt;- length(which((wday(myDates)&gt;1&wday(myDates)&lt;7)))\n\nNote that the value appears as 262L which is a count of a logical vector. Typing ?logical into the Console gives this in the Help:\nLogical vectors are coerced to integer vectors in contexts where a numerical value is required, with TRUE being mapped to 1L, FALSE to 0L and NA to NA_integer._\nI was familiar with length(), it does a count essentially of factors or vectors (type ?length into the Console for information) but which() wasn’t something I knew about. Chris Beeley explained what which does with the following example:\n\n# Generate a list of random logical values\na &lt;- sample(c(TRUE, FALSE), 10, replace = TRUE)\n\n# Look at list\na\n\n [1]  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE  TRUE FALSE FALSE\n\n# using which against the list gives the number in the list where the logic = TRUE\nwhich(a)\n\n[1] 1 2 3 4 8\n\n# counts how many logical arguments in the list (should be 10)\nlength(a)\n\n[1] 10\n\n# counts the number of TRUE logical arguments\nlength(which(a))\n\n[1] 5\n\n\nThen Chris Beeley suggested just using sum instead of length(which()) which counts a logical vector:\n\nsum(a)\n\n[1] 5\n\n\nIt seems this has been discussed on Stack Overflow before: https://stackoverflow.com/questions/2190756/how-to-count-true-values-in-a-logical-vector\nIt’s worthy of note that using sum will also count NAs so the example on Stack overflow suggest adding:\n\nsum(a, na.rm = TRUE)\n\n[1] 5\n\n\nThis won’t affect the objects created in this blog as there were no NAs in them but it’s just something that could cause a problem if used in a different context.\n\n  working_days &lt;- sum(wday(myDates)&gt;1&wday(myDates)&lt;7)\n\n# Just to check adding na.rm = TRUE gives the same result\n  working_days &lt;- sum(wday(myDates)&gt;1&wday(myDates)&lt;7, na.rm = TRUE)\n\nI then wanted to take into account bank/public holidays as I wouldn’t use the ticket on those days so I used the function holidayLONDON( from the package {timeDate}:\n\nlength(holidayLONDON(year = lubridate::year(startDate)))\n\n[1] 8\n\n\nI used lubridate::year because the package {timeDate} uses a parameter called year so the code would read year = year(startDate) which is confusing to me let alone the function!\nAgain, I counted the vectors using length(). This is a crude way of getting bank/public holidays as it is looking at a calendar year and not a period (July to July in this case).\nI did look at a package called {bizdays} but whilst that seemed to be good for building a calendar I couldn’t work out how to make it work so I just stuck with the {timeDate} package. I think as I get more confident in R it might be something I could look into the actual code for because all packages are open source and available to view through CRAN or GitHub.\nBack to the function…\nI then added - EmployHoliday so I could reduce the days by my paid holidays and - wfh to take into account days I’ve worked from home and therefore not travelled into work.\nThe next part of the code takes the entered Cost and divides by the Working_days, printing the output to the screen:\nper_day &lt;- Cost/working_days\nprint(per_day)\nAnd so the answer to the cost per day is printed in the Console:\n\nDailyBusFare_function(\"11/07/2019\",27,612,1)\n\n[1] 2.707965"
  },
  {
    "objectID": "blog/count-of-working-days-function.html#a-conclusion-of-sorts",
    "href": "blog/count-of-working-days-function.html#a-conclusion-of-sorts",
    "title": "Count of working days function",
    "section": "A conclusion… of sorts",
    "text": "A conclusion… of sorts\nWhilst this isn’t really related to the NHS it’s been useful to go through the process of producing a function to solve a problem and then to explain it, line by line, for the benefit of others.\nI’d recommend doing this to further your knowledge of R at whatever level you are and particularly if you are just learning or consider yourself a novice as sometimes blogs don’t always detail the reasons why things were done (or why they were not done because it all went wrong!)."
  },
  {
    "objectID": "blog/diverging-dot-plot-and-lollipop-charts-plotting-variance-with-ggplot2.html",
    "href": "blog/diverging-dot-plot-and-lollipop-charts-plotting-variance-with-ggplot2.html",
    "title": "Diverging Dot Plot and Lollipop Charts – Plotting Variance with ggplot2",
    "section": "",
    "text": "Creating the Dot Plot Variance chart\nThe data preparation was used in the previous blog entitled: Diverging Bar Charts – Plotting Variance with ggplot2.\n\n# 20240222 Added for qmd to run\nlibrary(ggplot2)\ntheme_set(theme_classic())\ndata(\"mtcars\") # load data\n\nmtcars$CarBrand &lt;- rownames(mtcars) # Create new column for car brands and names\n\nmtcars$mpg_z_score &lt;- round((mtcars$mpg - mean(mtcars$mpg))/sd(mtcars$mpg), digits=2)\n\nmtcars$mpg_type &lt;- ifelse(mtcars$mpg_z_score &lt;= 0, \"below\", \"above\")\n\nmtcars &lt;- mtcars[order(mtcars$mpg_z_score), ] #Ascending sort on Z Score\nmtcars$CarBrand &lt;- factor(mtcars$CarBrand, levels = mtcars$CarBrand)\n\nRefer to that if you need to know how to create the data prior to this tutorial.\nSetting up the Dot Plot Variance chart\n\nlibrary(ggplot2)\nggplot(mtcars, aes(x=CarBrand, y=mpg_z_score, label=mpg_z_score)) +\n  geom_point(stat='identity', aes(col=mpg_type), size=6) +\n  scale_color_manual(name=\"Mileage (deviation)\",\n                     labels = c(\"Above Average\", \"Below Average\"),\n                     values = c(\"above\"=\"#00ba38\", \"below\"=\"#0b8fd3\")) +\n  geom_text(color=\"white\", size=2) +\n  labs(title=\"Diverging Dot Plot (ggplot2)\",\n       subtitle=\"Z score showing Normalised mileage\", caption=\"Produced by Gary Hutson\") +\n  ylim(-2.5, 2.5) +\n  coord_flip()\n\nThis is very similar to the previous plot we created in the previous post, however there are a few differences. The main difference is that we use a geom_point() geometry and set the colour of the points based on whether the said point deviates above and below the average. In addition, we use the geom_text() to set the colour of the text in the points to white and specify the size of the text. The final difference is that I have added a Y limit (ylim) range of -2.5 standard deviation to positive 2.5 standard deviations.\nRunning this block of code, along with the data preparation code, will give you a chart that looks as below:\n\n\n\n\n\n\n\n\nCreating the Diverging Lollipop Chart\nThe code below shows how to build the diverging lollipop chart in R and ggplot2:\n\nggplot(mtcars, aes(x=CarBrand, y=mpg_z_score, label=mpg_z_score)) +\n  geom_point(stat='identity', aes(col=mpg_type), size=6) +\n  scale_color_manual(name=\"Mileage (deviation)\",\n                     labels = c(\"Above Average\", \"Below Average\"),\n                     values = c(\"above\"=\"#00ba38\", \"below\"=\"#0b8fd3\")) +\n  geom_segment(aes(y = 0,\n                   x = CarBrand,\n                   yend = mpg_z_score,\n                   xend = CarBrand),\n               color = \"black\") +\n  geom_text(color=\"white\", size=2) +\n  labs(title=\"Diverging Lollipop Chart\",\n       subtitle=\"Z score for normalised mileage\",\n       caption=\"Produced by Gary Hutson\") +\n  ylim(-2.5, 2.5) + coord_flip() + theme(panel.grid.major = element_blank(), panel.grid.minor =\n  element_blank())\n\nSimilar geometries are used here. What has been added here is the geom_segment() this shows how the line segments need to be added. The starting y is equal to 0 on the Y scale and the starting x is the first car by the car brand. Similarly, the end of the x (xend) is also the CarBrand.\nThe only other difference is to add a theme constraint to the end of the code to turn off the major and minor grid lines, this is achieved by setting the panel.grid.major and panel.grid.minor equal to element_blank().\nThe completed graph and plot is shown below:\n\n\n\n\n\n\n\n\nThere – we now have some lovely looking charts that can be put into a report to report on variance between categorical variables.\nThis blog was written by Gary Hutson, Principal Analyst, Activity & Access Team, Information & Insight at Nottingham University Hospitals NHS Trust, and was originally posted at Hutsons-Hacks.\n\n\n Back to topReuseCC0CitationFor attribution, please cite this work as:\nHutson, Gary. 2018. “Diverging Dot Plot and Lollipop Charts –\nPlotting Variance with Ggplot2.” May 24, 2018. https://nhs-r-community.github.io/nhs-r-community//blog/diverging-dot-plot-and-lollipop-charts-plotting-variance-with-ggplot2.html."
  },
  {
    "objectID": "blog/from-script-based-development-to-function-based-development-and-onwards-to-package-based-development-part-2.html",
    "href": "blog/from-script-based-development-to-function-based-development-and-onwards-to-package-based-development-part-2.html",
    "title": "From script-based development to function-based development and onwards to Package Based development: part 2",
    "section": "",
    "text": "At the NHS R Conference, I suggested to people that they should embrace the idea of package-based development rather than script-based work.\nIn the first part of this tutorial, in the fictional NHS-R Community greeting room, our humble analyst was tasked with greeting people. Rather than writing a script and needing to repeat themselves all the time with different variations of greetings and senders, they wrote a rather nifty little function to do this:\n\ngreet_person &lt;- function(greeting = \"Hello World\", sender = \"the NHS-R Community!\") {\n  if (!is.character(greeting)) {\n    stop(\"greeting must be a string\")\n  }\n\n  if (!is.character(sender)) {\n    stop(\"sender must be a string\")\n  }\n\n\n  if (length(sender) &gt; 1) {\n    warning(\"greet_person isn't very good at handling more than one sender. It is better to use just one sender at a time.\")\n  }\n\n  message(greeting, \" from \", sender)\n}\n\nAs we know, R is awesome and many people took up R on the background of some excellent publicity and training work by the NHS-R community. Our poor greeting team got overwhelmed by work: it is decided that the team of greeters needs to expand. There will now be a team of three greeters. Every other bit of work output from our NHS-R community team will involve greeting someone before we present our other awesome analysis to them.\nThis is going to be a nightmare! How can we scale our work to cope with multiple users, and multiple other pieces of work using our greeting function.\nIf we rely upon the scripts, we have to trust that others will use the scripts appropriately and not edit or alter them (accidentally or on purpose). Furthermore, if someone wants to greet someone at the beginning of their piece of analysis, they’ll either have to copy the code and paste it somewhere, or link to our script containing the function – which in turn means they need to keep a common pathname for everything and hope no-one breaks the function. Nightmare!\nFortunately, someone attended the last NHS-R conference and remembered that package-based development is a really handy way of managing to scale your R code in a sustainable way. So after a team meeting with copious caffeine, it is decided that greet_person needs to go into a new package, cunningly named {NHSRGreetings}. And here’s how we’re going to do it.\nIn R Studio, go to File and then to New Project. Click on New Directory, and then click on R Package. I am using RStudio 1.2 Preview for this tutorial which is downloadable from the R website. I would recommend doing this as some of the package management has been greatly simplified and some frustrating steps removed.\n\n\nScreenshot of RStudio’s package preview wizard\n\nClick on ‘Open in new session’ (so we can see the original code), and set the Package name as {NHSRGreetings}. We could just pull our old source files into the package – but for this tutorial I’m going to do things the longer way so you also know how to create new functions within an existing package.\nSet the project directory to somewhere memorable.\nFor now don’t worry about the git or {packrat} options – those are tutorials within themselves!\nYou are greeted with a package more or less configured up for you. A single source file, hello.R is set up for you within an R directory within the package. It’s not as cool as our function of course, but it’s not bad! It comes with some very helpful commented text:\n# Hello, world!\n#\n# This is an example function named 'hello'\n# which prints 'Hello, world!'.\n#\n# You can learn more about package authoring with RStudio at:\n#\n#   http://r-pkgs.had.co.nz/\n#\n# Some useful keyboard shortcuts for package authoring:\n#\n#   Install Package:           'Cmd + Shift + B'\n#   Check Package:             'Cmd + Shift + E'\n#   Test Package:              'Cmd + Shift + T'\nSo let’s check if the comments are right – hit Cmd + Shift + B on a Mac (on Windows and Linux you should see slightly different shortcuts). You can also access these options from the Build menu in the top right pane.\nYou will see the package build. R will then be restarted, and you’ll see it immediately performs the command library(NHSRGreetings) performed, which loads our newly built package.\nIf you type hello() at the command line, it will do as you may expect it to do!\nSo – time to customise our blank canvas and introduce our much more refined greeter.\nIn the root of our project you will see a file called DESCRIPTION. This contains all the information we need to customise our R project. Let’s customise the Title, Author, Maintainer and Descriptions for the package.\nWe can now create a new R file, and save it in the R subdirectory as greet_person.R. Copy over our greet_person function. We should be able to run install and our new function will be built in as part of the package.\nWe can now get individual team members to open the package, run the build once on their machine, and the package will be installed onto their machine. When they want to use any of the functions, they simply use the command library(NHSRGreetings) and the package will be ready to go with all the functions available to them. When you change the package, the authors will have to rebuild the package just the once to get access to the new features.\nWhen writing packages it is useful to be very wary about namespaces. One of the nice things about R is that there are thousands of packages available. The downside is that it makes it very likely that two individuals can choose the same name for a function. This makes it doubly important to pick appropriate names for things within a package.\nFor example, what if instead of the NHSRCommunity package someone wrote a {CheeseLoversRCommunity} package with a similarly names greet_person, but it did something totally different?\nIn a script, you have full control over the order you load your packages, so R will happily let you call functions from packages and trust that you know what order you loaded things in.\nIf you are a package author however, it’s assumed you may be installed on many machines, each with a potentially infinite set of combinations of different packages with names that may clash (or if they don’t already they might do in the future).\nSo within the package, any function which doesn’t come from R itself needs to have clearly defined which package it has come from.\nWithin DESCRIPTION you must define which package you use, and the minimum version. You do this with the Imports keyword. Attached is the Imports section of one of the SSNAP packages:\nImports:\n    methods (&gt;= 3.4.0),\n    lubridate (&gt;= 1.7.4),\n    tibble (&gt;= 1.4.2),\n    dplyr (&gt;= 0.7.5),\n    tibbletime (&gt;= 0.1.1),\n    glue (&gt;= 1.2.0),\n    purrr (&gt;= 0.2.5),\n    rlang (&gt;= 0.2.0),\n    readr (&gt;= 1.1.1),\n    stringr (&gt;= 1.3.1),\n    ssnapinterface (&gt;= 0.0.1)\nNext within your functions, rather than just calling the functions use the package name next to the function. For example instead of calling mutate() from the {dplyr} package, refer to it as dplyr::mutate() which tells R you mean the mutate function from the {dplyr} package rather than potentially any other package. There are ways to declare functions you are using a lot within an individual file – but this method makes things pretty foolproof.\nAnother tip is to avoid the {magrittr} pipe within package functions. Whilst {magrittr} makes analysis scripts nice and clean, firstly you still have the namespace issue to deal with (%&gt;%).\nIs actually a function, just one with a funny name – it is really called magrittr::%&gt;%() !) Secondly the way {magrittr} works can make debugging difficult. You don’t tend to see that from a script. But if you’re writing code in a package, which calls a function in another package, which calls code in another package, which uses {magrittr} – you end up with a really horrid nest of debugging errors: it is better to specify each step with a single variable which is reused.\nWhen you’ve got your code in, the next important thing to do is check your package. Build simply makes sure your code works. Check makes sure that you follow a lot of ‘rules’ of package making – including making sure R can safely and clearly know where every R function has come from. Check also demands that all R functions are documented: something which is outside of the scope of this tutorial and is probably the subject for another blog post – a documented function means if you type ?greet_person that you should be able to see how to use the function appropriately. It can help you create your own website for your package using the pkgdown package.\nIf your package both builds and checks completely and without errors or warnings, you might want to think about allowing the wider public to use your project. To do this, you should consider submitting your project to CRAN. This involves a fairly rigorous checking process but means anyone can download and use your package.\nIf we can get enough people to develop, share their code and upload their packages to CRAN we can work together to improve the use of R across our community.\nFeedback and responses to @drewhill79.\nThis blog was written by Dr. Andrew Hill, Clinical Lead for Stroke at St Helens and Knowsley Teaching Hospitals.\n\n\n\n Back to topReuseCC0CitationFor attribution, please cite this work as:\nHill, Andrew. 2019. “From Script-Based Development to\nFunction-Based Development and Onwards to Package Based Development:\nPart 2.” January 7, 2019. https://nhs-r-community.github.io/nhs-r-community//blog/from-script-based-development-to-function-based-development-and-onwards-to-package-based-development-part-2.html."
  },
  {
    "objectID": "blog/histogram-with-auto-binning-in-ggplot2.html",
    "href": "blog/histogram-with-auto-binning-in-ggplot2.html",
    "title": "Histogram with auto binning in ggplot2",
    "section": "",
    "text": "# 20240224 Added for qmd to run\nlibrary(ggplot2)\ntheme_set(theme_classic())\ndata(\"mtcars\") # load data\n\nmtcars$CarBrand &lt;- rownames(mtcars) # Create new column for car brands and names\n\nmtcars$mpg_z_score &lt;- round((mtcars$mpg - mean(mtcars$mpg))/sd(mtcars$mpg), digits=2)\n\nmtcars$mpg_type &lt;- ifelse(mtcars$mpg_z_score &lt;= 0, \"below\", \"above\")\n\nmtcars &lt;- mtcars[order(mtcars$mpg_z_score), ] #Ascending sort on Z Score\nmtcars$CarBrand &lt;- factor(mtcars$CarBrand, levels = mtcars$CarBrand)\n\nHistograms (with auto binning)\nAgain, we will use the mtcars dataset and use the fields in that to produce the chart, as we are doing this there is nothing to do on the data preparation side. That leaves us to have fun with the plot.\nBuilding the Histogram with auto binning\nI set up the plot, as per below:\n\nlibrary(ggplot2)\ntheme_set(theme_classic())\n\nI import the ggplot2 library and set my chart theme to a classic theme. The process next is to create the histogram plot and feed in the relevant data:\n\nplot &lt;- ggplot(mpg, aes(displ)) + scale_fill_brewer(palette = \"Blues\")\n\nI create a plot placeholder in memory so I can reuse this plot again and again in memory. This sets the aes layer equal to the displacement metric in the mtcars data frame. I then use the scale_fill_brewer command and select the palette to the Blues palette. A list of palettes can be found on the R Graph Gallery.\n\nThe next section uses the geom_histogram() geometry to force this to be a histogram:\n\nplot + geom_histogram(aes(fill=class),\n                      binwidth = .1,\n                      col=\"black\",\n                      size=.1) +\n  labs(title=\"Histogram with Auto Binning\",\n       caption=\"Produced by Gary Hutson\") + xlab(\"Displacement\")\n\nThe histogram uses the class of vehicle as the histogram fill, the binwidth is the width of the bins required, the colour is equal to black and the size is stipulated here. All that I then do is add the data labels to it and you have a lovely looking histogram built. This can be applied to any dataset. The output is as below:\n\n\n\n\n\n\n\n\nSpecifying binning values\nThe script can be simply changed in the histogram layer by adding the bins parameter:\n\nplot + geom_histogram(aes(fill=class),\n                   bins=5,\n                   col=\"black\",\n                   size=.1) +\n  labs(title=\"Histogram with Auto Binning\",\n       caption=\"Produced by Gary Hutson\") + xlab(\"Displacement\")\n\n\n\n\n\n\n\nThis blog was written by Gary Hutson, Principal Analyst, Activity & Access Team, Information & Insight at Nottingham University Hospitals NHS Trust, and was originally posted on Hutsons-Hacks.\n\n\n Back to topReuseCC0CitationFor attribution, please cite this work as:\nHutson, Gary. 2018. “Histogram with Auto Binning in\nGgplot2.” May 24, 2018. https://nhs-r-community.github.io/nhs-r-community//blog/histogram-with-auto-binning-in-ggplot2.html."
  },
  {
    "objectID": "blog/importing-and-exporting-data.html",
    "href": "blog/importing-and-exporting-data.html",
    "title": "Importing and exporting Data",
    "section": "",
    "text": "This blog originally appeared in http://gastrodatascience.com\nThere are a large number of file types that are able to store data. R is usually able to import most of them but there are some caveats. Below is a summary of methods I use for data imports using the most common file types.\nIt is worth saying that most datasets will come from excel or csv files. It is unusual to gain direct access to the database and these are the normal export types from most data storage systems."
  },
  {
    "objectID": "blog/importing-and-exporting-data.html#import-csv-or-text",
    "href": "blog/importing-and-exporting-data.html#import-csv-or-text",
    "title": "Importing and exporting Data",
    "section": "Import csv or text",
    "text": "Import csv or text\n\nread.table(\"mydata.txt\",header=T,stringsAsFActors=F) \n\n#or, and using tab as a delimiter:\n\nread_delim(\"SomeText.txt\", \"\\t\",trim_ws = TRUE)\n\n#Maybe get a csv off the internet:\ntbl &lt;- read.csv(\"http://www.example.com/download/data.csv\")\n\nTo prevent strings being imported as factors, add the parameter stringsAsFActors=F"
  },
  {
    "objectID": "blog/importing-and-exporting-data.html#import-from-excel",
    "href": "blog/importing-and-exporting-data.html#import-from-excel",
    "title": "Importing and exporting Data",
    "section": "Import from excel",
    "text": "Import from excel\n\nlibrary(XLConnect)\nwk = loadWorkbook(\"~Mydata.xlsx\")\ndfw = readWorksheet(wk, sheet=\"Sheet3\",header=TRUE)\n\n#Alternative and super friendly way\n#For excel imports using readxl package:\nlibrary(readxl)\nread_excel(\"~Mydata.xlsx\")"
  },
  {
    "objectID": "blog/importing-and-exporting-data.html#import-from-database",
    "href": "blog/importing-and-exporting-data.html#import-from-database",
    "title": "Importing and exporting Data",
    "section": "Import from database",
    "text": "Import from database\n\nlibrary(RODBC)\nchannel &lt;- odbcConnect(\"MyDatabase\", believeNRows=FALSE)\n#Get one of the tables\ntbl_PatientDetails&lt;-sqlFetch(channel, \"tblPtDetails\")"
  },
  {
    "objectID": "blog/importing-and-exporting-data.html#export-to-excel",
    "href": "blog/importing-and-exporting-data.html#export-to-excel",
    "title": "Importing and exporting Data",
    "section": "Export to excel",
    "text": "Export to excel\n\nlibrary(XLConnect)\nexc &lt;- loadWorkbook(\"~Mydata.xls\", create = TRUE)\ncreateSheet(exc,'Input')\nsaveWorkbook(exc)\nXLConnect::writeWorksheet(exc,mydata,sheet = \"Input\", startRow = 1, startCol = 2)\n\n#Another way is:\nlibrary(xlsx)\nwrite.xlsx(mydata, \"c:/mydata.xlsx\")"
  },
  {
    "objectID": "blog/importing-and-exporting-data.html#export-to-csv-or-a-tab-delimited-file",
    "href": "blog/importing-and-exporting-data.html#export-to-csv-or-a-tab-delimited-file",
    "title": "Importing and exporting Data",
    "section": "Export to csv or a tab delimited file",
    "text": "Export to csv or a tab delimited file\n\n write.csv(mydata, file=\"filename\", row.names=FALSE)\n write.table(mydata, \"c:/mydata.txt\", sep=\"\\t\")\n\nThere are also many other file types that can be imported and exported but these are the most common so the most practical."
  },
  {
    "objectID": "blog/local-public-health-joins-the-party.html",
    "href": "blog/local-public-health-joins-the-party.html",
    "title": "Local Public Health joins the paRty",
    "section": "",
    "text": "Having recently attended an R meetup in Birmingham, hearing of various user groups and hackathons that take place around certain technologies, I was getting a feeling that there was an increasing desire in the public health community to learn more about R and modern data science tools and techniques. I wondered whether there would be interest in a data science and R user group for the West Midlands public health intelligence community. I thought I’d raise the idea at the West Midlands Public Health Intelligence Group (WMPHIG) when I attended the quarterly meeting, but another attendee beat me to it and doing so confirmed there was some interest. I volunteered to arrange the first user group and Public Health England (PHE) kindly offered assistance.\nBetween us we setup a date and venue for the first meeting at the PHE offices in Birmingham and I was pleased to hear from Nicola at PHE that “…Tickets are selling like hotcakes! “\nNot knowing exactly how the group would best work, we suggested a loose structure for the meeting with the following discussion points:\n\nHow this group should work\n\nAssess current levels of knowledge/experience\n\nR training requirements\n\nR learning methods\n\nPublic health R use examples (including Fingertips R) & wider use examples\n\nWhat could be done with R / What else do people want to do with R\n\nChallenges and issues people have experienced/are experiencing\n\nPossible joint projects that might benefit all members\n\nWe ended up staying reasonably on topic, but there was plenty of useful and engaging discussion around the topics of data science and R. The was a nice mix of novice and more advanced R users (though no one admitted to being an expert 😉 ) in the group. Many of those who were more advanced had fairly recently been novice users. Whilst the more advanced users were able to share their experiences of their learning journeys, others were able to contribute on how we might develop use of data science and R in Public Health Intelligence. I was also impressed with some of the examples of R use that were shared with the group by analysts who have only been using it for a relatively short time. A key point shared was though R may seem a bit daunting at first, its worth jumping in and getting your analytical hands dirty!\nA number of attendees had also managed to attend the NHS-R Community Conference and shared positive experiences of the day and the knowledge they’d picked up.\nEveryone appeared to agree that R and other modern data science tools/methods can offer a lot to public health intelligence. There also appeared to be a desire to work together and help each other out on this learning journey. With that spirit in mind, we have agreed to share code and other useful information on K-Hub (https://khub.net/) and another meeting is going to be arranged for next quarter.\nThanks to all that attended and contributed and to PHE for helping with the organisation.\nThis blog was written by Andy Evans, Senior Officer at Birmingham City Council.\n\n\n\n Back to topReuseCC0CitationFor attribution, please cite this work as:\nEvans, Andy. 2018. “Local Public Health Joins the paRty.”\nNovember 22, 2018. https://nhs-r-community.github.io/nhs-r-community//blog/local-public-health-joins-the-party.html."
  },
  {
    "objectID": "blog/nhs-meets-r.html",
    "href": "blog/nhs-meets-r.html",
    "title": "NHS meets R",
    "section": "",
    "text": "Hello and welcome to our nascent NHS-R Community; a community dedicated to promoting the learning, application and utilisation of R in the National Health Service (NHS) in the United Kingdom. Like any community, NHS-R relies on the vibrancy of its participants to be relevant and productive – and fun.\n\n\nThe NHS is one of the best healthcare systems in the world[1]. It was launched in 1948 with the guiding principle of being free at the point of delivery – a kind of crowd funded open-source freeware equivalent of healthcare. More than half (52%) of the public say the NHS is what makes them most proud to be British, placing it above the armed forces (47%), the Royal Family (33%), Team GB (26%) and the BBC (22%)1.\nThe NHS in England deals with about 1 million people every 36 hours[2] and is continually generating vast amounts of data about the health and care of people[3]. This data is one of the most precious, yet under tapped, resources in the NHS. “Data is the new oil of the digital economy”[4] and drilling and mining NHS data could improve the NHS[5]. But mining these mountains of data is a colossal task.\nThis is where R comes in. R was conceived in 1992[6] as a free open-source statistical programming environment, which is now widely used in industry[7] (Google, Microsoft, Airbnb, New York Times, Lloyds of London, and so on) and academia, and is now ranked amongst the most popular (sixth as of 2017) programming languages[8]. But its use in the NHS is almost non-existent. Whilst there are several reasons for this, the absence of R at scale in the NHS, means that the NHS is unable to take advantage of the huge benefits of R, including cutting-edge visualisation and statistical tools, and a worldwide R community, which freely shares learning and resources.\nSo, our aim is to promote the use of R in the NHS, and help to make the NHS better.\nTo kick-start the NHS-R Community, we have developed a website [www.nhsrcommunity.com] and are offering four free workshops (3 days each, repeated in Yorkshire and Wales). Workshop (1) will be an introduction to R for healthcare analysts. Subsequent workshops will focus on the following problems:- (2) understanding and reporting hospital mortality statistics, (3) predicting urgent demand for hospital care and (4) evaluation of interventions using matched retrospective controls. Each workshop will be led by experts in both the problem domain and R, and captured electronically for wider dissemination. Registration for the workshops is now open*.\nHowever, anyone can contribute to the NHS-R Community, so why not share your experience (novice, beginner, or otherwise) of using R in the healthcare setting? Write a blog, share R tips, do an on-line R tutorial, suggest topics for ongoing development and support, and share ideas on how to embed R into the NHS.\nFrom the NHS-R Team [Posted: 19 March 2018]\nThe NHS-R Community project is funded by The Health Foundation.\n*NB: To be eligible for the workshops you must have a working NHS email address. Places are limited.\n\nhttps://www.newscientist.com/article/2140698-us-ranked-worst-healthcare-system-while-the-nhs-is-the-best/ Accessed 20240221\nhttps://www.nhs.uk/NHSEngland/thenhs/about/Pages/overview.aspx Accessed 20240221\nhttps://www.wired.com/insights/2014/07/data-new-oil-digital-economy/ Accessed 20240221\nhttp://blog.revolutionanalytics.com/2017/10/updated-history-of-r.html Accessed 20240221\nhttp://makemeanalyst.com/companies-using-r/ Accessed 20240221\nhttps://spectrum.ieee.org/computing/software/the-2017-top-programming-languages Accessed 20240221\n\nThis blog has been formatted to remove Latin Abbreviations."
  },
  {
    "objectID": "blog/nhs-meets-r.html#so-why-get-involved",
    "href": "blog/nhs-meets-r.html#so-why-get-involved",
    "title": "NHS meets R",
    "section": "",
    "text": "The NHS is one of the best healthcare systems in the world[1]. It was launched in 1948 with the guiding principle of being free at the point of delivery – a kind of crowd funded open-source freeware equivalent of healthcare. More than half (52%) of the public say the NHS is what makes them most proud to be British, placing it above the armed forces (47%), the Royal Family (33%), Team GB (26%) and the BBC (22%)1.\nThe NHS in England deals with about 1 million people every 36 hours[2] and is continually generating vast amounts of data about the health and care of people[3]. This data is one of the most precious, yet under tapped, resources in the NHS. “Data is the new oil of the digital economy”[4] and drilling and mining NHS data could improve the NHS[5]. But mining these mountains of data is a colossal task.\nThis is where R comes in. R was conceived in 1992[6] as a free open-source statistical programming environment, which is now widely used in industry[7] (Google, Microsoft, Airbnb, New York Times, Lloyds of London, and so on) and academia, and is now ranked amongst the most popular (sixth as of 2017) programming languages[8]. But its use in the NHS is almost non-existent. Whilst there are several reasons for this, the absence of R at scale in the NHS, means that the NHS is unable to take advantage of the huge benefits of R, including cutting-edge visualisation and statistical tools, and a worldwide R community, which freely shares learning and resources.\nSo, our aim is to promote the use of R in the NHS, and help to make the NHS better.\nTo kick-start the NHS-R Community, we have developed a website [www.nhsrcommunity.com] and are offering four free workshops (3 days each, repeated in Yorkshire and Wales). Workshop (1) will be an introduction to R for healthcare analysts. Subsequent workshops will focus on the following problems:- (2) understanding and reporting hospital mortality statistics, (3) predicting urgent demand for hospital care and (4) evaluation of interventions using matched retrospective controls. Each workshop will be led by experts in both the problem domain and R, and captured electronically for wider dissemination. Registration for the workshops is now open*.\nHowever, anyone can contribute to the NHS-R Community, so why not share your experience (novice, beginner, or otherwise) of using R in the healthcare setting? Write a blog, share R tips, do an on-line R tutorial, suggest topics for ongoing development and support, and share ideas on how to embed R into the NHS.\nFrom the NHS-R Team [Posted: 19 March 2018]\nThe NHS-R Community project is funded by The Health Foundation.\n*NB: To be eligible for the workshops you must have a working NHS email address. Places are limited.\n\nhttps://www.newscientist.com/article/2140698-us-ranked-worst-healthcare-system-while-the-nhs-is-the-best/ Accessed 20240221\nhttps://www.nhs.uk/NHSEngland/thenhs/about/Pages/overview.aspx Accessed 20240221\nhttps://www.wired.com/insights/2014/07/data-new-oil-digital-economy/ Accessed 20240221\nhttp://blog.revolutionanalytics.com/2017/10/updated-history-of-r.html Accessed 20240221\nhttp://makemeanalyst.com/companies-using-r/ Accessed 20240221\nhttps://spectrum.ieee.org/computing/software/the-2017-top-programming-languages Accessed 20240221\n\nThis blog has been formatted to remove Latin Abbreviations."
  },
  {
    "objectID": "blog/nhs-r-community-conference-my-experience-of-the-day.html",
    "href": "blog/nhs-r-community-conference-my-experience-of-the-day.html",
    "title": "NHS-R Community Conference: My experience of the day",
    "section": "",
    "text": "I went to the NHS-R Community Conference in Birmingham on Tuesday.\nIt was great.\nHere are three observations about it.\nFirst, the old versus the new. Quite a few of the speakers alluded to the idea that R is sometimes seen in the NHS as this ‘new’ thing that is here to ‘replace’ the ‘old’ tools of Excel, SQL, SPSS and so on. It’s an interesting dichotomy to ponder upon, (a) because it is of course infinitely more complex than that, and (b) because there are so many ways that it’s possible to cast R as the new ‘good guy’ and Excel and so on. as the old ‘bad guy’. Having expressed caveats though, it was interesting to hear throughout the day how often people tended to explain what R was doing by reference to how Excel would do (or – more pertinently – fail to do) the same thing, only it would be more clunky in Excel.\nIn fact, in the workshop on patient flow that I co-presented with John MacKintosh, we subconsciously had cast ourselves in these roles. I was the old bad guy who was over-reliant on Excel; John was the younger good guy who shows how you can do it better – and you can do more with it – in R. The visualizations we were showcasing were ones that I’d originally done in Excel and that John had improved considerably by using R.\nSecond, and this next point follows on from the Excel versus R idea, I am intrigued by how ‘light on its feet’ R is. Can R respond to suggestions and edits from managers and clinicians ‘on the fly’? One reservation I’ve had about R as a tool for using at the clinical/managerial interface is that it looks too ‘data-y’, and therefore too forbidding, too exclusive and as a result it frightens the horses. Whereas one of Excel’s virtues is that it’s at least familiar to pretty much everyone, and therefore a bit less daunting as an interface, and you can make use of that familiarity by showing your workings in a way that has a chance of being understood.\nBut in general I think I am persuaded by the swiftness and elegance of R as a data analysis tool. It might indeed look more forbidding than Excel but we can probably edit and re-draft our work ten times more quickly than we could in Excel, so in terms of rapid iterations (including iterations while we’re actually in the meeting), R wins. Again, I’ll quote an example from my workshop: John attempted some live editing of the code while we were presenting, and yes, it worked, so – yes – it was reassuring to know it can be done, even in in the middle of a presentation to an audience of 24 subject-matter experts.\nThird, and apologies of this observation seems a bit self-congratulatory, but it needs to be said. The mood of the conference was good. It felt congenial. There was a general ‘nice-ness’ vibe throughout the day. People were respectful, people were inclusive, it was easy to network.\nI remember thinking on the train as I made my way to the event that I might suffer from imposter syndrome when I got there. I have had very little exposure to R. I’ve made a start on the tutorials in DataCamp but I really haven’t got very far. And I am utterly indebted to my collaborator John MacKintosh when it comes to having my awareness raised as to the possibilities and potential of R. So I was a bit anxious that I might be sniffed at by the other delegates as someone who wasn’t a bona fide (genuine) R geek, given that so many of the delegates had technical skills that were in a different class altogether.\nBut I needn’t have worried. It turns out that’s not how the NHS-R community works. It’s inclusive, not exclusive. It’s a multi-disciplinary forum, not a talking shop for geeks. Which means that when the final plenary session for the day was trying to identify the main themes to emerge from the conference, it was collaboration that emerged for me as the key word. We do need to find ways of collaborating better. Collaboration that cuts across disciplines, and across organisations.\nBut on the evidence of the mood and feel of Tuesday’s conference, collaboration should be easy, because this is a community of people who want to help one another.\nThis blog has been formatted to remove Latin Abbreviations.\n\n\n\n Back to topReuseCC0CitationFor attribution, please cite this work as:\nPettinger, Neil. 2018. “NHS-R Community Conference: My Experience\nof the Day.” October 16, 2018. https://nhs-r-community.github.io/nhs-r-community//blog/nhs-r-community-conference-my-experience-of-the-day.html."
  },
  {
    "objectID": "blog/r-studio-shortcuts.html",
    "href": "blog/r-studio-shortcuts.html",
    "title": "R studio shortcuts",
    "section": "",
    "text": "I love keyboard shortcuts. I work in R studio and using keyboard shortcuts has saved me a lot of time. There is a full list of short cuts and I have pulled together my three most used shortcuts.\nCtrl+enter or cmd+enter (Mac) will run the command where the cursor is and then move the cursor down. This is perfect for when you want to run your code line by line.\nCtrl+shift+m or cmd+shift+m (Mac) will insert a pipe (if you don’t already use pipes then you can learn more in R for Data Science.\nCtrl+shift+F10 or cmd+shift+F10 (Mac) will restart your R session. It unloads your packages but leaves the elements in your environment untouched. I use this a lot when I am jumping between scripts to minimise conflicts between packages.\n\n\n\n Back to topReuseCC0CitationFor attribution, please cite this work as:\nVestesson, Emma. 2018. “R Studio Shortcuts.” May 21, 2018.\nhttps://nhs-r-community.github.io/nhs-r-community//blog/r-studio-shortcuts.html."
  },
  {
    "objectID": "blog/showcasing-functions-separate.html",
    "href": "blog/showcasing-functions-separate.html",
    "title": "Showcasing a function - separate()\n",
    "section": "",
    "text": "This was written originally in an Excel spreadsheet and used {datapasta} to copy into R as code to build the same data frame. {datapasta} can be access through RStudio as an Addin as well as code. Find out more about {datapasta} from the Introduction to R and R Studio course.\n\ndata &lt;- tibble::tribble(\n    ~Patient,          ~Codes,\n  \"PatientA\", \"A01, A02, A03\",\n  \"PatientB\", \"B01; B02; B03\",\n  \"PatientC\", \"C01; C03\",\n  \"PatientD\", \"D01. D02. D03\"\n  )"
  },
  {
    "objectID": "blog/showcasing-functions-separate.html#create-data",
    "href": "blog/showcasing-functions-separate.html#create-data",
    "title": "Showcasing a function - separate()\n",
    "section": "",
    "text": "This was written originally in an Excel spreadsheet and used {datapasta} to copy into R as code to build the same data frame. {datapasta} can be access through RStudio as an Addin as well as code. Find out more about {datapasta} from the Introduction to R and R Studio course.\n\ndata &lt;- tibble::tribble(\n    ~Patient,          ~Codes,\n  \"PatientA\", \"A01, A02, A03\",\n  \"PatientB\", \"B01; B02; B03\",\n  \"PatientC\", \"C01; C03\",\n  \"PatientD\", \"D01. D02. D03\"\n  )"
  },
  {
    "objectID": "blog/showcasing-functions-separate.html#separate-codes-by-position",
    "href": "blog/showcasing-functions-separate.html#separate-codes-by-position",
    "title": "Showcasing a function - separate()\n",
    "section": "Separate codes by position",
    "text": "Separate codes by position\nSeparate into columns in the order data appears\n\nlibrary(tidyverse)\n\ndata |&gt; \n  tidyr::separate(Codes, c(\"col1\", \"col2\", \"col3\"))\n\n# A tibble: 4 × 4\n  Patient  col1  col2  col3 \n  &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;\n1 PatientA A01   A02   A03  \n2 PatientB B01   B02   B03  \n3 PatientC C01   C03   &lt;NA&gt; \n4 PatientD D01   D02   D03  \n\n\nhttps://tidyr.tidyverse.org/reference/separate.html"
  },
  {
    "objectID": "blog/showcasing-functions-separate.html#add-a-pivot",
    "href": "blog/showcasing-functions-separate.html#add-a-pivot",
    "title": "Showcasing a function - separate()\n",
    "section": "Add a pivot",
    "text": "Add a pivot\nTo move wide data to longer:\n\ndata |&gt; \n  tidyr::separate(Codes, c(\"col1\", \"col2\", \"col3\")) |&gt; \n  tidyr::pivot_longer(cols = c(starts_with(\"col\")),\n               names_to = \"type\")\n\n# A tibble: 12 × 3\n   Patient  type  value\n   &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt;\n 1 PatientA col1  A01  \n 2 PatientA col2  A02  \n 3 PatientA col3  A03  \n 4 PatientB col1  B01  \n 5 PatientB col2  B02  \n 6 PatientB col3  B03  \n 7 PatientC col1  C01  \n 8 PatientC col2  C03  \n 9 PatientC col3  &lt;NA&gt; \n10 PatientD col1  D01  \n11 PatientD col2  D02  \n12 PatientD col3  D03"
  },
  {
    "objectID": "blog/success-story-kate-cheema.html",
    "href": "blog/success-story-kate-cheema.html",
    "title": "Success story - Kate Cheema, Director of Health Intelligence, British Heart Foundation",
    "section": "",
    "text": "Tell us a bit about yourself, your team and how your story begins?\nThe Health Intelligence team at the British Heart Foundation just about still qualifies as a ‘new’ team at the stately age of 2-and-a-bit years. Our remit is to collate and analyse a wide range of data related to cardiovascular disease and its treatment and outcomes across the UK and use this to provide critical insight and context to the BHF’s charitable mission work. Some of our work is population focussed, understanding the patterns and trends in CVD prevalence and incidence. Some is more system focussed, getting under the skin of variation in care for CVD patients and learning more about how patients are impacted by service change. And still more is supporting particular programmes of work, such as the National Defibrillator Network or our community mobilisation projects. You’ll see our numbers in BHF adverts and occasionally one of us gets wheeled out to talk stats on local radio. Lots to do! We’re a small team, just 5.5 people, serving a large organisation so we need to work smart.\n\n\nWhat was the problem/challenge you were trying to address?\nCVD is a complex and varied group of illnesses and the data describing it can be very nuanced. Pair that with an organisation that needs to use it for comms and media work, as well as informing strategy and programmes of work and the result is the need for a hybrid model of delivery that does the basics brilliantly, allows for easy ‘self serve’ and frees up time to support colleagues with the complex stuff. So our challenge was to build a library of core resources for internal use, as automated as possible and presented in an accessible format for all colleagues to use.\n\n\nHow R helped you? Any particular libraries/products/packages you found the most useful?\nR has been invaluable in the whole project to date (still lots to do!) but two specific resources spring to mind. Firstly, the Tidyverse suite of packages has been invaluable in streamlining, and making repeatable, our data reshaping. Much of the publicly available data we use is downloaded from websites and is (ahem) not exactly in a useful format. Having standardised, and generally very simple, reshaping scripts to reuse across the team has saved hours of time, not to mention to ability to automate the download in the first place (kudos to ::curl::).\nSecondly, we have utilised R Markdown extensively in the automated production of simple off-the-(Sharepoint)-shelf PDF based reports. Accessible to all, impossible to break (famous last words) these generally take the form of a key set of data visualisations of a specific topic, usually rendered using ggplot2 but also using network visualisation packages (::network::, ::igraph::, ::tidygraph:: ) and n-gram analysis of text data (::tidytext::) where required.\n\n\nWhat is the result?\nThe beginnings of a library accessible to all in the BHF and a decent chunk of time saved. A couple of our reports are scheduled to run and publish automatically with zero intervention from the team outside of checking it’s there. We’ve used R in other standalone projects (for example forecasting work, network analysis as part of an evaluation project) too. We’re in the process of improving our R capability, in terms of skills and in terms of infrastructure, so this is really just the start of the story for us.\nThis blog has been formatted to remove Latin Abbreviations.\n\n\n\n\n Back to topReuseCC0CitationFor attribution, please cite this work as:\nCheema, Kate. 2022. “Success Story - Kate Cheema, Director of\nHealth Intelligence, British Heart Foundation.” March 21, 2022.\nhttps://nhs-r-community.github.io/nhs-r-community//blog/success-story-kate-cheema.html."
  },
  {
    "objectID": "blog/success-story-william-bryant.html",
    "href": "blog/success-story-william-bryant.html",
    "title": "Success story - Dr William Bryant, Senior Data Scientist, GOSH",
    "section": "",
    "text": "The Challenge:\nTo create a template workflow for a stream of analytics and data presentation projects for research and operational use at GOSH. To ensure that the codebase is: interpretable, reusable & maintainable and to control technical debt around the solutions we provide, so that they can be handed over to other technical teams in the Trust.\n\n\nThe Solution:\nWe have created a ‘Data Science’ template, inspired by Cookiecutter Data Science and using the targets R package to logically break down code into small reusable chunks applied to our research datasets.\n\n\nThe Results:\nWe are now running (and have completed) a number of projects from estimation of surgery lengths with XGBOOST, to dashboarding cardiac data for MDTs, to creating articles on historic data trends at GOSH and many more. This has provided us a rich common codebase applicable in all sorts of contexts, as well as giving us a headstart when understanding each other’s code and collaborating on development.\n\n\n\n\n Back to topReuseCC0CitationFor attribution, please cite this work as:\nBryant, William. 2022. “Success Story - Dr William Bryant, Senior\nData Scientist, GOSH.” March 21, 2022. https://nhs-r-community.github.io/nhs-r-community//blog/success-story-william-bryant.html."
  },
  {
    "objectID": "blog/the-joy-of-r.html",
    "href": "blog/the-joy-of-r.html",
    "title": "The joy of R",
    "section": "",
    "text": "Hello. My name is Julian and I am an R addict. I got hooked about 3 years ago when I took on a new role in Public Health England developing a public health data science team. My professional background is as a doctor and Consultant in Public Health and have spent the last 15 years in the health intelligence field so I thought I knew something about data and analysis. I realised I didn’t know anything about data science so I decided to do a course and ended up doing the Coursera data science MOOC from Johns Hopkins because it was health related. For the course, you need to learn R - and so my habit started. (It turned out I knew nothing about data and analysis as well).\nI had done an R course 15 years ago but never used it. Any analysis I did used spreadsheets, SPSS, Mapinfo and host of other tools, and I had never written a single line of code until 3 years ago (apart some very basic SQL). That’s all changed.\nApart from a brief obsession with Tableau a few years ago (which I still love), learning R has for me, been utterly transformational. Now my basic analytical workflow is R + Google (for getting answers when you are stuck) + Git (for sharing and storing code) + Mendeley (reference management software). That’s it.\nI barely open Excel except to look at data structure so I can import data into R; I don’t use GIS; I hardly even open word to write a document - I do that in R (like this blog); and recently the option to output to power point has appeared in R Markdown so I’ve started using that as well.\nOn top of that I have learned a whole heap of analytical and other skills through using R. I feel comfortable getting and analysing data of any size, shape and complexity including text, websites, APIs, very large datasets; and quickly. I can now rapidly produce scientific reports, scrape websites, mine text, automate analysis, build machine learning pipelines, create high quality graphics using the fab ggplot2 and its relatives, have co-authored a package to read Fingertips data (fingertipsR - very proud of this) and am getting my head around regular expressions. I have even managed a couple of Shiny documents. There is nothing I have wanted to do that I can’t do in R; and a huge range of things I didn’t know you could do or had never heard of.\nSo what is it about R that makes it so great? In the last 5 years it has moved from an academic stats package to a professional data science tool. One of the reasons is the development of the tidy data framework [1] and tools to make data wrangling or munging much easier. This is a much overlooked part of the analysts life - all the things you need to do with data before you can analyse it (50 - 70% of the process) has been paid serious attention and made much easier with packages like dplyr and tidyr. And a lot of attention has been made to making coding more logical and syntax more “English”. Another reason is the development of R Studio and R Markdown which give you button press outputs in a range of high quality formats. And there is a focus on reproducibility - the ability for analysis to be repeated exactly which requires combining data, analysis and results in a form others can follow. This is good science and will become much more widespread. You can do this in R and Git.\nMy addiction has infected my team and the analytical community in PHE. We are spreading R rapidly and writing packages to automate routine analysis and reporting. We routinely use Gitlab to share and collaborate on code, and are introducing software development ideas like code review and unit testing. In short we are trying to help analysts (if they want to) become analyst-developers.\nThere are downsides to R of course. There is a (big) learning curve, ICT get twitchy, there is a huge range of packages and any number of ways of doing things, and things often break. But as any addict would say, these are just obstacles to be overcome and there is a lot of support out there.\nR is not the only direction of travel - we do use PowerBI (running R scripts), and we do a bit of development in Python, but one thing is certain - I can’t go back to pre R days.\nSo there’s my confession. I’m a data junkie and an R addict. If you want to see my descent I put stuff on an RPubs page from time to time and I have a Github page. If you want to help me - feel free to get into touch or send me a pull request.\nThanks to Seb Fox at PHE and David Whiting of Medway Council for inspiration and support."
  },
  {
    "objectID": "blog/the-joy-of-r.html#references",
    "href": "blog/the-joy-of-r.html#references",
    "title": "The joy of R",
    "section": "References",
    "text": "References\n1 Wickham H. 2014;59:1–23. doi:10.18637/jss.v059.i10"
  },
  {
    "objectID": "blog/thoughts-on-the-nhs-r-conference.html",
    "href": "blog/thoughts-on-the-nhs-r-conference.html",
    "title": "Thoughts on the NHS-R conference",
    "section": "",
    "text": "It’s been a few weeks since the first NHS-R conference was held in Birmingham.\nI co-presented a couple of workshops with Neil Pettinger on visualising patient flow, covering the following\n\nimporting from Excel (and connecting to SQL Server)\ndplyr\nggplot2 & plotly\ngifski for basic animation\nautomating reports with officer\ngganimate demo\ntidy evaluation via a custom plot function\nalternative flow plots with ggalluvial\na simple Shiny app\nCode, data and templates for the workshop are available here.\n\nI hope those who attended found it useful.\nThe event itself was a huge success. I’d have loved to have been able to see all the sessions that were taking place.\nThere were a series of lightning talks – there are Trusts using R for machine learning and classification, and others using it for predicting admissions Chris Beeley presented on the use of Shiny, while Jacob Anhøj led a session on SPC using his qicharts2 package. It was a huge honour to meet Jacob, I’ve used his package almost since inception, and I know how much effort has gone into the rigorous analysis the package provides.\nI was also very pleased to meet Chris (Beeley), Gary Hutson, Ed Watkinson, Zoë (who masquerades on Twitter as Applied Information Nottingham) and Garry Fothergill face to face, and to see Val Perigo, Paul Stroner and Professor Mohammed Mohammed again. I also want to say thank you to Shahima who helped me revise my incredibly shoddy attempt at a bio, and for organising the event.\nFrom my point of view, the workshop was enjoyable to deliver. There were a couple of technical glitches ( I couldn’t share my laptop screen, so had to project it and then fly blind , which led to me spending more time facing the screen than the audience, which was not ideal), but even with that, we got through the material and it all worked within the alloted time.\nIt was great to be in a room full of analysts passionate about R and the NHS. Neil mentioned ‘community’ in his blog post, and it is true. There was a greate vibe, as I’d expect to be honest. I’ve never been one for being part of a gang but anyone into R in the NHS is instinctively all right by me 🙂 I’m pretty sure there was enough brain power in that room to tackle any analytical challenge that could get thrown at the NHS. The challenge is in harnessing that power , promoting R as the incredible tool that it is, and enabling us to work collaboratively rather than in silos. If only there was an R package for that..\nThis blog was written by John MacKintosh, Data Manager at NHS Highland and was originally posted on John Mackintosh’s own blog Data by John\n\n\n\n Back to topReuseCC0CitationFor attribution, please cite this work as:\nMacKintosh, John. 2018. “Thoughts on the NHS-R Conference.”\nNovember 15, 2018. https://nhs-r-community.github.io/nhs-r-community//blog/thoughts-on-the-nhs-r-conference.html."
  },
  {
    "objectID": "books/index.html",
    "href": "books/index.html",
    "title": "Books",
    "section": "",
    "text": "NHS-R Way book\n\n\n\ncode-of-conduct\n\n\ntraining\n\n\n\nEverything you need or want to know about NHS-R Community including: how to contribute and get involved, code styles, training preparation materials.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStatement on Tools\n\n\n\ndata-science\n\n\ninstallation\n\n\ngetting-started\n\n\n\nIn this book we’ve compiled a set of technical resources, links and write down our experiences of using open data science programs like R and Python within the NHS and…\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHealth Inequalities\n\n\n\ndata-science\n\n\nhealth-inequalities\n\n\n\nThis project is a collection of information and knowledge related to analytical work on “Health Inequalities”, as performed in the NHS and the wider UK Health and Social…\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOpen Analytics Resources\n\n\n\ndata-science\n\n\nanalysis\n\n\nlinks\n\n\ntraining\n\n\n\nUseful links for health and care analysts and data scientists.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "books/posts/NHSR-way/index.html",
    "href": "books/posts/NHSR-way/index.html",
    "title": "NHS-R Way book",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "books/posts/statement-on-tools/index.html",
    "href": "books/posts/statement-on-tools/index.html",
    "title": "Statement on Tools",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Contact us",
    "section": "",
    "text": "Email: nhs.rcommunity@nhs.net\nGitHub: @nhs-r-community\nLinkedIn: NHS-R Community\nFosstodon (Mastodon): @NHSrCommunity@fosstodon.org\n\nFor details on how NHS-R Community use social media can be found in the NHS-R Community book chapter Community Handbook.\n\n\n\n Back to top"
  }
]